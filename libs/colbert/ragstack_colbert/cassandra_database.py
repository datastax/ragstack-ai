"""
This module provides an implementation of the BaseVectorStore abstract class, specifically designed
for use with a Cassandra database backend. It allows for the efficient storage and management of text embeddings
generated by a ColBERT model, facilitating scalable and high-relevancy retrieval operations.
"""

import logging
from typing import List, Optional, Tuple

import cassio
from cassandra.cluster import ResponseFuture, Session
from cassio.table.query import Predicate, PredicateOperator
from cassio.table.tables import ClusteredMetadataVectorCassandraTable

from .base_database import BaseDatabase
from .constant import DEFAULT_COLBERT_DIM
from .objects import Chunk, Vector


class CassandraDatabase(BaseDatabase):
    """
    An implementation of the BaseDatabase abstract base class using Cassandra as the backend
    storage system. This class provides methods to store, retrieve, and manage text embeddings within
    a Cassandra database, specifically designed for handling vector embeddings generated by ColBERT.

    The table schema and custom index for ANN queries are automatically created if they do not exist.
    """

    _table: ClusteredMetadataVectorCassandraTable
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            raise ValueError(
                "This class cannot be instantiated directly. Please use the `from_astra()` or `from_session()` class methods."
            )
        return cls._instance

    @classmethod
    def from_astra(
        cls,
        database_id: str,
        astra_token: str,
        keyspace: Optional[str] = "default_keyspace",
        table_name: Optional[str] = "colbert",
        timeout: Optional[int] = 180,
    ):
        if cls._instance is None:
            cassio.init(token=astra_token, database_id=database_id, keyspace=keyspace)
            session = cassio.config.resolve_session()
            session.default_timeout = timeout

            return cls.from_session(
                session=session, keyspace=keyspace, table_name=table_name
            )
        return cls._instance

    @classmethod
    def from_session(
        cls,
        session: Session,
        keyspace: Optional[str] = "default_keyspace",
        table_name: Optional[str] = "colbert",
    ):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize(
                session=session, keyspace=keyspace, table_name=table_name
            )
        return cls._instance

    def _initialize(
        self,
        session: Session,
        keyspace: str,
        table_name: str,
    ):
        """
        Initializes a new instance of the CassandraVectorStore.

        Parameters:
            session (Session): The Cassandra session to use.
            keyspace (str): The keyspace in which the table exists or will be created.
            table_name (str): The name of the table to use or create for storing embeddings.
            timeout (int, optional): The default timeout in seconds for Cassandra operations. Defaults to 180.
        """

        try:
            is_astra = session.cluster.cloud
        except:
            is_astra = False

        logging.info(
            f"Cassandra store is running on {'AstraDB' if is_astra else 'Apache Cassandra'}."
        )

        self._table = ClusteredMetadataVectorCassandraTable(
            session=session,
            keyspace=keyspace,
            table=table_name,
            row_id_type=["INT", "INT"],
            vector_dimension=DEFAULT_COLBERT_DIM,
            vector_source_model="bert" if is_astra else None,
            vector_similarity_function=None if is_astra else "DOT_PRODUCT",
        )

    def add_chunks(self, chunks: List[Chunk]) -> List[Tuple[str, int]]:
        """
        Stores a list of embedded text chunks in the vector store

        Parameters:
            chunks (List[Chunk]): A list of `Chunk` instances to be stored.

        Returns:
            a list of tuples: (doc_id, chunk_id)
        """

        futures: List[Tuple[str, int, int, ResponseFuture]] = []

        for chunk in chunks:
            doc_id = chunk.doc_id
            chunk_id = chunk.chunk_id
            text = chunk.text
            metadata = chunk.metadata

            future = self._table.put_async(
                partition_id=doc_id,
                row_id=(chunk_id, -1),
                body_blob=text,
                metadata=metadata,
            )

            futures.append((doc_id, chunk_id, -1, future))

            for index, vector in enumerate(chunk.embedding):
                future = self._table.put_async(
                    partition_id=doc_id, row_id=(chunk_id, index), vector=vector
                )
                futures.append((doc_id, chunk_id, index, future))

            results: List[Tuple[str, int]] = []
            for doc_id, chunk_id, embedding_id, future in futures:
                try:
                    future.result()
                    results.append((doc_id, chunk_id))
                except Exception as e:
                    if embedding_id == -1:
                        logging.error(
                            f"issue inserting document data: {doc_id} chunk: {chunk_id}: {e}"
                        )
                    else:
                        logging.error(
                            f"issue inserting document embedding: {doc_id} chunk: {chunk_id} embedding: {embedding_id}: {e}"
                        )

        return results

    def delete_chunks(self, doc_ids: List[str]) -> None:
        """
        Deletes chunks from the vector store based on their document id.

        Parameters:
            doc_ids (List[str]): A list of document identifiers specifying the chunks to be deleted.

        Returns:
            True if the delete was successful.
        """

        futures = [
            (doc_id, self._table.delete_partition_async(partition_id=doc_id))
            for doc_id in doc_ids
        ]

        success = True
        for doc_id, future in futures:
            try:
                future.result()
            except Exception as e:
                success = False
                logging.error(f"issue on delete of document: {doc_id}: {e}")
        return success

    async def search_relevant_chunks(self, vector: Vector, n: int) -> List[Chunk]:
        """
        Retrieves 'n' ANN results for an embedded token vector.

        Returns:
            A list of Chunks with only `doc_id`, `chunk_id`, and `embedding` set.
            Here `embedding` is only a single vector, the one which matched on the ANN search.
            Fewer than 'n' results may be returned.
        """

        chunks: List[Chunk] = []

        # TODO: only return partition_id and row_id after cassio supports this
        rows = await self._table.aann_search(vector=vector, n=n)
        for row in rows:
            chunks.append(
                Chunk(
                    doc_id=row["partition_id"],
                    chunk_id=row["row_id"][0],
                    embedding=[row["vector"]],
                )
            )
        return chunks

    async def get_chunk_embedding(self, doc_id: str, chunk_id: int) -> Chunk:
        """
        Retrieve the embedding data for a chunk.

        Returns:
            A chunk with `doc_id`, `chunk_id`, and `embedding` set.
        """

        row_id = (chunk_id, Predicate(PredicateOperator.GT, -1))
        rows = await self._table.aget_partition(partition_id=doc_id, row_id=row_id)

        embedding = [row["vector"] for row in rows]

        return Chunk(doc_id=doc_id, chunk_id=chunk_id, embedding=embedding)

    async def get_chunk_data(
        self, doc_id: str, chunk_id: int, include_embedding: Optional[bool]
    ) -> Chunk:
        """
        Retrieve the text and metadata for a chunk.

        Returns:
            A chunk with `doc_id`, `chunk_id`, `text`, and `metadata` set.
        """

        row_id = (chunk_id, Predicate(PredicateOperator.EQ, -1))
        row = await self._table.aget(partition_id=doc_id, row_id=row_id)

        if include_embedding is True:
            embedded_chunk = await self.get_chunk_embedding(
                doc_id=doc_id, chunk_id=chunk_id
            )
            embedding = embedded_chunk.embedding
        else:
            embedding = None

        return Chunk(
            doc_id=doc_id,
            chunk_id=chunk_id,
            text=row["body_blob"],
            metadata=row["metadata"],
            embedding=embedding,
        )

    def close(self) -> None:
        """
        Cleans up any open resources.
        """
        pass
