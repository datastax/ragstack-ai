"""
This module provides an implementation of the BaseVectorStore abstract class, specifically designed
for use with a Cassandra database backend. It allows for the efficient storage and management of text embeddings
generated by a ColBERT model, facilitating scalable and high-relevancy retrieval operations.
"""

import logging
from typing import List, Optional, Set, Tuple

import cassio
from cassandra.cluster import ResponseFuture, Session
from cassio.table.query import Predicate, PredicateOperator
from cassio.table.tables import ClusteredMetadataVectorCassandraTable

from .base_database import BaseDatabase
from .constant import DEFAULT_COLBERT_DIM
from .objects import Chunk, Vector


class CassandraDatabase(BaseDatabase):
    """
    An implementation of the BaseDatabase abstract base class using Cassandra as the backend
    storage system. This class provides methods to store, retrieve, and manage text embeddings within
    a Cassandra database, specifically designed for handling vector embeddings generated by ColBERT.

    The table schema and custom index for ANN queries are automatically created if they do not exist.
    """

    _table: ClusteredMetadataVectorCassandraTable
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            raise ValueError(
                "This class cannot be instantiated directly. Please use the `from_astra()` or `from_session()` class methods."
            )
        return cls._instance

    @classmethod
    def from_astra(
        cls,
        database_id: str,
        astra_token: str,
        keyspace: Optional[str] = "default_keyspace",
        table_name: Optional[str] = "colbert",
        timeout: Optional[int] = 180,
    ):
        if cls._instance is None:
            cassio.init(token=astra_token, database_id=database_id, keyspace=keyspace)
            session = cassio.config.resolve_session()
            session.default_timeout = timeout

            return cls.from_session(
                session=session, keyspace=keyspace, table_name=table_name
            )
        return cls._instance

    @classmethod
    def from_session(
        cls,
        session: Session,
        keyspace: Optional[str] = "default_keyspace",
        table_name: Optional[str] = "colbert",
    ):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize(
                session=session, keyspace=keyspace, table_name=table_name
            )
        return cls._instance

    def _initialize(
        self,
        session: Session,
        keyspace: str,
        table_name: str,
    ):
        """
        Initializes a new instance of the CassandraVectorStore.

        Parameters:
            session (Session): The Cassandra session to use.
            keyspace (str): The keyspace in which the table exists or will be created.
            table_name (str): The name of the table to use or create for storing embeddings.
            timeout (int, optional): The default timeout in seconds for Cassandra operations. Defaults to 180.
        """

        try:
            is_astra = session.cluster.cloud
        except:
            is_astra = False

        logging.info(
            f"Cassandra store is running on {'AstraDB' if is_astra else 'Apache Cassandra'}."
        )

        self._table = ClusteredMetadataVectorCassandraTable(
            session=session,
            keyspace=keyspace,
            table=table_name,
            row_id_type=["INT", "INT"],
            vector_dimension=DEFAULT_COLBERT_DIM,
            vector_source_model="bert" if is_astra else None,
            vector_similarity_function=None if is_astra else "DOT_PRODUCT",
        )

    def add_chunks(self, chunks: List[Chunk]) -> List[Tuple[str, int]]:
        """
        Stores a list of embedded text chunks in the vector store

        Parameters:
            chunks (List[Chunk]): A list of `Chunk` instances to be stored.

        Returns:
            a list of tuples: (doc_id, chunk_id)
        """

        futures: List[Tuple[str, int, int, ResponseFuture]] = []

        for chunk in chunks:
            doc_id = chunk.doc_id
            chunk_id = chunk.chunk_id
            text = chunk.text
            metadata = chunk.metadata

            future = self._table.put_async(
                partition_id=doc_id,
                row_id=(chunk_id, -1),
                body_blob=text,
                metadata=metadata,
            )

            futures.append((doc_id, chunk_id, -1, future))

            for index, vector in enumerate(chunk.embedding):
                future = self._table.put_async(
                    partition_id=doc_id, row_id=(chunk_id, index), vector=vector
                )
                futures.append((doc_id, chunk_id, index, future))

            results: List[Tuple[str, int]] = []
            for doc_id, chunk_id, embedding_id, future in futures:
                try:
                    future.result()
                    results.append((doc_id, chunk_id))
                except Exception as e:
                    if embedding_id == -1:
                        logging.error(
                            f"issue inserting document data: {doc_id} chunk: {chunk_id}: {e}"
                        )
                    else:
                        logging.error(
                            f"issue inserting document embedding: {doc_id} chunk: {chunk_id} embedding: {embedding_id}: {e}"
                        )

        return results

    def delete_chunks(self, doc_ids: List[str]) -> None:
        """
        Deletes chunks from the vector store based on their document id.

        Parameters:
            doc_ids (List[str]): A list of document identifiers specifying the chunks to be deleted.

        Returns:
            True if the delete was successful.
        """

        futures = [
            (doc_id, self._table.delete_partition_async(partition_id=doc_id))
            for doc_id in doc_ids
        ]

        success = True
        for doc_id, future in futures:
            try:
                future.result()
            except Exception as e:
                success = False
                logging.error(f"issue on delete of document: {doc_id}: {e}")
        return success

    async def search_relevant_chunks(self, vector: Vector, n: int) -> List[Chunk]:
        """
        Retrieves 'n' ANN results for an embedded token vector.

        Returns:
            A list of Chunks with only `doc_id` and `chunk_id` set.
            Fewer than 'n' results may be returned.
        """

        chunks: Set[Chunk] = set()

        # TODO: only return partition_id and row_id after cassio supports this
        rows = await self._table.aann_search(vector=vector, n=n)
        for row in rows:
            chunks.add(
                Chunk(
                    doc_id=row["partition_id"],
                    chunk_id=row["row_id"][0],
                )
            )
        return list(chunks)

    async def get_chunk_embedding(self, doc_id: str, chunk_id: int) -> Chunk:
        """
        Retrieve the embedding data for a chunk.

        Returns:
            A chunk with `doc_id`, `chunk_id`, and `embedding` set.
        """

        row_id = (chunk_id, Predicate(PredicateOperator.GT, -1))
        rows = await self._table.aget_partition(partition_id=doc_id, row_id=row_id)

        embedding = [row["vector"] for row in rows]

        return Chunk(doc_id=doc_id, chunk_id=chunk_id, embedding=embedding)

    async def get_chunk_data(
        self, doc_id: str, chunk_id: int, include_embedding: Optional[bool]
    ) -> Chunk:
        """
        Retrieve the text and metadata for a chunk.

        Returns:
            A chunk with `doc_id`, `chunk_id`, `text`, and `metadata` set.
        """

        row_id = (chunk_id, Predicate(PredicateOperator.EQ, -1))
        row = await self._table.aget(partition_id=doc_id, row_id=row_id)

        if include_embedding is True:
            embedded_chunk = await self.get_chunk_embedding(
                doc_id=doc_id, chunk_id=chunk_id
            )
            embedding = embedded_chunk.embedding
        else:
            embedding = None

        return Chunk(
            doc_id=doc_id,
            chunk_id=chunk_id,
            text=row["body_blob"],
            metadata=row["metadata"],
            embedding=embedding,
        )

    def close(self) -> None:
        """
        Cleans up any open resources.
        """
        pass
