"""
This module provides an implementation of the BaseVectorStore abstract class, specifically designed
for use with a Cassandra database backend. It allows for the efficient storage and management of text embeddings
generated by a ColBERT model, facilitating scalable and high-relevancy retrieval operations.
"""

import logging
from typing import Any, Dict, List, Optional, Set, Tuple

import torch
from cassandra.cluster import ResponseFuture, Session
from cassio.table.query import Predicate, PredicateOperator
from cassio.table.tables import ClusteredMetadataVectorCassandraTable
from torch import Tensor

from .base_vector_store import BaseVectorStore
from .objects import BaseChunk, ChunkData, EmbeddedChunk


class CassandraVectorStore(BaseVectorStore):
    """
    An implementation of the BaseVectorStore abstract base class using Cassandra as the backend
    storage system. This class provides methods to store, retrieve, and manage text embeddings within
    a Cassandra database, specifically designed for handling vector embeddings generated by ColBERT.

    Attributes:
        session (Session): The Cassandra session instance used for database operations.
        keyspace (str): The name of the Cassandra keyspace to use.
        table_name (str): The name of the table within the keyspace for storing chunk embeddings.
        full_table_name (str): The full name of the table (including keyspace) for queries.
        insert_chunk_stmt (PreparedStatement): Prepared statement for inserting text chunks.
        insert_colbert_stmt (PreparedStatement): Prepared statement for inserting embeddings.
        query_colbert_ann_stmt (PreparedStatement): Prepared statement for ANN queries.
        query_colbert_chunks_stmt (PreparedStatement): Prepared statement for retrieving chunks.
        query_chunk_stmt (PreparedStatement): Prepared statement for retrieving chunk bodies.
        delete_chunks_by_doc_id_stmt (PreparedStatement): Prepared statement for deleting chunks.

    The table schema and custom index for ANN queries are automatically created if they do not exist.
    """

    _table: ClusteredMetadataVectorCassandraTable

    def __init__(
        self, session: Session, keyspace: str, table_name: str, timeout: int = 180
    ):
        """
        Initializes a new instance of the CassandraVectorStore.

        Parameters:
            session (Session): The Cassandra session to use.
            keyspace (str): The keyspace in which the table exists or will be created.
            table_name (str): The name of the table to use or create for storing embeddings.
            timeout (int, optional): The default timeout in seconds for Cassandra operations. Defaults to 180.
        """

        cluster_name = session.cluster.metadata.cluster_name.lower()
        is_astra = "cndb" == cluster_name
        logging.debug(f"colbert store is running on {'astra' if is_astra else 'apache cassandra'}")

        session.default_timeout = timeout

        self._table = ClusteredMetadataVectorCassandraTable(
            session=session,
            keyspace=keyspace,
            table=table_name,
            row_id_type=["INT", "INT"],
            vector_dimension=128,
            vector_source_model="bert" if is_astra else None,
            vector_similarity_function=None if is_astra else "DOT_PRODUCT",
        )

    def put_chunks(
        self, chunks: List[EmbeddedChunk], delete_existing: Optional[bool] = False
    ) -> None:
        """
        Stores a list of EmbeddedChunk instances in the Cassandra database, managing both the text
        body and the embeddings of each chunk. Optionally deletes existing chunks for each document
        before insertion.

        Parameters:
            chunks (List[EmbeddedChunk]): A list of EmbeddedChunk instances to store.
            delete_existing (Optional[bool]): A flag indicating whether to delete existing chunks for the
                                              documents related to the chunks being inserted.
        """

        if delete_existing:
            doc_ids = [c.doc_id for c in chunks]
            self.delete_chunks(list(set(doc_ids)))

        futures: List[Tuple[str, int, int, ResponseFuture]] = []

        for chunk in chunks:
            doc_id = chunk.doc_id
            chunk_id = chunk.chunk_id
            text = chunk.data.text
            metadata = {} if len(chunk.data.metadata) == 0 else chunk.data.metadata

            future = self._table.put_async(partition_id=doc_id, row_id=(chunk_id, -1), body_blob=text, metadata=metadata)

            futures.append((doc_id, chunk_id, -1, future))

            for index, vector in enumerate(chunk.embeddings.tolist()):
                future = self._table.put_async(partition_id=doc_id, row_id=(chunk_id, index), vector=vector)
                futures.append((doc_id, chunk_id, index, future))

            for (doc_id, chunk_id, embedding_id, future) in futures:
                try:
                    future.result()
                except Exception as e:
                    if embedding_id == -1:
                        logging.error(f"issue inserting document data: {doc_id} chunk: {chunk_id}: {e}")
                    else:
                        logging.error(f"issue inserting document embedding: {doc_id} chunk: {chunk_id} embedding: {embedding_id}: {e}")

    def delete_chunks(self, doc_ids: List[str]) -> None:
        """
        Deletes all chunks associated with the specified document identifiers.

        Parameters:
            doc_ids (List[str]): A list of document identifiers whose chunks should be deleted.
        """

        futures = [(doc_id, self._table.delete_partition_async(partition_id = doc_id)) for doc_id in doc_ids]

        for (doc_id, future) in futures:
            try:
                future.result()
            except Exception as e:
                logging.error(f"issue on delete of document: {doc_id}: {e}")

    async def search_relevant_chunks(self, vector: List[float], n: int) -> List[BaseChunk]:
        """
        Retrieves 'n' ANN results for an embedded token vector.

        Returns:
            A set of tuples of (doc_id, chunk_id). Fewer than 'n' results may be returned.
        """

        chunks: Set[BaseChunk] = set()

        # TODO: only return partition_id and row_id after cassio supports this
        rows = await self._table.aann_search(vector=vector, n=n)
        for row in rows:
            chunks.add(BaseChunk(
                doc_id=row["partition_id"],
                chunk_id=row["row_id"][0],
            ))
        return list(chunks)

    async def get_chunk_embeddings(self, chunk: BaseChunk) -> Tuple[BaseChunk, List[Tensor]]:
        """
        Retrieve all the embedding data for a chunk.

        Returns:
            A tuple where the first value is BaseChunk, and the second
        value is the list of embeddings for the chunk.
        """

        row_id = (chunk.chunk_id, Predicate(PredicateOperator.GT, -1))
        rows = await self._table.aget_partition(partition_id=chunk.doc_id, row_id=row_id)

        embeddings = [torch.Tensor(row["vector"]) for row in rows]

        return (chunk, embeddings)

    async def get_chunk_data(self, chunk: BaseChunk) -> Tuple[BaseChunk, ChunkData]:
        """
        Fetches the text for a given chunk.

        Returns:
            Tuple containing the chunk, and chunk_data
        """

        row_id = (chunk.chunk_id, Predicate(PredicateOperator.EQ, -1))
        row = await self._table.aget(partition_id=chunk.doc_id, row_id=row_id)

        chunk_data = ChunkData(
            text=row["body_blob"],
            metadata=row["metadata"]
        )
        return (chunk, chunk_data)

    def close(self) -> None:
        """
        Closes the Cassandra session and any other resources. This method should be overridden to
        ensure proper cleanup if necessary.
        """
        pass
