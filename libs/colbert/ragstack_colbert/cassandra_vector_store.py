"""
This module provides an implementation of the BaseVectorStore abstract class, specifically designed
for use with a Cassandra database backend. It allows for the efficient storage and management of text embeddings
generated by a ColBERT model, facilitating scalable and high-relevancy retrieval operations.
"""

import logging
from typing import List, Optional, Tuple, Set, Dict, Any
from torch import Tensor
import torch

from cassandra.cluster import Session, ResponseFuture
from cassandra.concurrent import execute_concurrent_with_args

from .base_embedding import EmbeddedChunk
from .base_vector_store import BaseVectorStore
from cassio.table.tables import ClusteredMetadataVectorCassandraTable
from cassio.table.query import Predicate, PredicateOperator

from .token_embedding import EmbeddedChunk
from .vector_store import ColbertVectorStore


class CassandraVectorStore(BaseVectorStore):
    """
    An implementation of the BaseVectorStore abstract base class using Cassandra as the backend
    storage system. This class provides methods to store, retrieve, and manage text embeddings within
    a Cassandra database, specifically designed for handling vector embeddings generated by ColBERT.

    Attributes:
        session (Session): The Cassandra session instance used for database operations.
        keyspace (str): The name of the Cassandra keyspace to use.
        table_name (str): The name of the table within the keyspace for storing chunk embeddings.
        full_table_name (str): The full name of the table (including keyspace) for queries.
        insert_chunk_stmt (PreparedStatement): Prepared statement for inserting text chunks.
        insert_colbert_stmt (PreparedStatement): Prepared statement for inserting embeddings.
        query_colbert_ann_stmt (PreparedStatement): Prepared statement for ANN queries.
        query_colbert_chunks_stmt (PreparedStatement): Prepared statement for retrieving chunks.
        query_chunk_stmt (PreparedStatement): Prepared statement for retrieving chunk bodies.
        delete_chunks_by_doc_id_stmt (PreparedStatement): Prepared statement for deleting chunks.

    The table schema and custom index for ANN queries are automatically created if they do not exist.
    An implementation of the ColbertVectorStore abstract base class using CassIO as the backend
    storage system.
    """

    _table: ClusteredMetadataVectorCassandraTable

    def __init__(
        self, session: Session, keyspace: str, table_name: str, timeout: int = 180
    ):
        """
        Initializes a new instance of the CassandraVectorStore.

        Parameters:
            session (Session): The Cassandra session to use.
            keyspace (str): The keyspace in which the table exists or will be created.
            table_name (str): The name of the table to use or create for storing embeddings.
            timeout (int, optional): The default timeout in seconds for Cassandra operations. Defaults to 180.
        """

        cluster_name = session.cluster.metadata.cluster_name.lower()
        is_astra = "cndb" == cluster_name
        logging.debug(f"colbert store is running on {'astra' if is_astra else 'cassandra'}")

        self._table = ClusteredMetadataVectorCassandraTable(
            session=session,
            keyspace=keyspace,
            table=table_name,
            row_id_type=["INT", "INT"],
            vector_dimension=128,
            vector_source_model="bert" if is_astra else None,
            vector_similarity_function=None if is_astra else "DOT_PRODUCT",
        )

    def put_chunks(
        self, chunks: List[EmbeddedChunk], delete_existing: Optional[bool] = False
    ) -> None:
        """
        Stores a list of EmbeddedChunk instances in the Cassandra database, managing both the text
        body and the embeddings of each chunk. Optionally deletes existing chunks for each document
        before insertion.

        Parameters:
            chunks (List[EmbeddedChunk]): A list of EmbeddedChunk instances to store.
            delete_existing (Optional[bool]): A flag indicating whether to delete existing chunks for the
                                              documents related to the chunks being inserted.
        """

        if delete_existing:
            doc_ids = [c.doc_id for c in chunks]
            self.delete_chunks(list(set(doc_ids)))

        futures: List[Tuple[str, int, int, ResponseFuture]] = []

        for chunk in chunks:
            doc_id = chunk.doc_id
            chunk_id = chunk.chunk_id

            future = self._table.put_async(partition_id=doc_id, row_id=(chunk_id, -1), body_blob=chunk.text, metadata=chunk.metadata)

            futures.append((doc_id, chunk_id, -1, future))

            for index, vector in enumerate(chunk.embeddings.tolist()):
                future = self._table.put_async(partition_id=doc_id, row_id=(chunk_id, index), vector=vector)
                futures.append((doc_id, chunk_id, index, future))

            for (doc_id, chunk_id, embedding_id, future) in futures:
                try:
                    future.result()
                except Exception as e:
                    if embedding_id == -1:
                        logging.error(f"issue inserting document data: {doc_id} chunk: {chunk_id}: {e}")
                    else:
                        logging.error(f"issue inserting document embedding: {doc_id} chunk: {chunk_id} embedding: {embedding_id}: {e}")

    def delete_chunks(self, doc_ids: List[str]) -> None:
        """
        Deletes all chunks associated with the specified document identifiers.

        Parameters:
            doc_ids (List[str]): A list of document identifiers whose chunks should be deleted.
        """

        futures = [(doc_id, self._table.delete_partition_async(partition_id = doc_id)) for doc_id in doc_ids]

        for (doc_id, future) in futures:
            try:
                future.result()
            except Exception as e:
                logging.error(f"issue on delete of document: {doc_id}: {e}")

    async def get_relevant_chunks(self, vector: List[float], n: int) -> List[Tuple[str, int]]:
        """
        Retrieves 'n' ANN results for an embedded token vector.

        Returns:
            A set of tuples of (doc_id, chunk_id). Fewer than 'n' results may be returned.
        """

        chunks: Set[Tuple[str, int]] = set()

        # TODO: only return partition_id and row_id after cassio supports this
        rows = await self._table.aann_search(vector=vector, n=n)
        for row in rows:
            print(row)
            doc_id = row["partition_id"]
            chunk_id = row["row_id"][0]

            chunks.add((doc_id, chunk_id))
        return chunks

    async def get_chunk_embeddings(self, doc_id: str, chunk_id: int) -> Tuple[Tuple[str, int], List[Tensor]]:
        """
        Retrieve all the embedding data for a chunk.

        Returns:
            A tuple where the first value is a tuple of (doc_id, chunk_id), and the second
        value is the list of embeddings for the chunk.
        """

        row_id = (chunk_id, Predicate(PredicateOperator.GT, -1))
        rows = await self._table.aget_partition(partition_id=doc_id, row_id=row_id)

        embeddings = [torch.Tensor(row.vector) for row in rows]

        return ((doc_id, chunk_id), embeddings)

    async def get_chunk_text_and_metadata(self, doc_id: str, chunk_id: int) -> Tuple[str, int, str, Dict[str, Any]]:
        """
        Fetches the text for a given doc_id and chunk_id.

        Returns:
            Tuple containing the doc_id, chunk_id, text, and metadata
        """

        row_id = (chunk_id, Predicate(PredicateOperator.EQ, -1))
        row = await self._table.aget(partition_id=doc_id, row_id=row_id)

        return (doc_id, chunk_id, row["body_blob"], row["metadata"])

    def close(self) -> None:
        """
        Closes the Cassandra session and any other resources. This method should be overridden to
        ensure proper cleanup if necessary.
        """
        pass
