{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates the use of a _Hybrid Graph Store_.\n",
    "This combines the benefits of a traditional vector store (locating nodes by vector similarity) with the benefits of a graph graph (connecting relevant but not necessarily similar information).\n",
    "\n",
    "It demonstrates loading a PDF, chunking it and writing it to the Graph Store using the standard LangChain patterns.\n",
    "The only addition is the extraction of \"keywords\" using [keybert](https://maartengr.github.io/KeyBERT/index.html).\n",
    "This demonstrates how chunks may be linked.\n",
    "\n",
    "Other ways that chunks could be linked:\n",
    "\n",
    "- Using TF-IDF to compute keywords from chunks, rather than keybert.\n",
    "- Using links (`<a href=\"...\">`) in the content and associated URLs to connect explicit links. This would even work with anchors within a page!\n",
    "- Connecting images and tables on a page to the other content on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) When developing locally, this reloads the module code\n",
    "# when changes are made, making it easier to iterate.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Required in Colab) Install the graph store library from the repository.\n",
    "# This will also install the dependencies.\n",
    "%pip install ragstack-ai-knowledge-store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the following.\n",
    "1. If you're just running the notebook, it's probably best to run the cell using `getpass` to set the necessary\n",
    "   environment variables.\n",
    "1. If you're developing, it's likely easiest to create a `.env` file and store the necessary credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Option 1) - Set the environment variables from getpass.\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "os.environ[\"ASTRA_DB_DATABASE_ID\"] = input(\"Enter Astra DB Database ID: \")\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass.getpass(\n",
    "    \"Enter Astra DB Application Token: \"\n",
    ")\n",
    "\n",
    "keyspace = input(\"Enter Astra DB Keyspace (Empty for default): \")\n",
    "if keyspace:\n",
    "    os.environ[\"ASTRA_DB_KEYSPACE\"] = keyspace\n",
    "else:\n",
    "    os.environ.pop(\"ASTRA_DB_KEYSPACE\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Option 2) - Load the `.env` file.\n",
    "# See `env.template` for an example of what you should have there.\n",
    "%pip install python-dotenv\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Astra DB Graph Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cassandra connection from environment variables).\n",
    "import cassio\n",
    "\n",
    "cassio.init(auto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph store.\n",
    "from ragstack_langchain.graph_store import CassandraGraphStore\n",
    "\n",
    "graph_store = CassandraGraphStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Documents\n",
    "In this section we ingest documents to the hybrid graph store.\n",
    "We'll use `keybert` for extracting keywords which will automatically link between chunks with common keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pypdf langchain-text-splitters keybert langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=64,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "pages = loader.load_and_split(text_splitter)\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(\n",
    "    [doc.page_content for doc in pages], stop_words=\"english\"\n",
    ")\n",
    "\n",
    "for doc, kws in zip(pages, keywords):\n",
    "    # Consider only taking keywords within a certain distance?\n",
    "    doc.metadata[\"keywords\"] = [kw for (kw, _) in kws]\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store.add_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "In this section, we'll set up a retrieval chain using the graph store.\n",
    "\n",
    "We can configure how many chunks are retrieved by the vector search as well as how deep to traverse the keyword edges.\n",
    "If we traverse to depth 0, the hybrid graph store is equivalent to a vector store.\n",
    "Using a depth of 1 or 2 we are able to retrieve related, but dissimilar chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever0 = graph_store.as_retriever(depth=0)\n",
    "retriever1 = graph_store.as_retriever(depth=1)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain0 = (\n",
    "    {\"context\": retriever0 | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain1 = (\n",
    "    {\"context\": retriever1 | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain0.invoke(\"How does LayoutParser work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain1.invoke(\"How does LayoutParser work?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-framework-aiP65pJh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
