{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdownify in /Users/benjamin.chambers/Library/Caches/pypoetry/virtualenvs/agent-framework-aiP65pJh-py3.11/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/benjamin.chambers/Library/Caches/pypoetry/virtualenvs/agent-framework-aiP65pJh-py3.11/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /Users/benjamin.chambers/Library/Caches/pypoetry/virtualenvs/agent-framework-aiP65pJh-py3.11/lib/python3.11/site-packages (from markdownify) (4.12.2)\n",
      "Requirement already satisfied: six<2,>=1.15 in /Users/benjamin.chambers/Library/Caches/pypoetry/virtualenvs/agent-framework-aiP65pJh-py3.11/lib/python3.11/site-packages (from markdownify) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/benjamin.chambers/Library/Caches/pypoetry/virtualenvs/agent-framework-aiP65pJh-py3.11/lib/python3.11/site-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install markdownify python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Astra Documentation into Knowledge Store\n",
    "\n",
    "First, we'll crawl the DataStax documentation. LangChain includes a `SiteMapLoader` but it loads all of the pages into memory simultaneously, which makes it impossible to index larger sites from small environments (such as CoLab). So, we'll scrape the sitemap ourselves and iterate over the URLs, allowing us to process documents in batches and flush them to Astra DB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the URLs from the Site Maps\n",
    "First, we use Beautiful Soup to parse the XML content of each sitemap and get the list of URLs.\n",
    "We also add a few extra URLs for external sites that are also useful to include in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sitemaps to crawl the content\n",
    "SITEMAPS = [\n",
    "    \"https://docs.datastax.com/en/sitemap-astra-db-vector.xml\",\n",
    "    \"https://docs.datastax.com/en/sitemap-cql.xml\",\n",
    "    \"https://docs.datastax.com/en/sitemap-dev-app-drivers.xml\",\n",
    "    \"https://docs.datastax.com/en/sitemap-glossary.xml\",\n",
    "    \"https://docs.datastax.com/en/sitemap-astra-db-serverless.xml\"\n",
    "]\n",
    "\n",
    "# Additional URLs to crawl for content.\n",
    "EXTRA_URLS = [\n",
    "    \"https://github.com/jbellis/jvector\"\n",
    "]\n",
    "\n",
    "SITE_PREFIX = \"astra\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def load_pages(sitemap_url):\n",
    "    r = requests.get(sitemap_url,\n",
    "                     headers={\n",
    "                         # Astra docs only return a sitemap with a user agent set.\n",
    "                         \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:58.0) Gecko/20100101 Firefox/58.0\",\n",
    "                     })\n",
    "    xml = r.text\n",
    "\n",
    "    soup = BeautifulSoup(xml, features=\"xml\")\n",
    "    url_tags = soup.find_all(\"url\")\n",
    "    for url in url_tags:\n",
    "        yield(url.find(\"loc\").text)\n",
    "\n",
    "\n",
    "# For maintenance purposes, we could check only the new articles since a given time.\n",
    "URLS = [\n",
    "    url\n",
    "    for sitemap_url in SITEMAPS\n",
    "    for url in load_pages(sitemap_url)\n",
    "] + EXTRA_URLS\n",
    "len(URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the content from each URL\n",
    "Next, we create the code to load each page. This performs the following steps:\n",
    "\n",
    "1. Parses the HTML with BeautifulSoup\n",
    "2. Locates the \"content\" of the HTML using an appropriate selector based on the URL\n",
    "3. Find the link (`<a href=\"...\">`) tags in the content and collect the absolute URLs (for creating edges).\n",
    "\n",
    "Adding the URLs of these references to the metadata allows the knowledge store to create edges between the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document\n",
    "from typing import AsyncIterator, Iterable, Set\n",
    "from markdownify import MarkdownConverter\n",
    "from urllib.parse import urlparse, urljoin, urldefrag\n",
    "\n",
    "def parse_url(link,\n",
    "              page_url,\n",
    "              drop_fragment: bool = True):\n",
    "  href = link.get('href')\n",
    "  if href is None:\n",
    "    return None\n",
    "  url = urlparse(href)\n",
    "  if url.scheme not in [\"http\", \"https\", \"\"]:\n",
    "    return None\n",
    "\n",
    "  # Join the HREF with the page_url to convert relative paths to absolute.\n",
    "  url = urljoin(page_url, href)\n",
    "\n",
    "  # Fragments would be useful if we chunked a page based on section.\n",
    "  # Then, each chunk would have a different URL based on the fragment.\n",
    "  # Since we aren't doing that yet, they just \"break\" links. So, drop\n",
    "  # the fragment.\n",
    "  if drop_fragment:\n",
    "    return urldefrag(url).url\n",
    "  else:\n",
    "     return url\n",
    "\n",
    "def parse_hrefs(soup: BeautifulSoup, url: str) -> Set[str]:\n",
    "  links = soup.find_all('a')\n",
    "  links = {parse_url(link, page_url=url) for link in links}\n",
    "\n",
    "  # Remove entries for any 'a' tag that failed to parse (didn't have href,\n",
    "  # or invalid domain, etc.)\n",
    "  links.discard(None)\n",
    "\n",
    "  # Remove self links.\n",
    "  links.discard(url)\n",
    "\n",
    "  return links\n",
    "\n",
    "def locate_content(soup: BeautifulSoup, url: str) -> BeautifulSoup:\n",
    "    content = None\n",
    "    if url.startswith(\"https://docs.datastax.com/en/\"):\n",
    "        content = soup.select_one(\"article.doc\")\n",
    "    if url.startswith(\"https://github.com\"):\n",
    "        content = soup.select_one(\"article.entry-content\")\n",
    "    assert content is not None, f\"Unable to locate content for {url}\"\n",
    "    return content\n",
    "\n",
    "markdown_converter = MarkdownConverter(autolinks_false=\"false\", heading_style=\"ATX\")\n",
    "\n",
    "def process_document(html: Document) -> Document:\n",
    "    url = html.metadata[\"source\"]\n",
    "    soup = BeautifulSoup(html.page_content, \"html.parser\")\n",
    "    content = locate_content(soup, url)\n",
    "\n",
    "    content_md = markdown_converter.convert_soup(content)\n",
    "    hrefs = parse_hrefs(content, url)\n",
    "    return Document(\n",
    "        page_content = content_md,\n",
    "        metadata = {\n",
    "            # Assign the unique ID for the `Document` in the graph.\n",
    "            \"content_id\": url,\n",
    "            # This document references all documents with matching urls.\n",
    "            \"hrefs\": hrefs,\n",
    "            # These are the URLs the document is \"defined\" at.\n",
    "            \"urls\": [url]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "async def load_pages(urls: Iterable[str]) -> AsyncIterator[Document]:\n",
    "    loader = AsyncHtmlLoader(\n",
    "        urls,\n",
    "        requests_per_second=4,\n",
    "        # Astra docs require a user agent\n",
    "        header_template = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:58.0) Gecko/20100101 Firefox/58.0\"\n",
    "        }\n",
    "    )\n",
    "    async for html in loader.alazy_load():\n",
    "        yield process_document(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment\n",
    "Before we initialize the Knowledge Store and write the documents we need to set some environment variables.\n",
    "In colab, this will prompt you for input. When running locally, this will load from `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in colab. Loading '.env' (see 'env.template' for example)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    # (Option 1) - Set the environment variables from getpass.\n",
    "    print(\"In colab. Using getpass/input for environment variables.\")\n",
    "    import getpass\n",
    "    import os\n",
    "\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "    os.environ[\"ASTRA_DB_DATABASE_ID\"] = input(\"Enter Astra DB Database ID: \")\n",
    "    os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass.getpass(\"Enter Astra DB Application Token: \")\n",
    "\n",
    "    keyspace = input(\"Enter Astra DB Keyspace (Empty for default): \")\n",
    "    if keyspace:\n",
    "        os.environ[\"ASTRA_DB_KEYSPACE\"] = keyspace\n",
    "    else:\n",
    "        os.environ.pop(\"ASTRA_DB_KEYSPACE\", None)\n",
    "else:\n",
    "    print(\"Not in colab. Loading '.env' (see 'env.template' for example)\")\n",
    "    import dotenv\n",
    "    dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Cassio and Knowledge Store\n",
    "With the environment variables set we initialize the Cassio library for talking to Cassandra / Astra DB.\n",
    "We also create the `KnowledgeStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x12b7161d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = input(\"Drop Tables? [(Y)es/(N)o]\")\n",
    "if answer.lower() in [\"y\",\"yes\"]:\n",
    "    import cassio\n",
    "    cassio.init(auto=True)\n",
    "    from cassio.config import check_resolve_session, check_resolve_keyspace\n",
    "    session = check_resolve_session()\n",
    "    keyspace = check_resolve_keyspace()\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {keyspace}.{SITE_PREFIX}_nodes\")\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {keyspace}.{SITE_PREFIX}_edges\")\n",
    "else:\n",
    "     # Handle no / \"wrong\" input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragstack_knowledge_store import CassandraKnowledgeStore\n",
    "from ragstack_knowledge_store.directed_edge_extractor import DirectedEdgeExtractor\n",
    "\n",
    "cassio.init(auto=True)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "SITE_PREFIX=\"astra_docs\"\n",
    "knowledge_store = CassandraKnowledgeStore(\n",
    "    embeddings,\n",
    "    edge_extractors = [\n",
    "        DirectedEdgeExtractor.for_hrefs_to_urls(),\n",
    "    ],\n",
    "    node_table=f\"{SITE_PREFIX}_nodes\",\n",
    "    edge_table=f\"{SITE_PREFIX}_edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Documents\n",
    "Finally, we fetch pages and write them to the knowledge store in batches of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1368/1368 [04:24<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 (of 1368) URLs were not found\n"
     ]
    }
   ],
   "source": [
    "not_found = 0\n",
    "found = 0\n",
    "\n",
    "docs = []\n",
    "async for doc in load_pages(URLS):\n",
    "    if doc.page_content.startswith(\"\\n# Page Not Found\"):\n",
    "        not_found += 1\n",
    "        continue\n",
    "\n",
    "    docs.append(doc)\n",
    "    found += 1\n",
    "\n",
    "    if len(docs) >= 50:\n",
    "        knowledge_store.add_documents(docs)\n",
    "        docs.clear()\n",
    "\n",
    "if docs:\n",
    "    knowledge_store.add_documents(docs)\n",
    "print(f\"{not_found} (of {not_found + found}) URLs were not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and execute the RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "template = \"\"\"You are a helpful technical support bot. You should provide complete answers explaining the options the user has available to address their problem. Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    formatted = \"\\n\\n\".join(f\"From {doc.metadata['content_id']}: {doc.page_content}\" for doc in docs)\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following question. This is an interesting question because the ideal answer should be concise and in-depth, based on how the vector indexing is actually implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION=\"What vector indexing algorithms does Astra use?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Helper method to render markdown in responses to a chain.\n",
    "def run_and_render(chain, question):\n",
    "    result = chain.invoke(question)\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector-Only Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth 0 doesn't traverses edges and is equivalent to vector similarity only.\n",
    "vector_retriever = knowledge_store.as_retriever(search_kwargs={\"depth\": 0})\n",
    "\n",
    "vector_rag_chain = (\n",
    "    {\"context\": vector_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Astra DB uses multiple indexing techniques to speed up searches in its vector databases. The key vector indexing algorithms used are:\n",
       "\n",
       "1. **JVector**: Astra DB Serverless (Vector) databases utilize the JVector vector search engine to construct a graph index. This engine enables efficient search operations by adding new documents to the graph immediately. JVector also supports vector compression through quantization to save space and improve performance. For more details, you can visit the [JVector GitHub page](https://github.com/jbellis/jvector).\n",
       "\n",
       "2. **Storage-Attached Index (SAI)**: This indexing technique is used to efficiently find rows that satisfy query predicates. Astra DB provides numeric, text, and vector-based indexes to support various types of searches. SAI helps in loading a superset of all possible results from storage based on the provided predicates, evaluating the search criteria, and sorting the results by vector similarity. The top results are then returned to the user. For more information, see the [Storage-Attached Indexing (SAI) Overview](https://docs.datastax.com/en/cql/astra/developing/indexing/sai/sai-overview.html).\n",
       "\n",
       "These indexing techniques help in optimizing the performance and efficiency of vector searches in Astra DB."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_and_render(vector_rag_chain, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Traversal Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth 1 does vector similarity and then traverses 1 level of edges.\n",
    "graph_retriever = knowledge_store.as_retriever(search_kwargs={\"depth\": 1})\n",
    "\n",
    "graph_rag_chain = (\n",
    "    {\"context\": graph_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Astra DB Serverless uses multiple indexing techniques to optimize vector searches. Here are the key indexing methods employed:\n",
       "\n",
       "1. **JVector**: Astra uses the [JVector vector search engine](https://github.com/jbellis/jvector) to construct a graph index. JVector builds a graph-based index that enables efficient and scalable vector searches. When new documents are added, JVector immediately integrates them into the graph, allowing for prompt search capabilities. JVector also supports vector compression with techniques like quantization to save space and improve performance.\n",
       "\n",
       "2. **Storage-Attached Index (SAI)**: SAI is another indexing technique used by Astra DB to efficiently search rows that satisfy query predicates. Astra DB provides numeric-, text-, and vector-based indexes to support various search operations. SAI can be customized according to specific requirements, such as a particular similarity function or text transformation. When a search operation is executed, SAI loads a superset of potential results from storage based on the provided predicates and then evaluates and sorts these results by vector similarity. The top `limit` results are returned to the user.\n",
       "\n",
       "These indexing techniques help Astra DB Serverless perform efficient and scalable vector searches, ensuring that queries return relevant results quickly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_and_render(graph_rag_chain, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMR Graph Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_graph_retriever = knowledge_store.as_retriever(\n",
    "    search_type = \"mmr_traversal\",\n",
    "    search_kwargs = {\n",
    "        \"k\": 4,\n",
    "        \"fetch_k\": 10,\n",
    "        \"depth\": 2,\n",
    "        # \"score_threshold\": 0.2,\n",
    "    },\n",
    ")\n",
    "\n",
    "mmr_graph_rag_chain = (\n",
    "    {\"context\": mmr_graph_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Astra DB Serverless uses multiple indexing techniques to speed up vector searches, with a focus on efficient and scalable solutions. Specifically, Astra employs the following vector indexing algorithms:\n",
       "\n",
       "1. **JVector**:\n",
       "   - **Graph Index**: Astra DB uses the [JVector vector search engine](https://github.com/jbellis/jvector) to construct a graph index. JVector is a graph-based index that builds on the DiskANN design with composable extensions.\n",
       "   - **Graph-based Indexes**: These indexes are simpler to implement and typically faster. They can be constructed and updated incrementally, making them more suitable for general-purpose indexing compared to partition-based approaches.\n",
       "   - **On-Disk Adjacency List**: The graph is represented by an on-disk adjacency list per node, with additional data stored inline to support two-pass searches.\n",
       "   - **Two-Pass Searches**: The first pass uses lossily compressed representations of the vectors kept in memory, and the second pass uses a more accurate representation read from disk.\n",
       "\n",
       "2. **Product Quantization (PQ)**:\n",
       "   - **Compression**: JVector can compress vectors using product quantization to save space and improve performance.\n",
       "   - **Anisotropic Weighting**: Optionally used with product quantization to improve recall.\n",
       "\n",
       "3. **Storage-Attached Index (SAI)**:\n",
       "   - **Efficient Searching**: SAI is an indexing technique used to efficiently find rows that satisfy query predicates. It supports numeric-, text-, and vector-based indexes.\n",
       "   - **Customization**: You can customize indexes based on specific requirements, such as using a specific similarity function or text transformation.\n",
       "\n",
       "These indexing techniques enable Astra DB Serverless to provide efficient and scalable vector search capabilities, suitable for various high-dimensional data scenarios often encountered in machine learning and AI applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_and_render(mmr_graph_rag_chain, QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector [0]:    https://docs.datastax.com/en/astra-db-serverless/get-started/concepts.html\n",
      "Vector [1]:    https://docs.datastax.com/en/cql/astra/getting-started/vector-search-quickstart.html\n",
      "Vector [2]:    https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html\n",
      "Vector [3]:    https://docs.datastax.com/en/astra-db-serverless/get-started/astra-db-introduction.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph [0]:     https://docs.datastax.com/en/astra-db-serverless/get-started/concepts.html\n",
      "Graph [1]:     https://docs.datastax.com/en/cql/astra/getting-started/vector-search-quickstart.html\n",
      "Graph [2]:     https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html\n",
      "Graph [3]:     https://docs.datastax.com/en/astra-db-serverless/get-started/astra-db-introduction.html\n",
      "Graph [4]:     https://docs.datastax.com/en/astra-db-serverless/databases/load-data.html\n",
      "Graph [5]:     https://docs.datastax.com/en/astra-db-serverless/databases/manage-collections.html\n",
      "Graph [6]:     https://docs.datastax.com/en/astra-db-serverless/integrations/embedding-providers/azure-openai.html\n",
      "Graph [7]:     https://docs.datastax.com/en/astra-db-serverless/integrations/embedding-providers/nvidia-nemo.html\n",
      "Graph [8]:     https://docs.datastax.com/en/astra-db-serverless/integrations/embedding-providers/openai.html\n",
      "Graph [9]:     https://docs.datastax.com/en/astra-db-serverless/databases/database-overview.html\n",
      "Graph [10]:     https://docs.datastax.com/en/astra-db-serverless/integrations/semantic-kernel.html\n",
      "Graph [11]:     https://docs.datastax.com/en/astra-db-serverless/tutorials/chatbot.html\n",
      "Graph [12]:     https://docs.datastax.com/en/astra-db-serverless/tutorials/recommendations.html\n",
      "Graph [13]:     https://docs.datastax.com/en/cql/astra/developing/indexing/sai/sai-overview.html\n",
      "Graph [14]:     https://docs.datastax.com/en/glossary/index.html\n",
      "Graph [15]:     https://github.com/jbellis/jvector\n",
      "Graph [16]:     https://docs.datastax.com/en/astra-db-serverless/administration/configure-sso.html\n",
      "Graph [17]:     https://docs.datastax.com/en/astra-db-serverless/administration/customer-keys-overview.html\n",
      "Graph [18]:     https://docs.datastax.com/en/astra-db-serverless/administration/manage-application-tokens.html\n",
      "Graph [19]:     https://docs.datastax.com/en/astra-db-serverless/administration/manage-database-access.html\n",
      "Graph [20]:     https://docs.datastax.com/en/astra-db-serverless/administration/manage-database-ip-access-list.html\n",
      "Graph [21]:     https://docs.datastax.com/en/astra-db-serverless/api-reference/devops-api.html\n",
      "Graph [22]:     https://docs.datastax.com/en/astra-db-serverless/api-reference/overview.html\n",
      "Graph [23]:     https://docs.datastax.com/en/astra-db-serverless/cli-reference/astra-cli.html\n",
      "Graph [24]:     https://docs.datastax.com/en/astra-db-serverless/cql/develop-with-cql.html\n",
      "Graph [25]:     https://docs.datastax.com/en/astra-db-serverless/databases/change-data-capture.html\n",
      "Graph [26]:     https://docs.datastax.com/en/astra-db-serverless/databases/connection-methods-comparison.html\n",
      "Graph [27]:     https://docs.datastax.com/en/astra-db-serverless/databases/vector-search.html\n",
      "Graph [28]:     https://docs.datastax.com/en/astra-db-serverless/get-started/examples.html\n",
      "Graph [29]:     https://docs.datastax.com/en/astra-db-serverless/get-started/quickstart.html\n",
      "Graph [30]:     https://docs.datastax.com/en/astra-db-serverless/integrations/integrations-overview.html\n",
      "MMR Graph [0]: https://docs.datastax.com/en/astra-db-serverless/get-started/concepts.html\n",
      "MMR Graph [1]: https://docs.datastax.com/en/astra-db-serverless/cli-reference/astra-cli.html\n",
      "MMR Graph [2]: https://github.com/jbellis/jvector\n",
      "MMR Graph [3]: https://docs.datastax.com/en/cql/astra/developing/indexing/indexing-concepts.html\n"
     ]
    }
   ],
   "source": [
    "# Set the question and see what documents each technique retrieves.\n",
    "QUESTION=\"What vector indexing algorithms does Astra use?\"\n",
    "for i, doc in enumerate(vector_retriever.invoke(QUESTION)):\n",
    "  print(f\"Vector [{i}]:    {doc.metadata['content_id']}\")\n",
    "\n",
    "for i, doc in enumerate(graph_retriever.invoke(QUESTION)):\n",
    "  print(f\"Graph [{i}]:     {doc.metadata['content_id']}\")\n",
    "\n",
    "for i, doc in enumerate(mmr_graph_retriever.invoke(QUESTION)):\n",
    "  print(f\"MMR Graph [{i}]: {doc.metadata['content_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "With vector only we retrieved chunks from the Astra documentation explaining that it used JVector.\n",
    "Since it didn't follow the link to [JVector on GitHub](https://github.com/jbellis/jvector) it didn't actually answer the question.\n",
    "\n",
    "The graph retrieval started with the same set of chunks, but it followed the edge to the documents we loaded from GitHub.\n",
    "This allowed the LLM to read in more depth how JVector is implemented, which allowed it to answer the question more clearly and with more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-framework-aiP65pJh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
