{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Which PDF Splitter works the best?\n",
    "\n",
    "In this notebook we will do the embedding using the different splitting libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U trulens-eval # includes lang-chain as a dependency\n",
    "! pip3 install -U ipython ipywidgets # required for the trulens UI to run from inside the notebook\n",
    "! pip3 install -U llama-index # for the llamaindex-cli tool to download datasets\n",
    "! pip3 install -U langchain-openai # for the Azure LLM models\n",
    "! pip3 install -U astrapy # to access AstraDB vector store\n",
    "\n",
    "! pip3 install -U llmsherpa # for the LayoutPDFReader pdf parser\n",
    "! pip3 install -U pypdfium2 # for the PyPDFium2Loader\n",
    "! pip3 install -U pdfminer-six # for the PDFMinerLoader\n",
    "! pip3 install -U pypdf # for the PyPDFLoader\n",
    "! pip3 install -U pymupdf # for the PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should restart your environment after installing the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook assumes the following env vars exist in a .env file:\n",
    "#\n",
    "# ASTRA_DB_ENDPOINT=https://<uuid>-<region>.apps.astra.datastax.com\n",
    "# ASTRA_DB_TOKEN=AstraCS:<secret>:<secret>\n",
    "# AZURE_OPENAI_ENDPOINT=https://<domain>.openai.azure.com/\n",
    "# AZURE_OPENAI_API_KEY=<secret>\n",
    "# OPENAI_API_TYPE=azure\n",
    "# OPENAI_API_VERSION=2023-05-15\n",
    "\n",
    "# and optionally this var if you want to use an external database for TruLens:\n",
    "# TRULENS_DB_CONN_STRING=<db connection string>\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"ASTRA_DB_ENDPOINT\"] = os.environ.get(\"ASTRA_DB_ENDPOINT_PDF_SPLITS\")\n",
    "os.environ[\"ASTRA_DB_TOKEN\"] = os.environ.get(\"ASTRA_DB_TOKEN_PDF_SPLITS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Azure LLMs for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Azure-based models\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents into memory, chunk, create embeddings, store in AstraDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayoutPDFReader\n",
    "\n",
    "Most PDF to text parsers do not provide layout information. Often times, even the sentences are split with arbitrary CR/LFs making it very difficult to find paragraph boundaries. This poses various challenges in chunking and adding long running contextual information such as section header to the passages while indexing/vectorizing PDFs for LLM applications such as retrieval augmented generation (RAG).\n",
    "\n",
    "LayoutPDFReader solves this problem by parsing PDFs along with hierarchical layout information such as:\n",
    "\n",
    "Sections and subsections along with their levels.\n",
    "Paragraphs - combines lines.\n",
    "Links between sections and paragraphs.\n",
    "Tables along with the section the tables are found in.\n",
    "Lists and nested lists.\n",
    "Join content spread across pages.\n",
    "Removal of repeating headers and footers.\n",
    "Watermark removal.\n",
    "With LayoutPDFReader, developers can find optimal chunks of text to vectorize, and a solution for limited context window sizes of LLMs.\n",
    "\n",
    "* https://github.com/nlmatics/llmsherpa\n",
    "* ☆ 586\n",
    "\n",
    "### PyPDFium2Loader\n",
    "\n",
    "pypdfium2 is an ABI-level Python 3 binding to PDFium, a powerful and liberal-licensed library for PDF rendering, inspection, manipulation and creation.\n",
    "\n",
    "* https://github.com/pypdfium2-team/pypdfium2\n",
    "* ☆ 231\n",
    "* https://pdfium.googlesource.com/pdfium/\n",
    "\n",
    "### PDFMinerLoader\n",
    "\n",
    "Pdfminer.six is a community maintained fork of the original PDFMiner. It is a tool for extracting information from PDF documents. It focuses on getting and analyzing text data. Pdfminer.six extracts the text from a page directly from the sourcecode of the PDF. It can also be used to get the exact location, font or color of the text.\n",
    "\n",
    "* https://github.com/pdfminer/pdfminer.six \n",
    "* ☆ 5.1k\n",
    "\n",
    "### PyPDFLoader\n",
    "\n",
    "A pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files\n",
    "\n",
    "* https://github.com/py-pdf/pypdf\n",
    "* ☆ 7.0k\n",
    "\n",
    "### PyMuPDFLoader\n",
    "\n",
    "A high performance Python library for data extraction, analysis, conversion & manipulation of PDF (and other) documents.\n",
    "\n",
    "* https://github.com/pymupdf/PyMuPDF\n",
    "* ☆ 3.6k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFium2Loader, PyMuPDFLoader, PyPDFLoader, PDFMinerLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.vectorstores.astradb import AstraDB\n",
    "import os\n",
    "\n",
    "collection_loader_map = {\n",
    "    \"PyPDFium2Loader\" : { \"loader\": PyPDFium2Loader, \"kwargs\": {}},\n",
    "    \"PyMuPDFLoader\" : { \"loader\": PyMuPDFLoader, \"kwargs\": {}},\n",
    "    \"PyPDFLoader\" : { \"loader\": PyPDFLoader, \"kwargs\": {}},\n",
    "    \"PDFMinerLoader_by_page\" : { \"loader\": PDFMinerLoader, \"kwargs\": {\"concatenate_pages\": False}},\n",
    "    \"PDFMinerLoader_by_pdf\" : { \"loader\": PDFMinerLoader, \"kwargs\": {\"concatenate_pages\": True}},\n",
    "}\n",
    "\n",
    "for collection_name in collection_loader_map:\n",
    "    vstore = AstraDB(\n",
    "        collection_name=collection_name,\n",
    "        embedding=open_ai_embeddings,\n",
    "        token=os.getenv(\"ASTRA_DB_TOKEN\"),\n",
    "        api_endpoint=os.getenv(\"ASTRA_DB_ENDPOINT\")\n",
    "    )\n",
    "\n",
    "    print(f\"Loading PDFs into {collection_name}:\")\n",
    "    loader_cls = collection_loader_map[collection_name][\"loader\"]\n",
    "    loader_kwargs = collection_loader_map[collection_name][\"kwargs\"]\n",
    "    loader = DirectoryLoader('data/', glob=f\"*/source_files/*.pdf\", show_progress=True, loader_cls=loader_cls, loader_kwargs=loader_kwargs)\n",
    "\n",
    "    splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "    vstore.add_documents(splitter.split_documents(loader.load()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "# pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "# pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "# doc = pdf_reader.read_pdf(pdf_url)\n",
    "\n",
    "# from llama_index.readers.schema.base import Document\n",
    "# from llama_index import VectorStoreIndex\n",
    "\n",
    "# index = VectorStoreIndex([])\n",
    "# for chunk in doc.chunks():\n",
    "#     index.insert(Document(text=chunk.to_context_text(), extra_info={}))\n",
    "# query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "datasets = []\n",
    "\n",
    "# Example: Find all .txt files in the specified directory\n",
    "for file_path in glob.glob('data/*/source_files/*.pdf'):\n",
    "    dataset = file_path.split(\"/\")[1]\n",
    "    if dataset not in datasets:\n",
    "        datasets.append(dataset)\n",
    "\n",
    "print(datasets)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
