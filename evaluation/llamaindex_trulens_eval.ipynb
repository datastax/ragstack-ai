{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U ragstack-ai trulens_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trulens                                  0.13.4\n",
      "trulens-eval                             0.20.1\n"
     ]
    }
   ],
   "source": [
    "! pip3 list | grep trulens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"llama_512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# this notebook assumes the following env vars exist in a .env file:\n",
    "#\n",
    "# ASTRA_DB_ENDPOINT\n",
    "# ASTRA_DB_TOKEN\n",
    "# AZURE_OPENAI_ENDPOINT\n",
    "# AZURE_OPENAI_API_KEY\n",
    "# OPENAI_API_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure LLMs for LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import AzureOpenAI as AzureChatOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "import os\n",
    "\n",
    "temperature = 0.0\n",
    "\n",
    "gpt_35_turbo = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    model=\"gpt-35-turbo\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_35_turbo_16k = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo-16k\",\n",
    "    model=\"gpt-35-turbo-16k\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4 = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4\",\n",
    "    model=\"gpt-4\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"1106-preview\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4_32k = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4-32k\",\n",
    "    model=\"gpt-4-32k\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbedding(\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init an AstraDB vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import AstraDBVectorStore\n",
    "import os\n",
    "\n",
    "astra_db_vstore = AstraDBVectorStore(\n",
    "    collection_name=collection_name,\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_ENDPOINT\"),\n",
    "    token=os.getenv(\"ASTRA_DB_TOKEN\"),\n",
    "    embedding_dimension=1536,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset:  patronus_ai_financebench\n",
      "Loaded dataset:  uber_10k\n",
      "Loaded dataset:  blockchain_solana\n",
      "Loaded dataset:  covid_qa\n",
      "Loaded dataset:  llama_2_paper\n",
      "Loaded dataset:  evaluating_llm_survey_paper\n",
      "Loaded dataset:  mini_squad_v2\n",
      "Loaded dataset:  origin_of_covid_19\n",
      "Loaded dataset:  braintrust_coda_help_desk\n",
      "Loaded dataset:  paul_grahman_essay\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "base_path = \"./data/\"\n",
    "\n",
    "datasets = {}\n",
    "golden_set = []\n",
    "\n",
    "for name in os.listdir(base_path):\n",
    "    if os.path.isdir(os.path.join(base_path, name)):\n",
    "        datasets[name] = []\n",
    "        with open(os.path.join(base_path, name, \"rag_dataset.json\")) as f:\n",
    "            examples = json.load(f)['examples']\n",
    "            for e in examples:\n",
    "                datasets[name].append(e[\"query\"])\n",
    "                golden_set.append({\n",
    "                    \"query\": e[\"query\"],\n",
    "                    \"response\": e[\"reference_answer\"],\n",
    "                })\n",
    "            print(\"Loaded dataset: \", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer, VectorStoreIndex, StorageContext, ServiceContext\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=gpt_35_turbo,\n",
    "    embed_model=embeddings,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=astra_db_vstore,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=astra_db_vstore,\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The symptoms of COVID-19 include persistent pain or pressure in the chest, high temperature (above 38Â°C), irritability, confusion, reduced consciousness, anxiety, depression, sleep disorders, headache, muscle or joint pain, different types of skin rash, nausea or vomiting, diarrhea, chills, dizziness, shortness of breath, loss of appetite, loss of taste or smell, nasal congestion, conjunctivitis (red eyes), sore throat, and gastrointestinal disturbances such as diarrhea and nausea. In severe cases, symptoms may also include inflammation, organ damage, blood clots, strokes, and brain impairments.\n"
     ]
    }
   ],
   "source": [
    "# try a query\n",
    "response = query_engine.query(\"What are the symptoms?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url postgresql://postgres:***@127.0.0.1:5432 .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru(database_url=os.getenv(\"TRULENS_DB_CONN_STRING\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Feedback Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text.collect() .\n",
      "âœ… In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In relevance_with_cot_reasons, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In relevance_with_cot_reasons, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In qs_relevance_with_cot_reasons, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In qs_relevance_with_cot_reasons, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "âœ… In agreement_measure, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In agreement_measure, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import AzureOpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruLlama, Feedback\n",
    "from trulens_eval.app import App\n",
    "import numpy as np\n",
    "# Initialize provider class\n",
    "azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "context = App.select_context(query_engine)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "grounded = Groundedness(groundedness_provider=azureOpenAI)\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"groundedness\")\n",
    "    .on(context.collect()).on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(azureOpenAI.relevance_with_cot_reasons, name=\"answer_relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(azureOpenAI.qs_relevance_with_cot_reasons, name=\"context_relevance\")\n",
    "    .on_input().on(context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "ground_truth_collection = GroundTruthAgreement(golden_set, provider=azureOpenAI)\n",
    "f_answer_correctness = (\n",
    "    Feedback(ground_truth_collection.agreement_measure, name=\"answer_correctness\")\n",
    "    .on_input_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in datasets:\n",
    "    app = f\"{name}_{collection_name}\"\n",
    "    tru_recorder = TruLlama(\n",
    "        query_engine,\n",
    "        app_id=app,\n",
    "        feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness, f_answer_correctness],\n",
    "        feedback_mode=\"deferred\",\n",
    "    )\n",
    "    for query in datasets[name]:\n",
    "        with tru_recorder as recording:\n",
    "            query_engine.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def waitForResults(app, index):\n",
    "    # it normally takes about 10 seconds to get results\n",
    "    # so delay until that time, and then check more frequently\n",
    "    print(f\"waiting for results on app: {app} index: {index}\")\n",
    "    start = datetime.now()\n",
    "    time.sleep(7)\n",
    "    while True:\n",
    "        time.sleep(2)\n",
    "        df, feedbackColumns = tru.get_records_and_feedback([app])\n",
    "        row = df.loc[index]\n",
    "        completeCount = 0\n",
    "        for fbCol in feedbackColumns:\n",
    "            if not np.isnan(row[fbCol]):\n",
    "                completeCount += 1\n",
    "        if completeCount == len(feedbackColumns):\n",
    "            return\n",
    "        else:\n",
    "            print(f\"index: {index} has completeCount: {completeCount}, continuing to wait\")\n",
    "        if (datetime.now() - start).total_seconds() > 30:\n",
    "            print(\"timeout, giving up\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "count = 0\n",
    "\n",
    "for name in datasets:\n",
    "    shortUuid = str(uuid.uuid4())[9:13]\n",
    "    app = f\"{name}_{collection_name}_{shortUuid}\"\n",
    "    tru_recorder = TruLlama(\n",
    "        query_engine,\n",
    "        app_id=app,\n",
    "        feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness, f_answer_correctness],\n",
    "    )\n",
    "    index = 0\n",
    "    for query in datasets[name]:\n",
    "        with tru_recorder as recording:\n",
    "            query_engine.query(query)\n",
    "        waitForResults(app, index)\n",
    "        index +=1\n",
    "        count +=1\n",
    "        if count > 10:\n",
    "            break\n",
    "    if count > 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
