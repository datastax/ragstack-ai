{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColBERT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test Setup\n",
    "\n",
    "**Baseline** (RED)\n",
    "  * LlamaIndex 0.10.x with AstraDBVectorStore \n",
    "  * Embedding\n",
    "    * SimpleDirectoryReader\n",
    "    * TokenTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "    * OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "  * Query (with TruLlama recorder)\n",
    "     * VectorIndexRetriever(similarity_top_k=5)\n",
    "     * Prompt: llama_index.core.get_response_synthesizer()\n",
    "     * AzureOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "**ColBERT** (GREEN)\n",
    "  * LlamaIndex 0.10.x with embedding.AstraDB\n",
    "  * Embedding\n",
    "    * SimpleDirectoryReader\n",
    "    * TokenTextSplitter(chunk_size=160, chunk_overlap=50)\n",
    "    * ColbertTokenEmbeddings(doc_maxlen=220, nbits=1, kmeans_niters=4, nranks=1)\n",
    "  * Query (with TruLlama recorder)\n",
    "    * ColbertAstraRetriever(k=5, query_maxlen=32)\n",
    "    * Prompt: llama_index.core.get_response_synthesizer()\n",
    "    * AzureOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "**ColBERT_k_5_query_maxlen_64** (BLUE)\n",
    "  * LlamaIndex 0.10.x with embedding.AstraDB\n",
    "  * Embedding\n",
    "    * SimpleDirectoryReader\n",
    "    * TokenTextSplitter(chunk_size=160, chunk_overlap=50)\n",
    "    * ColbertTokenEmbeddings(doc_maxlen=220, nbits=1, kmeans_niters=4, nranks=1)\n",
    "  * Query (with TruLlama recorder)\n",
    "    * ColbertAstraRetriever(k=5, query_maxlen=64)\n",
    "    * Prompt: llama_index.core.get_response_synthesizer()\n",
    "    * AzureOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "**RAGatouille** (Violet)\n",
    "  * LlamaIndex 0.10.x \n",
    "  * Embedding\n",
    "    * SimpleDirectoryReader\n",
    "    * RAGatouille default embedding\n",
    "  * Query (with TruLlama recorder)\n",
    "    * RAGatouilleRetriever(k=5)\n",
    "    * Prompt: llama_index.core.get_response_synthesizer()\n",
    "    * AzureOpenAI(model=\"gpt-3.5-turbo\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "#### From the 11 datasets, compared to the baseline, ColBERT `maxlen=64` was:\n",
    "* Significantly better for 4\n",
    "* Slightly better for 2\n",
    "* Slightly worse for 3\n",
    "* Significantly worse for 0\n",
    "* Tie for 2\n",
    "\n",
    "This confirms that ColBERT `maxlen=64` generally performs better than the baseline.\n",
    "\n",
    "#### From the 11 datasets, compared to the baseline, ColBERT `maxlen=32` was:\n",
    "* Significantly better for 4\n",
    "* Slightly better for 2\n",
    "* Slightly worse for 2\n",
    "* Significantly worse for 2\n",
    "* Inconclusive for 1\n",
    "\n",
    "This confirms that ColBERT `maxlen=64` performs similarly to the baseline.\n",
    "\n",
    "#### From the 11 datasets, compared to RAGatouille, ColBERT `maxlen=64` was:\n",
    "* Significantly better for 1\n",
    "* Slightly better for 1\n",
    "* Slightly worse for 5\n",
    "* Significantly worse for 0\n",
    "* Tie for 4\n",
    "\n",
    "This confirms that ColBERT `maxlen=64` performs similarly to RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockchain Solana Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A labelled RAG dataset based off an article, From Bitcoin to Solana – Innovating Blockchain towards Enterprise Applications),by Xiangyu Li, Xinyu Wang, Tingli Kong, Junhao Zheng and Min Luo, consisting of queries, reference answers, and reference contexts.\n",
    "\n",
    "Source Data: 1 PDF file (27 pages total)\n",
    "\n",
    "Number Of Examples: 58\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s):\n",
    "https://arxiv.org/abs/2207.05240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blockchain_solana.png](blockchain_solana.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT slightly better\n",
    "\n",
    "Both ColBERT tests perform slightly better on answer correctness. Also context relevance and grounded-mean scores are higher.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs slightly worse than RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braintrust Coda Help Desk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A list of automatically generated question/answer pairs from the Coda (https://coda.io/) help docs. This dataset is interesting because most models include Coda’s documentation as part of their training set, so you can baseline performance without RAG.\n",
    "\n",
    "Source Data: 50 Markdown files\n",
    "\n",
    "Number Of Examples: 100\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s): https://gist.githubusercontent.com/wong-codaio/b8ea0e087f800971ca5ec9eef617273e/raw/39f8bd2ebdecee485021e20f2c1d40fd649a4c77/articles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![braintrust_coda_help_desk.png](braintrust_coda_help_desk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT significantly better\n",
    "\n",
    "ColBERT shows much tighter and higher answer correctness and context relevance. Grounded-ness also much better.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs slightly worse than RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covid Qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A human-annotated RAG dataset consisting of over 300 question-answer pairs. This dataset represents a subset of the Covid-QA dataset available on Kaggle and authored by Xhlulu. It is a collection of frequently asked questions on COVID from various websites. This subset only considers the top 10 webpages containing the most question-answer pairs.\n",
    "\n",
    "Source Data: 10 html files\n",
    "\n",
    "Number Of Examples: 316\n",
    "\n",
    "Examples Generated By: Human\n",
    "\n",
    "Source(s): https://www.kaggle.com/datasets/xhlulu/covidqa/?select=news.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![covid_qa.png](covid_qa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tie\n",
    "\n",
    "Nearly identical performance between ColBERT and the baseline.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs similarly to RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Llm Survey Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A labelled RAG dataset over the comprehensive, spanning 111 pages in total, survey on evaluating LLMs.\n",
    "\n",
    "Source Data: 1 PDF file (111 pages total)\n",
    "\n",
    "Number Of Examples: 276\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s): https://arxiv.org/pdf/2310.19736.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![evaluating_llm_survey_paper.png](evaluating_llm_survey_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT significantly better\n",
    "\n",
    "ColBERT `(maxlen=32)` shows much tighter and higher answer correctness. Context relevance is slightly better.  Grounded-ness also better. LLM hallucinates lees with ColBERT.\n",
    "\n",
    "ColBERT `(maxlen=64)` has even stronger results.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs similarly to RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Of Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A labelled RAG dataset based off an article, The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches, by Md Zahangir Alom, Tarek M. Taha, Christopher Yakopcic, Stefan Westberg, Paheding Sidike, Mst Shamima Nasrin, Brian C Van Esesn, Abdul A S. Awwal, Vijayan K. Asari, consisting of queries, reference answers, and reference contexts.\n",
    "\n",
    "Source Data: 1 PDF file (39 pages total)\n",
    "\n",
    "Number Of Examples: 160\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s): https://arxiv.org/abs/1803.01164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![history_of_alexnet.png](history_of_alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT slightly better\n",
    "\n",
    "ColBERT `(maxlen=64)` shows slightly better context relevance and answer correctness.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs similarly to RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 2 Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A labelled RAG dataset based off the Llama 2 ArXiv PDF.\n",
    "\n",
    "Source Data: 1 PDF file (77 pages total)\n",
    "\n",
    "Number Of Examples: 100\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s): https://arxiv.org/abs/2307.09288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![llama_2_paper.png](llama_2_paper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tie\n",
    "\n",
    "ColBERT `(maxlen=32)` has significantly lower answer correctness and grounded-ness, despite having nearly identical context relevance.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs similarly to the baseline.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs similarly to RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Squad V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This is a subset of the original SquadV2 dataset. In particular, it considers only the top 10 Wikipedia pages in terms of having questions about them.\n",
    "\n",
    "Source Data: 10 txt files\n",
    "\n",
    "Number Of Examples: 195\n",
    "\n",
    "Examples Generated By: Human\n",
    "\n",
    "Source(s): https://huggingface.co/datasets/squad_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mini_squad_v2.png](mini_squad_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT significantly better\n",
    "\n",
    "ColBERT `(maxlen=32)` shows significantly better answer correctness, context relevance and grounded-ness.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs significantly better than ColBERT `(maxlen=32)`\n",
    "\n",
    "ColBERT `(maxlen=64)` performs significantly better than RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Origin Of COVID-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A labelled RAG dataset based off an article, The Origin Of COVID-19 and Why It Matters, by Morens DM, Breman JG, Calisher CH, Doherty PC, Hahn BH, Keusch GT, Kramer LD, LeDuc JW, Monath TP, Taubenberger JK, consisting of queries, reference answers, and reference contexts.\n",
    "\n",
    "Source Data: 1 PDF file (5 pages total)\n",
    "\n",
    "Number Of Examples: 24\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s): https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7470595/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![origin_of_covid_19.png](origin_of_covid_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT slightly worse\n",
    "\n",
    "ColBERT `(maxlen=64)` has slightly lower answer correctness and significantly lower context_relevance.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs slightly better than RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patronus AI FinanceBench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: This is a subset of the original FinanceBench dataset. FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). This is an open source sample of 150 annotated examples used in the evaluation and analysis of models assessed in the FinanceBench paper. The dataset comprises of questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard.\n",
    "\n",
    "Source Data: 32 PDF files (4148 pages total)\n",
    "\n",
    "Number Of Examples: 98\n",
    "\n",
    "Examples Generated By: Human\n",
    "\n",
    "Source(s): https://huggingface.co/datasets/PatronusAI/financebench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![patronus_finance_bench.png](patronus_ai_financebench.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT slightly worse\n",
    "\n",
    "Nearly identical performance between ColBERT and the baseline. If anything, ColBERT is slightly worse.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs slightly worse than RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paul Graham Essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A labelled RAG dataset based off an essay by Paul Graham, consisting of queries, reference answers, and reference contexts.\n",
    "\n",
    "Source Data: 1 txt file\n",
    "\n",
    "Number Of Examples: 44\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s): http://www.paulgraham.com/articles.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![paul_grahmam_essay.png](paul_grahman_essay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT significantly better\n",
    "\n",
    "ColBERT has better context relevance. Answer correctness and grounded-ness are significantly better.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs slightly worse than RAGatouille."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uber 10K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A labelled RAG dataset based on the Uber 2021 10K document, consisting of queries, reference answers, and reference contexts.\n",
    "\n",
    "Source Data: 1 PDF file (307 pages total)\n",
    "\n",
    "Number Of Examples: 822\n",
    "\n",
    "Examples Generated By: AI\n",
    "\n",
    "Source(s): https://s23.q4cdn.com/407969754/files/doc_financials/2022/ar/2021-Annual-Report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![uber_10k.png](uber_10k.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColBERT slightly worse\n",
    "\n",
    "ColBERT `(maxlen=32)` shows significantly lower scores on context relevance, answer correctness and grounded-ness as compared to the baseline.\n",
    "\n",
    "ColBERT `(maxlen=64)` shows a big improvement over ColBERT `(maxlen=32)`. Both answer correctness and context relevance are just slightly worse than the baseline.\n",
    "\n",
    "ColBERT `(maxlen=64)` performs slightly worse than RAGatouille."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
