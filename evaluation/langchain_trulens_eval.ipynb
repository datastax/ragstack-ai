{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U trulens-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 uninstall -y langchain llama-index trulens-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 list | grep trulens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"open_ai_512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# this notebook assumes the following env vars exist in a .env file:\n",
    "#\n",
    "# ASTRA_DB_ENDPOINT\n",
    "# ASTRA_DB_TOKEN\n",
    "# AZURE_OPENAI_ENDPOINT\n",
    "# AZURE_OPENAI_API_KEY\n",
    "# OPENAI_API_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure LLMs for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Azure-based models\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "temperature = 0.0\n",
    "\n",
    "gpt_35_turbo = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_35_turbo_16k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    model_version=\"1106-preview\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4_32k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4-32k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init an AstraDB vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.astradb import AstraDB\n",
    "import os\n",
    "\n",
    "astra_db_vstore = AstraDB(\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    "    token=os.getenv(\"ASTRA_DB_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "base_path = \"./data/\"\n",
    "\n",
    "datasets = {}\n",
    "golden_set = []\n",
    "\n",
    "for name in os.listdir(base_path):\n",
    "    if os.path.isdir(os.path.join(base_path, name)):\n",
    "        datasets[name] = []\n",
    "        with open(os.path.join(base_path, name, \"rag_dataset.json\")) as f:\n",
    "            examples = json.load(f)['examples']\n",
    "            index = 0\n",
    "            for e in examples:\n",
    "                datasets[name].append(e[\"query\"])\n",
    "                golden_set.append({\n",
    "                    \"query\": e[\"query\"],\n",
    "                    \"response\": e[\"reference_answer\"],\n",
    "                })\n",
    "                index += 1\n",
    "            print(\"Loaded dataset: \", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a LCEL chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "# build a prompt\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say: \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": astra_db_vstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | gpt_35_turbo\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a query\n",
    "response = rag_chain.invoke(\"What are the symptoms?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru(database_url=os.getenv(\"TRULENS_DB_CONN_STRING\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Feedback Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import AzureOpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruChain, Feedback\n",
    "from trulens_eval.app import App\n",
    "import numpy as np\n",
    "# Initialize provider class\n",
    "azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "context = App.select_context(rag_chain)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "grounded = Groundedness(groundedness_provider=azureOpenAI)\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"groundedness\")\n",
    "    .on(context.collect()).on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(azureOpenAI.relevance_with_cot_reasons, name=\"answer_relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(azureOpenAI.qs_relevance_with_cot_reasons, name=\"context_relevance\")\n",
    "    .on_input().on(context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "ground_truth_collection = GroundTruthAgreement(golden_set, provider=azureOpenAI)\n",
    "f_answer_correctness = (\n",
    "    Feedback(ground_truth_collection.agreement_measure, name=\"answer_correctness\")\n",
    "    .on_input_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in datasets:\n",
    "    app = f\"{name}_{collection_name}\"\n",
    "    tru_recorder = TruChain(\n",
    "        rag_chain,\n",
    "        app_id=app,\n",
    "        feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness, f_answer_correctness],\n",
    "        feedback_mode=\"deferred\",\n",
    "    )\n",
    "    for query in datasets[name]:\n",
    "        with tru_recorder as recording:\n",
    "            rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def waitForResults(app, index):\n",
    "    # it normally takes about 10 seconds to get results\n",
    "    # so delay until that time, and then check more frequently\n",
    "    print(f\"waiting for results on app: {app} index: {index}\")\n",
    "    start = datetime.now()\n",
    "    time.sleep(7)\n",
    "    while True:\n",
    "        time.sleep(2)\n",
    "        df, feedbackColumns = tru.get_records_and_feedback([app])\n",
    "        row = df.loc[index]\n",
    "        completeCount = 0\n",
    "        for fbCol in feedbackColumns:\n",
    "            if not np.isnan(row[fbCol]):\n",
    "                completeCount += 1\n",
    "        if completeCount == len(feedbackColumns):\n",
    "            return\n",
    "        else:\n",
    "            print(f\"index: {index} has completeCount: {completeCount}, continuing to wait\")\n",
    "        if (datetime.now() - start).total_seconds() > 30:\n",
    "            print(\"timeout, giving up\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "count = 0\n",
    "\n",
    "for name in datasets:\n",
    "    shortUuid = str(uuid.uuid4())[9:13]\n",
    "    app = f\"{name}_{collection_name}_{shortUuid}\"\n",
    "    tru_recorder = TruChain(\n",
    "        rag_chain,\n",
    "        app_id=app,\n",
    "        feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness, f_answer_correctness],\n",
    "    )\n",
    "    index = 0\n",
    "    for query in datasets[name]:\n",
    "        with tru_recorder as recording:\n",
    "            rag_chain.invoke(query)\n",
    "        waitForResults(app, index)\n",
    "        index +=1\n",
    "        count +=1\n",
    "        if count > 10:\n",
    "            break\n",
    "    if count > 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
