{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q ragstack-ai trulens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "gpt_35_turbo = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "gpt_35_turbo_16k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "gpt_4 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    model_version=\"1106-preview\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "gpt_4_32k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4-32k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.astradb import AstraDB\n",
    "import os\n",
    "vstore = AstraDB(\n",
    "    collection_name=\"open_ai_512\",\n",
    "    embedding=embeddings,\n",
    "    token=os.getenv(\"ASTRA_DB_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Standard RAG, nothing fancy\n",
    "base_retriever = vstore.as_retriever()\n",
    "\n",
    "base_chain = (\n",
    "    {\"context\": base_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | gpt_35_turbo_16k\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tracers.context import register_configure_hook\n",
    "\n",
    "from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, TypeVar, Union, Generator\n",
    "from uuid import UUID\n",
    "\n",
    "from contextvars import ContextVar\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "class RetrievedContextCallbackHandler(BaseCallbackHandler):\n",
    "    documents: Sequence[Document]\n",
    "    run_id: UUID\n",
    "    parent_run_id: Optional[UUID]\n",
    "\n",
    "    def on_retriever_end(self, documents: Sequence[Document], run_id: UUID, parent_run_id: Optional[UUID] = None, **kwargs) -> None:\n",
    "        self.documents = documents\n",
    "        self.run_id = run_id\n",
    "        self.parent_run_id = parent_run_id\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"boop\"\n",
    "\n",
    "    def __copy__(self) -> \"RetrievedContextCallbackHandler\":\n",
    "        \"\"\"Return a copy of the callback handler.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def __deepcopy__(self, memo: Any) -> \"RetrievedContextCallbackHandler\":\n",
    "        \"\"\"Return a deep copy of the callback handler.\"\"\"\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def always_verbose(self) -> bool:\n",
    "        \"\"\"Whether to call verbose callbacks even if verbose is False.\"\"\"\n",
    "        return True\n",
    "\n",
    "retrieved_context_callback_var: ContextVar[Optional[RetrievedContextCallbackHandler]] = ContextVar(\n",
    "    \"retrieved_context_callback\", default=None\n",
    ")\n",
    "\n",
    "register_configure_hook(retrieved_context_callback_var, True)\n",
    "\n",
    "@contextmanager\n",
    "def get_retrieved_context_callback() -> Generator[RetrievedContextCallbackHandler, None, None]:\n",
    "    cb = RetrievedContextCallbackHandler()\n",
    "    retrieved_context_callback_var.set(cb)\n",
    "    yield cb\n",
    "    retrieved_context_callback_var.set(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_retrieved_context_callback() as cb:\n",
    "    resp = base_chain.invoke(\"What are the symptoms?\", )\n",
    "    print(f\"context documents: {cb.documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from trulens_eval import OpenAI as fOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "provider = fOpenAI(model_engine=\"gpt_4\", endpoint=openai_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Feedback\n",
    "\n",
    "f_qa_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_selection = TruLlama.select_source_nodes().node.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance_with_cot_reasons,\n",
    "             name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context_selection)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons,\n",
    "             name=\"Groundedness\"\n",
    "            )\n",
    "    .on(context_selection)\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = my_llm_app(query)\n",
    "\n",
    "from trulens_eval import TruChain\n",
    "tru_recorder = TruChain(\n",
    "    my_llm_app,\n",
    "    app_id='Chain1_ChatApplication')\n",
    "\n",
    "response, tru_record = tru_recorder.with_record(my_llm_app, query)\n",
    "json_like = tru_record.layout_calls_as_app()\n",
    "\n",
    "context_selection = json_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from trulens_eval import TruChain\n",
    "\n",
    "chain = RetrievalQA.from_llm(llm=gpt_35_turbo_16k, prompt=prompt, retriever=vstore.as_retriever())\n",
    "\n",
    "# f_lang_match, f_qa_relevance, f_qs_relevance are feedback functions\n",
    "tru_recorder = TruChain(\n",
    "    chain,\n",
    "    app_id='Chain1_ChatApplication',\n",
    "    feedbacks=[f_lang_match, f_qa_relevance, f_qs_relevance]\n",
    ")\n",
    "with tru_recorder as recording:\n",
    "    chain(\"What are the symptoms?\")\n",
    "\n",
    "tru_record = recording.records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Logging\n",
    "\n",
    "https://www.trulens.org/trulens_eval/logging/#wrap-with-truchain-to-instrument-your-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap with TruChain to instrument your chain\n",
    "tc = TruChain(chain, app_id='Chain1_ChatApplication')\n",
    "\n",
    "# Making the first call to your wrapped LLM Application will now also produce a log or \"record\" of the chain execution.\n",
    "prompt_input = 'que hora es?'\n",
    "response, record = tc.call_with_record(prompt_input)\n",
    "\n",
    "# We can log the records but first we need to log the chain itself.\n",
    "tru.add_app(app=tc)\n",
    "\n",
    "# Then we can log the record\n",
    "tru.add_record(record)\n",
    "\n",
    "# Capturing app feedback such as user feedback of the responses can be added with one call.\n",
    "thumb_result = True\n",
    "tru.add_feedback(name=\"üëç (1) or üëé (0)\", record_id=record.record_id, result=thumb_result)\n",
    "\n",
    "# To assess your LLM quality, you can provide the feedback functions to `tru.run_feedback()` in a list provided to `feedback_functions`\n",
    "\n",
    "from trulens_eval import Feedback\n",
    "\n",
    "f_qa_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input_output()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "json_like = record.layout_calls_as_app()\n",
    "\n",
    "print(json_like['app'])\n",
    "context_selection = json_like['app']['source_nodes']\n",
    "\n",
    "print(context_selection)\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons,\n",
    "             name=\"Groundedness\"\n",
    "            )\n",
    "    .on(context_selection)\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance_with_cot_reasons,\n",
    "             name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context_selection)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "feedback_results = tru.run_feedback_functions(\n",
    "    record=record,\n",
    "    feedback_functions=[f_qa_relevance, f_groundedness, f_qs_relevance]\n",
    ")\n",
    "display(feedback_results)\n",
    "\n",
    "# After capturing feedback, you can then log it to your local database.\n",
    "\n",
    "tru.add_feedbacks(feedback_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_like['app']['combine_documents_chain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with RAGAS + LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "ragas_chain = RetrievalQA.from_llm(llm=gpt_35_turbo_16k, prompt=prompt, retriever=vstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing it out\n",
    "question = \"What are the symptoms?\"\n",
    "result = ragas_chain({\"query\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"ground_truths\"] = [\"Symptoms include fever, coughing, sore throat, fatigue, and shortness of breath.\\nHowever, be aware that at this stage if you have a cough or a cold, it's likely that you just have a cough or a cold and not coronavirus.\\nIf you have serious symptoms such as difficulty breathing, call 000 for urgent medical help.\\nIf you get these symptoms above after being in contact with someone who has been diagnosed with COVID-19, seek medical attention.\\nThe same goes if you develop symptoms within 14 days of returning home to Australia after being overseas.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLM\n",
    "from ragas.metrics import context_precision, answer_relevancy, faithfulness, context_recall\n",
    "\n",
    "# wrappers around azure_models\n",
    "ragas_gpt4 = LangchainLLM(gpt_4)\n",
    "ragas_gpt35 = LangchainLLM(gpt_35_turbo)\n",
    "\n",
    "# patch the new RagasLLM instance\n",
    "answer_relevancy.llm = ragas_gpt35\n",
    "\n",
    "# embeddings can be used as it is\n",
    "answer_relevancy.embeddings = embeddings\n",
    "\n",
    "context_precision.llm = ragas_gpt35\n",
    "context_recall.llm = ragas_gpt35\n",
    "faithfulness.llm = ragas_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.langchain import RagasEvaluatorChain\n",
    "\n",
    "# make eval chains\n",
    "eval_chains = {\n",
    "    m.name: RagasEvaluatorChain(metric=m) for m in [context_precision, answer_relevancy, faithfulness, context_recall]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "for name, eval_chain in eval_chains.items():\n",
    "    score_name = f\"{name}_score\"\n",
    "    print(f\"{score_name}: {eval_chain(result)[score_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Datasets\n",
    "\n",
    "# data\n",
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")\n",
    "fiqa_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiqa_eval[\"baseline\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    fiqa_eval[\"baseline\"],\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/covid_qa/rag_dataset.json\") as f:\n",
    "    examples = json.load(f)['examples']\n",
    "\n",
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset creation\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"covid_qa\"\n",
    "\n",
    "try:\n",
    "    # check if dataset exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    # if not create a new one with the generated query examples\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, description=\"llamaindex covid_qa dataset\"\n",
    "    )\n",
    "    for e in examples:\n",
    "        client.create_example(\n",
    "            inputs={\"query\": e[\"query\"]},\n",
    "            outputs={\"ground_truths\": [e[\"reference_answer\"]]},\n",
    "            dataset_id=dataset.id,\n",
    "        )\n",
    "\n",
    "    print(\"Created a new dataset: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Setup\n",
    "\n",
    "Before you call `run_on_dataset` you need a factory function which creates a new instance of the QA chain you want to test. This is so that the internal state is not reused when running against each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory function that return a new qa chain\n",
    "def create_qa_chain(return_context=True):\n",
    "    qa_chain = RetrievalQA.from_llm(\n",
    "        llm=gpt_35_turbo_16k,\n",
    "        prompt=prompt,\n",
    "        retriever=vstore.as_retriever()\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = RagasEvaluatorChain(metric=context_precision)\n",
    "context_recall_chain = RagasEvaluatorChain(metric=context_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
