{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# this notebook assumes the following env vars exist in a .env file:\n",
    "#\n",
    "# ASTRA_DB_ENDPOINT\n",
    "# ASTRA_DB_TOKEN\n",
    "# AZURE_OPENAI_ENDPOINT\n",
    "# AZURE_OPENAI_API_KEY\n",
    "# OPENAI_API_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure LLMs for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Azure-based models\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "temperature = 0.0\n",
    "\n",
    "gpt_35_turbo = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_35_turbo_16k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    model_version=\"1106-preview\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4_32k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4-32k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init an AstraDB vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.astradb import AstraDB\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "collection_name = \"open_ai_512\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "astra_db_vstore = AstraDB(\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# base_path = \"./data/\"\n",
    "\n",
    "# datasets = {}\n",
    "# golden_set = []\n",
    "\n",
    "# for name in os.listdir(base_path):\n",
    "#     if os.path.isdir(os.path.join(base_path, name)):\n",
    "#         datasets[name] = []\n",
    "#         with open(os.path.join(base_path, name, \"rag_dataset.json\")) as f:\n",
    "#             examples = json.load(f)[\"examples\"]\n",
    "#             index = 0\n",
    "#             for e in examples:\n",
    "#                 datasets[name].append(e[\"query\"])\n",
    "#                 golden_set.append(\n",
    "#                     {\n",
    "#                         \"query\": e[\"query\"],\n",
    "#                         \"response\": e[\"reference_answer\"],\n",
    "#                     }\n",
    "#                 )\n",
    "#                 index += 1\n",
    "#             print(\"Loaded dataset: \", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the text of a short story that will be indexed in the vector store\n",
    "! curl https://raw.githubusercontent.com/CassioML/cassio-website/main/docs/frameworks/langchain/texts/amontillado.txt --output amontillado.txt\n",
    "inputs = [\"amontillado.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"amontillado.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# text_splitter = TokenTextSplitter(\n",
    "#     chunk_size=512, chunk_overlap=32,\n",
    "# )\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=128, chunk_overlap=16,\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# astra_db_vstore.add_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_openai import ChatOpenAi\n",
    "from langchain import hub\n",
    "\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "retriever = astra_db_vstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retrieval_tool\",\n",
    "    description=\"Retrieves documents relevant to the given prompt\",\n",
    ")\n",
    "tools = [retriever_tool]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "llm = ChatOpenAI(model=model)\n",
    "agent = initialize_agent(llm=llm, tools=tools, handle_parsing_errors=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a query\n",
    "response = agent.invoke(\"Based on the morals the story, what is the theme? \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "# tru = Tru(database_url=os.getenv(\"TRULENS_DB_CONN_STRING\"))\n",
    "tru = Tru()\n",
    "# tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the golden set of questions and ground_truths\n",
    "! curl -X GET \"https://datasets-server.huggingface.co/rows?dataset=explodinggradients%2Ffiqa&config=main&split=train&offset=0\" --output train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "golden_set = []\n",
    "\n",
    "with open(\"train.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    rows = data[\"rows\"]\n",
    "    for row in rows:\n",
    "        row = row[\"row\"]\n",
    "        entry = {\"query\": row[\"question\"], \"response\": row[\"ground_truths\"]}\n",
    "        golden_set.append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(golden_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Feedback Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import AzureOpenAI, OpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruChain, Feedback\n",
    "from trulens_eval.app import App, Select\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "# azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "provider = OpenAI(model_engine=model)\n",
    "# context = App.select_context(retriever)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "grounded = Groundedness(groundedness_provider=provider)\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness (A to C)\")\n",
    "    .on(Select.RecordCalls.tools[0]._run.rets[:].page_content.collect())\n",
    "    # .on(get_page_content(Select.RecordCalls.tools[0]._run.rets))\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons, name=\"Answer Relevance (A to Q)\"\n",
    ").on_input_output()\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.qs_relevance_with_cot_reasons, name=\"Context Relevance (C to Q)\")\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.tools[0]._run.rets[:].page_content.collect())\n",
    "    # .on(get_page_content(Select.RecordCalls.tools[0]._run.rets))\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "ground_truth_agreement = GroundTruthAgreement(golden_set, provider=provider)\n",
    "f_ground_truth = (\n",
    "    Feedback(ground_truth_agreement.agreement_measure, name=\"Ground Truth Agreement (A to GT)\")\n",
    "    .on_input_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_questions = [\n",
    "#     \"What is the theme of the story?\",\n",
    "#     \"What is the Cask of Amontillado?\",\n",
    "#     \"What motivates Montresor to seek revenge against Fortunato, and how does Poe reveal this motivation to the reader\",\n",
    "#     \"Analyze the character of Fortunato. How does Poe portray him, and what is the reader's impression of him?\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# The number of questions in the dataset to evaluate\n",
    "num_questions = 20\n",
    "\n",
    "apps = []\n",
    "for i in range(2):\n",
    "    app = f\"fiqa_agent_{i}_{model}\"\n",
    "    apps.append(app)\n",
    "\n",
    "for app in apps:\n",
    "    tru_recorder = TruChain(\n",
    "        agent,\n",
    "        app_id=app,\n",
    "        feedbacks=[\n",
    "            f_answer_relevance,\n",
    "            f_context_relevance,\n",
    "            f_groundedness,\n",
    "            f_ground_truth,\n",
    "        ],\n",
    "        feedback_mode=\"deferred\",\n",
    "    )\n",
    "\n",
    "    with tru_recorder as _:\n",
    "        for i in range(0, num_questions):\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    q = golden_set[i][\"query\"]\n",
    "                    response = agent.invoke(q)\n",
    "                    print(f\"\\nQ: {q}\\nResponse: {response}\")\n",
    "                    break  # Exit the loop if invoke is successful\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Encountered an exception: {e}. Backing off for {2 ** attempt} seconds.\"\n",
    "                    )\n",
    "                    time.sleep(2**attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start evaluator. Let this process run until all evaluation are complete!\n",
    "tru.start_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.stop_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.stop_dashboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
