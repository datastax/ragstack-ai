{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup LangSmith\n",
    "from langsmith import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in client.list_datasets():\n",
    "    print(dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in client.list_projects(reference_dataset_name=\"blockchain_solana\"):\n",
    "    print(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.feedback_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"id\": f\"{project.id}\",\n",
    "    \"name\": project.name,\n",
    "    \"url\": project.url,\n",
    "    \"dataset\" : project.extra[\"metadata\"][\"dataset\"],\n",
    "    \"collection\" : project.extra[\"metadata\"][\"collection\"],\n",
    "    \"eval-model\": project.extra[\"metadata\"][\"eval-model\"],\n",
    "    \"eval-run\": project.extra[\"metadata\"][\"run\"],\n",
    "    \"start_time\": project.start_time.isoformat(),\n",
    "    \"last_run_start_time\": project.last_run_start_time.isoformat(),\n",
    "    \"run_count\": project.run_count,\n",
    "    \"latency_p50\": project.latency_p50.total_seconds(),\n",
    "    \"latency_p99\": project.latency_p99.total_seconds(),\n",
    "    \"prompt_tokens\": project.prompt_tokens,\n",
    "    \"completion_tokens\": project.completion_tokens,\n",
    "    \"total_tokens\" : project.total_tokens,\n",
    "    \"answer_correctness_n\" : project.feedback_stats[\"answer_correctness_score\"][\"n\"],\n",
    "    \"answer_correctness_avg\" : project.feedback_stats[\"answer_correctness_score\"][\"avg\"],\n",
    "    \"answer_relevancy_n\" : project.feedback_stats[\"answer_relevancy_score\"][\"n\"],\n",
    "    \"answer_relevancy_avg\" : project.feedback_stats[\"answer_relevancy_score\"][\"avg\"],\n",
    "    \"context_recall_n\": project.feedback_stats[\"context_recall_score\"][\"n\"],\n",
    "    \"context_recall_avg\": project.feedback_stats[\"context_recall_score\"][\"avg\"],\n",
    "    \"context_relevancy_n\": project.feedback_stats[\"context_relevancy_score\"][\"n\"],\n",
    "    \"context_relevancy_avg\": project.feedback_stats[\"context_relevancy_score\"][\"avg\"],\n",
    "    \"faithfulness_n\": project.feedback_stats[\"faithfulness_score\"][\"n\"],\n",
    "    \"faithfulness_avg\": project.feedback_stats[\"faithfulness_score\"][\"avg\"],\n",
    "    }\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"results.jsonl\", \"w\") as out_file:\n",
    "    for dataset in client.list_datasets():\n",
    "        for project in client.list_projects(reference_dataset_id=dataset.id):\n",
    "            if project.total_tokens:\n",
    "                results = {\n",
    "                    \"id\": f\"{project.id}\",\n",
    "                    \"name\": project.name,\n",
    "                    \"url\": project.url,\n",
    "                    \"dataset\" : project.extra[\"metadata\"][\"dataset\"],\n",
    "                    \"collection\" : project.extra[\"metadata\"][\"collection\"],\n",
    "                    \"eval-model\": project.extra[\"metadata\"][\"eval-model\"],\n",
    "                    \"eval-run\": project.extra[\"metadata\"][\"run\"],\n",
    "                    \"start_time\": project.start_time.isoformat(),\n",
    "                    \"last_run_start_time\": project.last_run_start_time.isoformat(),\n",
    "                    \"run_count\": project.run_count,\n",
    "                    \"latency_p50\": project.latency_p50.total_seconds(),\n",
    "                    \"latency_p99\": project.latency_p99.total_seconds(),\n",
    "                    \"prompt_tokens\": project.prompt_tokens,\n",
    "                    \"completion_tokens\": project.completion_tokens,\n",
    "                    \"total_tokens\" : project.total_tokens,\n",
    "                    \"answer_correctness_n\" : project.feedback_stats[\"answer_correctness_score\"][\"n\"],\n",
    "                    \"answer_correctness_avg\" : project.feedback_stats[\"answer_correctness_score\"][\"avg\"],\n",
    "                    \"answer_relevancy_n\" : project.feedback_stats[\"answer_relevancy_score\"][\"n\"],\n",
    "                    \"answer_relevancy_avg\" : project.feedback_stats[\"answer_relevancy_score\"][\"avg\"],\n",
    "                    \"context_recall_n\": project.feedback_stats[\"context_recall_score\"][\"n\"],\n",
    "                    \"context_recall_avg\": project.feedback_stats[\"context_recall_score\"][\"avg\"],\n",
    "                    \"context_relevancy_n\": project.feedback_stats[\"context_relevancy_score\"][\"n\"],\n",
    "                    \"context_relevancy_avg\": project.feedback_stats[\"context_relevancy_score\"][\"avg\"],\n",
    "                    \"faithfulness_n\": project.feedback_stats[\"faithfulness_score\"][\"n\"],\n",
    "                    \"faithfulness_avg\": project.feedback_stats[\"faithfulness_score\"][\"avg\"],\n",
    "                }\n",
    "            else:\n",
    "                results = {\n",
    "                    \"id\": f\"{project.id}\",\n",
    "                    \"name\": project.name,\n",
    "                    \"url\": project.url,\n",
    "                    \"dataset\" : project.extra[\"metadata\"][\"dataset\"],\n",
    "                    \"collection\" : project.extra[\"metadata\"][\"collection\"],\n",
    "                    \"eval-model\": project.extra[\"metadata\"][\"eval-model\"],\n",
    "                    \"eval-run\": project.extra[\"metadata\"][\"run\"],\n",
    "                    \"start_time\": project.start_time.isoformat(),\n",
    "                    \"last_run_start_time\": None,\n",
    "                    \"run_count\": project.run_count,\n",
    "                    \"latency_p50\": None,\n",
    "                    \"latency_p99\": None,\n",
    "                    \"prompt_tokens\": project.prompt_tokens,\n",
    "                    \"completion_tokens\": project.completion_tokens,\n",
    "                    \"total_tokens\" : project.total_tokens,\n",
    "                    \"answer_correctness_n\" : None,\n",
    "                    \"answer_correctness_avg\" : None,\n",
    "                    \"answer_relevancy_n\" : None,\n",
    "                    \"answer_relevancy_avg\" : None,\n",
    "                    \"context_recall_n\": None,\n",
    "                    \"context_recall_avg\": None,\n",
    "                    \"context_relevancy_n\": None,\n",
    "                    \"context_relevancy_avg\": None,\n",
    "                    \"faithfulness_n\": None,\n",
    "                    \"faithfulness_avg\": None,\n",
    "                }\n",
    "\n",
    "            out_file.write(json.dumps(results) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_json(\"results.jsonl\", orient=\"records\", lines=True)\n",
    "df.to_csv(\"results.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TruLens Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "import os\n",
    "\n",
    "tru = Tru(database_url=os.getenv(\"TRULENS_DB_CONN_STRING\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_ids_to_example_count = {}\n",
    "\n",
    "for app in tru.get_apps():\n",
    "    app_id = app[\"app_id\"]\n",
    "    dfRecords, feedbackColumns = tru.get_records_and_feedback([app_id])\n",
    "    app_ids_to_example_count[app_id] = len(dfRecords)\n",
    "\n",
    "app_ids_to_example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = tru.get_leaderboard(app_ids=app_ids_to_example_count.keys())\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with specified columns and data types\n",
    "df = pd.DataFrame(columns=[\"langchain\", \"llamaindex\"])\n",
    "df = df.astype({\"langchain\": \"float\", \"llamaindex\": \"float\"})\n",
    "\n",
    "# Populate empty rows with dataset names\n",
    "for index, row in board.iterrows():\n",
    "    parts = index.split(\"_\")\n",
    "    tool = parts[0]\n",
    "    dataset = \"_\".join(parts[3:])\n",
    "\n",
    "    if dataset not in df.index:\n",
    "        df.loc[dataset] = [None, None]\n",
    "\n",
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of the empty dataFrame for specific measurements\n",
    "groundedness = df.copy(deep=True)\n",
    "answer_relevance = df.copy(deep=True)\n",
    "context_relevance = df.copy(deep=True)\n",
    "answer_correctness = df.copy(deep=True)\n",
    "latency = df.copy(deep=True)\n",
    "total_cost = df.copy(deep=True)\n",
    "\n",
    "for index, row in board.iterrows():\n",
    "    parts = index.split(\"_\")\n",
    "    tool = parts[0]\n",
    "    dataset = \"_\".join(parts[3:])\n",
    "\n",
    "    if tool == \"lc\":\n",
    "        tool = \"langchain\"\n",
    "    elif tool == \"llama\":\n",
    "        tool = \"llamaindex\"\n",
    "\n",
    "    groundedness.at[dataset, tool] = row[\"groundedness_measure_with_cot_reasons\"]\n",
    "    answer_relevance.at[dataset, tool] = row[\"relevance_with_cot_reasons\"]\n",
    "    context_relevance.at[dataset, tool] = row[\"qs_relevance_with_cot_reasons\"]\n",
    "    answer_correctness.at[dataset, tool] = row[\"agreement_measure\"]\n",
    "    latency.at[dataset, tool] = row[\"latency\"]\n",
    "    total_cost.at[dataset, tool] = row[\"total_cost\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = groundedness.plot.bar(colormap=ListedColormap(['#0671b7', '#f678a7']), ax=ax)\n",
    "\n",
    "# Some styling tweaks using Matplotlib\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_title('Groundedness (answer supported by the context)')\n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = answer_relevance.plot.bar(colormap=ListedColormap(['#0671b7', '#f678a7']), ax=ax)\n",
    "\n",
    "# Some styling tweaks using Matplotlib\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylim([0.75, 1])\n",
    "ax.set_title('Answer Relevance (answer relevance to query)')\n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = context_relevance.plot.bar(colormap=ListedColormap(['#0671b7', '#f678a7']), ax=ax)\n",
    "\n",
    "# Some styling tweaks using Matplotlib\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_title('Context Relevance (context relevance to query)')\n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = answer_correctness.plot.bar(colormap=ListedColormap(['#0671b7', '#f678a7']), ax=ax)\n",
    "\n",
    "# Some styling tweaks using Matplotlib\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_title('Answer Correctness (answer compared to ground)')\n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = latency.plot.bar(colormap=ListedColormap(['#0671b7', '#f678a7']), ax=ax)\n",
    "\n",
    "# Some styling tweaks using Matplotlib\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Dataset')\n",
    "#ax.set_ylim([0, 1])\n",
    "ax.set_title('Latency')\n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = total_cost.plot.bar(colormap=ListedColormap(['#0671b7', '#f678a7']), ax=ax)\n",
    "\n",
    "# Some styling tweaks using Matplotlib\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Dataset')\n",
    "#ax.set_ylim([0, 1])\n",
    "ax.set_title('Total Cost')\n",
    "fig.tight_layout(pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thorough Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps = {}\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"record_id\", \"input\", \"output\", \"tags\",\n",
    "    \"groundedness_measure_with_cot_reasons\",\n",
    "    \"relevance_with_cot_reasons\",\n",
    "    \"qs_relevance_with_cot_reasons\",\n",
    "    \"agreement_measure\",\n",
    "    \"latency\", \"total_tokens\", \"total_cost\"]\n",
    "\n",
    "for app in tru.get_apps():\n",
    "    app_id = app[\"app_id\"]\n",
    "    dfRecords, feedbackColumns = tru.get_records_and_feedback([app_id])\n",
    "    apps[app_id] = dfRecords[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test for normality, when used in the context of comparing two methods on the same set of test cases (especially in a paired test scenario), implies that each data point in one dataset corresponds to a data point in the other dataset. In other words, the data points are paired.\n",
    "\n",
    "This is particularly relevant when you want to:\n",
    "\n",
    "1. **Perform a Paired Sample Test**: In a paired sample t-test or a Wilcoxon signed-rank test, the difference between each pair of observations is crucial. These tests are based on the differences within each pair (i.e., each data point in Method 1 is subtracted from the corresponding data point in Method 2). Therefore, it's important that the two datasets are aligned such that each data point in one dataset has a direct, corresponding data point in the other dataset.\n",
    "\n",
    "1. **Test for Normality in Paired Differences**: When using the Shapiro-Wilk test in this context, you're typically testing the normality of these differences, not the individual datasets. Therefore, the datasets must be paired correctly before calculating these differences.\n",
    "\n",
    "For example, if you have a list of test cases and you apply Method 1 and Method 2 to each test case, you should ensure that the results for each method are aligned such that the result of Method 1 for Test Case 1 is in the same position (same index) as the result of Method 2 for Test Case 1, and so on for all test cases.\n",
    "\n",
    "In summary, for paired analyses, the order and pairing of data points between datasets are crucial. The results for each test case from each method need to be correctly aligned for the paired analysis to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-arrange data, and sort all dataFrames by the `input` column\n",
    "\n",
    "dataFrames = {}\n",
    "\n",
    "for app_id in apps:\n",
    "    parts = app_id.split(\"_\")\n",
    "    tool = parts[0]\n",
    "    dataset = \"_\".join(parts[3:])\n",
    "\n",
    "    if dataset not in dataFrames:\n",
    "        dataFrames[dataset] = {}\n",
    "\n",
    "    dataFrames[dataset][tool] = apps[app_id].sort_values(by=\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "columns_to_test = [\n",
    "    \"groundedness_measure_with_cot_reasons\",\n",
    "    \"relevance_with_cot_reasons\",\n",
    "    \"qs_relevance_with_cot_reasons\",\n",
    "    \"agreement_measure\",\n",
    "    \"latency\"]\n",
    "\n",
    "for dataset in dataFrames:\n",
    "    print(dataset)\n",
    "    lc_df = dataFrames[dataset][\"lc\"]\n",
    "    llama_df = dataFrames[dataset][\"llama\"]\n",
    "    if len(lc_df) != len(llama_df):\n",
    "        print(\"\\tDataFrames do not have equal row counts, skipping :(\")\n",
    "        continue\n",
    "    for test in columns_to_test:\n",
    "        print(f\"\\tTesting normality of {test}:\")\n",
    "        lc_values = lc_df[test].to_list()\n",
    "        llama_values = llama_df[test].to_list()\n",
    "\n",
    "        # Calculate the differences\n",
    "        differences = [x - y for x, y in zip(lc_values, llama_values)]\n",
    "\n",
    "        # Perform the Shapiro-Wilk Test\n",
    "        statistic, p_value = stats.shapiro(differences)\n",
    "\n",
    "        print(f\"\\t\\tShapiro-Wilk Test statistic: {statistic}\")\n",
    "        print(\"\\t\\tP-value:\", p_value)\n",
    "\n",
    "        # Interpretation\n",
    "        alpha = 0.05\n",
    "        if p_value > alpha:\n",
    "            print(\"\\t\\t\\tData follows a normal distribution (fail to reject H0)\")\n",
    "        else:\n",
    "            print(\"\\t\\t\\tData does NOT follow a normal distribution (reject H0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Most datasets comparisons do NOT follow a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Betterness\n",
    "\n",
    "**Wilcoxon Signed-Rank Test**: This is a non-parametric alternative to the paired sample t-test and is used to compare two related samples or repeated measurements on a single sample to assess whether their population mean ranks differ. It's appropriate for your scenario where you have paired data (the same cases tested with two different methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "columns_to_test = [\n",
    "    \"groundedness_measure_with_cot_reasons\",\n",
    "    \"relevance_with_cot_reasons\",\n",
    "    \"qs_relevance_with_cot_reasons\",\n",
    "    \"agreement_measure\",\n",
    "    \"latency\"]\n",
    "\n",
    "column_translation = {\n",
    "    \"groundedness_measure_with_cot_reasons\": \"groundedness\",\n",
    "    \"relevance_with_cot_reasons\" : \"answer_relevance\",\n",
    "    \"qs_relevance_with_cot_reasons\" : \"context_relevance\",\n",
    "    \"agreement_measure\" : \"answer_correctness\",\n",
    "    \"latency\" : \"latency\"\n",
    "}\n",
    "\n",
    "for dataset in dataFrames:\n",
    "    print(dataset)\n",
    "    # Assuming lc_df and llama_df are paired datasets\n",
    "    lc_df = dataFrames[dataset][\"lc\"]\n",
    "    llama_df = dataFrames[dataset][\"llama\"]\n",
    "    if len(lc_df) != len(llama_df):\n",
    "        print(\"\\tDataFrames do not have equal row counts, skipping :(\")\n",
    "        continue\n",
    "    for test in columns_to_test:\n",
    "        translated = column_translation[test]\n",
    "        print(f\"\\tTesting differences of {translated}:\")\n",
    "        lc_values = lc_df[test].to_list()\n",
    "        llama_values = llama_df[test].to_list()\n",
    "\n",
    "        # Perform the Wilcoxon Signed-Rank Test\n",
    "        stat, p = stats.wilcoxon(lc_values, llama_values)\n",
    "        print('\\t\\tStatistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "        # Calculate the median of the differences\n",
    "        differences = np.array(llama_values) - np.array(lc_values)\n",
    "        median_difference = np.median(differences)\n",
    "        print('\\t\\tMedian of Differences:', median_difference)\n",
    "\n",
    "        # Interpretation\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            print('\\t\\t\\tSame distribution (fail to reject H0)')\n",
    "        else:\n",
    "            print('\\t\\t\\tDifferent distribution (reject H0)')\n",
    "            if median_difference > 0:\n",
    "                print('\\t\\t\\t\\tLlama generally scores higher.')\n",
    "            elif median_difference < 0:\n",
    "                print('\\t\\t\\t\\tLangchain generally scores higher.')\n",
    "            else:\n",
    "                print('\\t\\t\\t\\tNo difference in the median scores.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataFrames.keys():\n",
    "    for tool_name in dataFrames[dataset_name].keys():\n",
    "        dataFrames[dataset_name][tool_name].to_json(f\"./results/tool_comparison/{dataset_name}-{tool_name}.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
