{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q ragstack-ai ragas==0.0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Azure-based models\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "temperature = 0.0\n",
    "\n",
    "gpt_35_turbo = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_35_turbo_16k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4 = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4\",\n",
    "    model_version=\"1106-preview\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "gpt_4_32k = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-4-32k\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "## Setup Vector Store\n",
    "from langchain.vectorstores.astradb import AstraDB\n",
    "import os\n",
    "vstore = AstraDB(\n",
    "    collection_name=\"open_ai_512\",\n",
    "    embedding=embeddings,\n",
    "    token=os.getenv(\"ASTRA_DB_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_ENDPOINT\")\n",
    ")\n",
    "\n",
    "## Setup LangSmith\n",
    "from langsmith import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "base_path = \"./data/\"\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for name in os.listdir(base_path):\n",
    "    if os.path.isdir(os.path.join(base_path, name)):\n",
    "        if not client.has_dataset(dataset_name=name):\n",
    "            # if not create a new one with the generated query examples\n",
    "            dataset = client.create_dataset(\n",
    "                dataset_name=name, description=f\"{name} dataset\"\n",
    "            )\n",
    "\n",
    "            with open(os.path.join(base_path, name, \"rag_dataset.json\")) as f:\n",
    "                examples = json.load(f)['examples']\n",
    "\n",
    "                for e in examples:\n",
    "                    exp = client.create_example(\n",
    "                        inputs={\"query\": e[\"query\"]},\n",
    "                        outputs={\"ground_truths\": [e[\"reference_answer\"]]},\n",
    "                        dataset_id=dataset.id,\n",
    "                    )\n",
    "\n",
    "                print(\"Created a new dataset: \", dataset.name)\n",
    "\n",
    "        # load the (new) dataset\n",
    "        datasets[name] = client.read_dataset(dataset_name=name)\n",
    "        print(\"Loaded dataset: \", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from ragas.llms import LangchainLLM\n",
    "from ragas.metrics import answer_correctness, answer_relevancy, context_recall, context_relevancy, answer_similarity, faithfulness\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# build a prompt\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say: \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# factory function that return a new qa chain\n",
    "# this is so state is not reused when running each example\n",
    "def create_qa_chain(return_context=True):\n",
    "    qa_chain = RetrievalQA.from_llm(\n",
    "        llm=gpt_35_turbo,\n",
    "        prompt=prompt,\n",
    "        retriever=vstore.as_retriever(),\n",
    "        return_source_documents=return_context,\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "## force ragas evaluators to use azure models instead of openai models\n",
    "\n",
    "# embeddings can be used as it is\n",
    "answer_relevancy.embeddings = embeddings\n",
    "answer_similarity.embeddings = embeddings\n",
    "\n",
    "# wrappers around azure models\n",
    "ragas_gpt4 = LangchainLLM(gpt_4)\n",
    "ragas_gpt35 = LangchainLLM(gpt_35_turbo)\n",
    "\n",
    "# patch the ragas evaluators\n",
    "answer_correctness.llm = ragas_gpt35\n",
    "answer_relevancy.llm = ragas_gpt35\n",
    "answer_similarity.llm = ragas_gpt35\n",
    "context_relevancy.llm = ragas_gpt35\n",
    "context_recall.llm = ragas_gpt35\n",
    "faithfulness.llm = ragas_gpt35\n",
    "\n",
    "answer_correctness.answer_similarity = answer_similarity\n",
    "answer_correctness.faithfulness = faithfulness\n",
    "\n",
    "# wrap evaluators for LangSmith\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_cor_chain = RagasEvaluatorChain(metric=answer_correctness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = RagasEvaluatorChain(metric=context_relevancy)\n",
    "context_rec_chain = RagasEvaluatorChain(metric=context_recall)\n",
    "\n",
    "# create a method to run evaluation on a dataset\n",
    "def run_evaluation(dataset_name, project_name: str | None = None, project_metadata: Dict[str, Any] | None = None):\n",
    "    evaluation_config = RunEvalConfig(\n",
    "        custom_evaluators=[\n",
    "            context_rec_chain,\n",
    "            answer_cor_chain,\n",
    "            faithfulness_chain,\n",
    "            answer_rel_chain,\n",
    "            context_rel_chain,\n",
    "        ],\n",
    "        prediction_key=\"result\",\n",
    "    )\n",
    "\n",
    "    return run_on_dataset(\n",
    "        client,\n",
    "        dataset_name,\n",
    "        create_qa_chain,\n",
    "        evaluation=evaluation_config,\n",
    "        project_metadata=project_metadata,\n",
    "        project_name=project_name,\n",
    "        input_mapper=lambda x: x\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    result = run_evaluation(\"origin_of_covid_19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
