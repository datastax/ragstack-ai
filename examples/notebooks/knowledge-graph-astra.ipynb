{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QV7vLoUxmE-"
      },
      "source": [
        "# Introduction / Setup\n",
        "\n",
        "This notebook shows how to use LangChain's [`LLMGraphTransformer`](https://python.langchain.com/docs/use_cases/graph/constructing/#llm-graph-transformer) to extract knowledge triples and store them in [DataStax AstraDB](https://www.datastax.com/products/datastax-astra)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZq4cwyvxmFA"
      },
      "outputs": [],
      "source": [
        "# (Optional) When developing locally, this reloads the module code when changes are made,\n",
        "# making it easier to iterate.\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7GTp6u2xmFB",
        "outputId": "54252903-566c-457f-fc00-f3c978c43ad5"
      },
      "outputs": [],
      "source": [
        "# (Required in Colab) Install the knowledge graph library from the repository.\n",
        "# This will also install the dependencies.\n",
        "%pip install https://github.com/datastax-labs/knowledge-graphs-langchain/archive/main.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gllSoDEbxmFB"
      },
      "source": [
        "## Environment\n",
        "Pick one of the following.\n",
        "1. If you're just running the notebook, it's probably best to run the cell using `getpass` to set the necessary\n",
        "   environment variables.\n",
        "1. If you're developing, it's likely easiest to create a `.env` file and store the necessary credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIEtcs1gxmFB"
      },
      "outputs": [],
      "source": [
        "# (Option 1) - Set the environment variables from getpass.\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI API Key: \")\n",
        "os.environ[\"ASTRA_DB_DATABASE_ID\"] = input(\"Enter Astra DB Database ID: \")\n",
        "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass.getpass(\"Enter Astra DB Application Token: \")\n",
        "\n",
        "keyspace = input(\"Enter Astra DB Keyspace (Empty for default): \")\n",
        "if keyspace:\n",
        "    os.environ[\"ASTRA_DB_KEYSPACE\"] = keyspace\n",
        "else:\n",
        "    os.environ.pop(\"ASTRA_DB_KEYSPACE\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_bvEQBtxmFB",
        "outputId": "40ddfdca-85a3-44c0-876f-7e0716c2d98c"
      },
      "outputs": [],
      "source": [
        "# (Option 2) - Load the `.env` file.\n",
        "# See `env.template` for an example of what you should have there.\n",
        "%pip install python-dotenv\n",
        "import dotenv\n",
        "dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVOcSnaVxmFC"
      },
      "source": [
        "## Initialize Astra DB / Cassandra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu1mD-iHxmFC"
      },
      "outputs": [],
      "source": [
        "# Initialize cassandra connection from environment variables).\n",
        "import cassio\n",
        "cassio.init(auto=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJguSB_xxmFC"
      },
      "source": [
        "## Create Graph Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuxGzUsDxmFC"
      },
      "outputs": [],
      "source": [
        "# Create graph store.\n",
        "from knowledge_graph.cassandra_graph_store import CassandraGraphStore\n",
        "graph_store = CassandraGraphStore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1A-cAirxmFC"
      },
      "source": [
        "# Extracting Knowledge Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO3MAqxYxmFC"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Prompt used by LLMGraphTransformer is tuned for Gpt4.\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
        "\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3caTswdxmFD",
        "outputId": "b896e0a4-765e-4fa4-d770-7479aecd4f99"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "text = \"\"\"\n",
        "Marie Curie, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
        "She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
        "Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
        "She was, in 1906, the first woman to become a professor at the University of Paris.\n",
        "\"\"\"\n",
        "documents = [Document(page_content=text)]\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
        "print(f\"Relationships:{graph_documents[0].relationships}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKB0ABgVxmFD",
        "outputId": "0475f58a-c567-400c-c1ed-79cf1af90d5a"
      },
      "outputs": [],
      "source": [
        "# Render the extracted graph to GraphViz.\n",
        "from knowledge_graph.render import render_graph_documents\n",
        "render_graph_documents(graph_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auwoEFM6xmFD"
      },
      "outputs": [],
      "source": [
        "# Save the extracted graph documents to the AstraDB / Cassandra Graph Store.\n",
        "graph_store.add_graph_documents(graph_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaeVA6DexmFD"
      },
      "source": [
        "### Optional: Predefine entities / relationships\n",
        "\n",
        "The below shows how to configure the `LLMGraphTransformer` with specific kinds of nodes and relationships it is allowed to extract.\n",
        "This is useful for constraining what will be extracted.\n",
        "\n",
        "```python\n",
        "llm_transformer_filtered = LLMGraphTransformer(\n",
        "    llm=llm,\n",
        "    allowed_nodes=[\"Person\", \"Country\", \"Organization\"],\n",
        "    allowed_relationships=[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],\n",
        ")\n",
        "graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
        "    documents\n",
        ")\n",
        "print(f\"Nodes:{graph_documents_filtered[0].nodes}\")\n",
        "print(f\"Relationships:{graph_documents_filtered[0].relationships}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH2LmgK9xmFD"
      },
      "source": [
        "# Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mEM0zkIxmFD"
      },
      "source": [
        "We can query the `GraphStore` directly. The `as_runnable` method takes some configuration for how to extract the subgraph and returns a LangChain `Runnable` which can be invoked on a node or sequence of nodes to traverse from those starting points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfX2ETyrxmFD",
        "outputId": "d4497173-ff7c-44c4-837c-14d7b78858a8"
      },
      "outputs": [],
      "source": [
        "from knowledge_graph.traverse import Node\n",
        "\n",
        "graph_store.as_runnable(steps=2).invoke(Node(\"Marie Curie\", \"Person\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMO-lFFvxmFD"
      },
      "source": [
        "For getting started, the library also provides a `Runnable` for extracting the starting entities from a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZotgyV3AxmFD",
        "outputId": "c001a0d4-7338-4f93-f613-5e7512e327c7"
      },
      "outputs": [],
      "source": [
        "# Example showing extracted entities (nodes)\n",
        "from knowledge_graph import extract_entities\n",
        "extract_entities(llm).invoke({ \"question\": \"Who is Marie Curie?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAYRXj7-xmFD"
      },
      "source": [
        "## Query Chain\n",
        "\n",
        "We'll create a chain which does the following:\n",
        "\n",
        "1. Use the entity extraction `Runnable` from the library in order to determine the starting points.\n",
        "2. Retrieve the sub-knowledge graphs starting from those nodes.\n",
        "3. Create a context containing those knowledge triples.\n",
        "4. Apply the LLM to answer the question given the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJUth70xxmFD"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from knowledge_graph import extract_entities\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model_name = \"gpt-4\")\n",
        "\n",
        "def _combine_relations(relations):\n",
        "    return \"\\n\".join(map(repr, relations))\n",
        "\n",
        "ANSWER_PROMPT = (\n",
        "    \"The original question is given below.\"\n",
        "    \"This question has been used to retrieve information from a knowledge graph.\"\n",
        "    \"The matching triples are shown below.\"\n",
        "    \"Use the information in the triples to answer the original question.\\n\\n\"\n",
        "    \"Original Question: {question}\\n\\n\"\n",
        "    \"Knowledge Graph Triples:\\n{context}\\n\\n\"\n",
        "    \"Response:\"\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    { \"question\": RunnablePassthrough() }\n",
        "    | RunnablePassthrough.assign(entities = extract_entities(llm))\n",
        "    | RunnablePassthrough.assign(triples = itemgetter(\"entities\") | graph_store.as_runnable())\n",
        "    | RunnablePassthrough.assign(context = itemgetter(\"triples\") | RunnableLambda(_combine_relations))\n",
        "    | ChatPromptTemplate.from_messages([ANSWER_PROMPT])\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIv3Ka2bxmFD"
      },
      "source": [
        "## Example\n",
        "And finally, we can run the chain end to end to answer a question using the retrieved knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "towGxGCcxmFE",
        "outputId": "9c1f9843-b3e5-44cc-9ca7-ed4ca77e97ba"
      },
      "outputs": [],
      "source": [
        "chain.invoke(\"Who is Marie Curie?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "knowledge-graph-bxUBmW8M-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
