{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/nvidia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NeMo Guardrails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will need a vector-enabled Astra database. This notebook uses OpenAI, though you can certainly use the NVIDIA models, as NeMo [supports all LLM providers supported by Langchain](https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/user_guides/configuration-guide.md#supported-llm-models).\n",
    "\n",
    "* Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
    "* Create an [OpenAI account](https://openai.com/)\n",
    "* Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
    "* Get your Astra DB Endpoint: \n",
    "  * `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`\n",
    "\n",
    "See the [Prerequisites](https://docs.datastax.com/en/ragstack/docs/prerequisites.html) page for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbmake": {
     "post_cell_execute": [
      "# astrapy required to get raw client to truncate collection. Can be removed once timeouts are resolved.\n",
      "! pip install -qU astrapy\n"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU ragstack-ai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbmake": {
     "post_cell_execute": [
      "import string\n",
      "import random\n",
      "collection = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\n"
     ]
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your settings for Astra DB and OpenAI:\n",
    "keys = [\"ASTRA_DB_APPLICATION_TOKEN\", \"ASTRA_DB_API_ENDPOINT\", \"NVIDIA_API_KEY\"]\n",
    "for key in keys:\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass(f\"Enter {key}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Collections are where documents are stored. ex: test\n",
    "collection = input(\"Collection: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import RailsConfig\n",
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "\n",
    "\n",
    "          # Your task is to check if the user message below complies with the company policy for talking with the company bot.\n",
    "def config(engine, model):\n",
    "    return f\"\"\"\n",
    "    models:\n",
    "      - type: main\n",
    "        engine: {engine}\n",
    "        model: {model}\n",
    "\n",
    "    prompts:\n",
    "      - task: self_check_input\n",
    "        content: |\n",
    "          Your task is to check if the user message below complies with the following policies. \n",
    "                \n",
    "          Policy for the user messages:\n",
    "          - should not contain fruits\n",
    "          - should not contain vegetables\n",
    "\n",
    "          User message: \"{{ user_input }}\"\n",
    "\n",
    "          Question: Should the user message be blocked (Yes or No)?\n",
    "          Answer:\n",
    "    \n",
    "    rails:\n",
    "      input:\n",
    "        flows:\n",
    "          - self check input\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "engine = \"openai\"\n",
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "yaml = config(engine, model_name)\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml,\n",
    ")\n",
    "guardrails = RunnableRails(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astra vector store configured\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.astradb import AstraDB\n",
    "\n",
    "vstore = AstraDB(\n",
    "    collection_name=collection,\n",
    "    embedding=embedding,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    ")\n",
    "print(\"Astra vector store configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbmake": {
     "post_cell_execute": [
      "assert len(inserted_ids) > 0"
     ]
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "SAMPLE_DATA = [\n",
    "    \"MyFakeProductForTesting is a versatile testing tool designed to streamline the testing process for software developers, quality assurance professionals, and product testers. It provides a comprehensive solution for testing various aspects of applications and systems, ensuring robust performance and functionality.\",  # noqa: E501\n",
    "    \"MyFakeProductForTesting comes equipped with an advanced dynamic test scenario generator. This feature allows users to create realistic test scenarios by simulating various user interactions, system inputs, and environmental conditions. The dynamic nature of the generator ensures that tests are not only diverse but also adaptive to changes in the application under test.\",  # noqa: E501\n",
    "    \"The product includes an intelligent bug detection and analysis module. It not only identifies bugs and issues but also provides in-depth analysis and insights into the root causes. The system utilizes machine learning algorithms to categorize and prioritize bugs, making it easier for developers and testers to address critical issues first.\",  # noqa: E501\n",
    "    \"MyFakeProductForTesting first release happened in June 2020.\",\n",
    "]\n",
    "\n",
    "BASIC_QA_PROMPT = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "vstore.add_texts(SAMPLE_DATA)\n",
    "retriever = vstore.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "prompt = PromptTemplate.from_template(BASIC_QA_PROMPT)\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_rails = guardrails | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': \"I'm sorry, I can't respond to that.\"}\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_rails.invoke(\"When was MyFakeProductForTesting first released?\")\n",
    "# assert '2020' in response\n",
    "print(response)\n",
    "# Would have expected an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': \"I'm sorry, I can't respond to that.\"}\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_rails.invoke(\"What color is an apple?\")\n",
    "# assert \"I'm sorry\" in response\n",
    "print(response)\n",
    "# This is expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is the capital of France?\")\n",
    "# response = chain.invoke({\"input\": \"What is the capital of France?\"})\n",
    "# assert \"Paris\" in response\n",
    "print(response)\n",
    "# Would have expected \"I cannot answer based on the context provided.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with slightly different prompt:\n",
    "from nemoguardrails import RailsConfig\n",
    "from nemoguardrails.integrations.langchain.runnable_rails import RunnableRails\n",
    "\n",
    "\n",
    "def config(engine, model):\n",
    "    return f\"\"\"\n",
    "    models:\n",
    "      - type: main\n",
    "        engine: {engine}\n",
    "        model: {model}\n",
    "\n",
    "    prompts:\n",
    "      - task: self_check_input\n",
    "        content: |\n",
    "          Your task is to check if the user message below complies with the company policy for talking with the company bot.\n",
    "                \n",
    "          Company Policy for the user messages:\n",
    "          - should not contain fruits\n",
    "          - should not contain vegetables\n",
    "\n",
    "          User message: \"{{ user_input }}\"\n",
    "\n",
    "          Question: Should the user message be blocked (Yes or No)?\n",
    "          Answer:\n",
    "    \n",
    "    rails:\n",
    "      input:\n",
    "        flows:\n",
    "          - self check input\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "engine = \"openai\"\n",
    "model_name = \"gpt-3.5-turbo-16k\"\n",
    "yaml = config(engine, model_name)\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml,\n",
    ")\n",
    "guardrails = RunnableRails(config)\n",
    "chain_with_rails = guardrails | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFakeProductForTesting was first released in June 2020.\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_rails.invoke(\"When was MyFakeProductForTesting first released?\")\n",
    "# assert '2020' in response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_rails.invoke(\"What color is an apple?\")\n",
    "# assert \"I'm sorry\" in response\n",
    "print(response)\n",
    "# Would have expected {\"output\": \"I'm sorry, ... \"} as per \n",
    "# https://github.com/NVIDIA/NeMo-Guardrails/tree/develop/docs/user_guides/langchain/chain-with-guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer to the question.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is the capital of France?\")\n",
    "# response = chain.invoke({\"input\": \"What is the capital of France?\"})\n",
    "# assert \"Paris\" in response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
