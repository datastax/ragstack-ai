{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/nvidia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NVIDIA NeMo Guardrails\n",
                "\n",
                "This notebooks demonstrates how to set up and use NVIDIA NeMo's Guardrails. [NVIDIA NeMo](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/) is a cloud-native framework designed for AI models. [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) is a toolkit for adding programmable guardrails to LLM-based conversational applications. In this notebook, you will create an RAG-based application featuring Retrieval rails that specifically guard against retrieval of chunks with potentially sensitive content. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "You will need a vector-enabled Astra database. This notebook uses OpenAI, though you can certainly use the NVIDIA models, as NeMo [supports all LLM providers supported by Langchain](https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/user_guides/configuration-guide.md#supported-llm-models).\n",
                "\n",
                "* Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
                "* Create an [OpenAI account](https://openai.com/)\n",
                "* Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
                "* Get your Astra DB Endpoint: \n",
                "  * `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`\n",
                "\n",
                "See the [Prerequisites](https://docs.datastax.com/en/ragstack/docs/prerequisites.html) page for more details."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "`ragstack-ai` includes all the packages you need to build a RAG pipeline. \n",
                "\n",
                "`nemoguardrails` for NeMo guardrails. \n",
                "\n",
                "`langchain-nvidia-ai-endpoints` includes the NVIDIA models.\n",
                "\n",
                "`datasets` is used to import a sample dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "! pip install -qU ragstack-ai nemoguardrails langchain-nvidia-ai-endpoints datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "nbmake": {
                    "post_cell_execute": [
                        "import string\n",
                        "import random\n",
                        "collection = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\n"
                    ]
                }
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from getpass import getpass\n",
                "\n",
                "# Enter your settings for Astra DB and OpenAI:\n",
                "keys = [\"ASTRA_DB_APPLICATION_TOKEN\", \"ASTRA_DB_API_ENDPOINT\", \"NVIDIA_API_KEY\"]\n",
                "for key in keys:\n",
                "    if key not in os.environ:\n",
                "        os.environ[key] = getpass(f\"Enter {key}: \")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "tags": [
                    "skip-execution"
                ]
            },
            "outputs": [],
            "source": [
                "# Collections are where documents are stored. ex: test\n",
                "collection = input(\"Collection: \")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create Guardrails"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Colang is a modeling language enabling the design of guardrails.\n",
                "# In it, you define user and bot behaviors, as well as the flow of conversations.\n",
                "\n",
                "# https://github.com/NVIDIA/NeMo-Guardrails/blob/feature/runnable-rails/docs/user_guides/colang-language-syntax-guide.md\n",
                "COLANG_CONFIG = \"\"\"\n",
                "define user express greeting\n",
                "    \"hi\"\n",
                "    \"hello\"\n",
                "    \"how are you?\"\n",
                "\n",
                "define bot express greeting\n",
                "    \"Hi! I'm RAGBot\"\n",
                "    \"Hello, I am RAGBot\"\n",
                "\n",
                "define bot ask how to help\n",
                "    \"What information do you need?\"\n",
                "\n",
                "define flow greeting\n",
                "    user express greeting\n",
                "    bot express greeting\n",
                "    bot ask how to help\n",
                "\n",
                "define flow qa\n",
                "    $last_user_message = user ...\n",
                "    $answer = execute qa_chain(query=$last_user_message)\n",
                "    bot $answer\n",
                "\n",
                "\"\"\"\n",
                "# TODO: FRAZ - define the qa_chain -- it is an action that we need to pass to rails config at runtime. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nemoguardrails import RailsConfig\n",
                "from nemoguardrails.integration.langchain.runnable_rails import RunnableRails\n",
                "\n",
                "\n",
                "def yaml_config(engine, model):\n",
                "    return f\"\"\"\n",
                "    models:\n",
                "      - type: main\n",
                "        engine: {engine}\n",
                "        model: {model}\n",
                "    \"\"\"\n",
                "\n",
                "\n",
                "engine = \"openai\"\n",
                "model_name = \"gpt-3.5-turbo-16k\"\n",
                "yaml = yaml_config(engine, model_name)\n",
                "config = RailsConfig.from_content(\n",
                "    COLANG_CONFIG,\n",
                "    yaml,\n",
                ")\n",
                "guardrails = RunnableRails(config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create RAG Pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Embedding Model and Vector Store"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
                "\n",
                "embedding = NVIDIAEmbeddings(model=\"nvolveqa_40k\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Astra vector store configured\n"
                    ]
                }
            ],
            "source": [
                "from langchain.vectorstores.astradb import AstraDB\n",
                "\n",
                "vstore = AstraDB(\n",
                "    collection_name=collection,\n",
                "    embedding=embedding,\n",
                "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
                "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
                ")\n",
                "print(\"Astra vector store configured\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "An example entry:\n",
                        "{'author': 'aristotle', 'quote': 'Love well, be loved and do something of value.', 'tags': 'love;ethics'}\n"
                    ]
                }
            ],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Load a sample dataset\n",
                "philo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n",
                "print(\"An example entry:\")\n",
                "print(philo_dataset[16])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.schema import Document\n",
                "\n",
                "# Constructs a set of documents from your data. Documents can be used as inputs to your vector store.\n",
                "docs = []\n",
                "for entry in philo_dataset:\n",
                "    metadata = {\"author\": entry[\"author\"]}\n",
                "    if entry[\"tags\"]:\n",
                "        # Add metadata tags to the metadata dictionary\n",
                "        for tag in entry[\"tags\"].split(\";\"):\n",
                "            metadata[tag] = \"y\"\n",
                "    # Create a LangChain document with the quote and metadata tags\n",
                "    doc = Document(page_content=entry[\"quote\"], metadata=metadata)\n",
                "    docs.append(doc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "nbmake": {
                    "post_cell_execute": [
                        "assert len(inserted_ids) > 0"
                    ]
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Inserted 450 documents.\n"
                    ]
                }
            ],
            "source": [
                "# Create embeddings by inserting your documents into the vector store.\n",
                "inserted_ids = vstore.add_documents(docs)\n",
                "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### QA Retrieval\n",
                "\n",
                "Retrieve context from your vector database, and pass it to the NVIDIA model with a prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'In the given context, philosophers are most concerned with the subject of philosophy itself. Aristotle discusses philosophy as starting with wonder and also mentions that it can make people sick. Hegel, on the other hand, discusses the relationship between knowledge and education. Therefore, it can be inferred that philosophers are concerned with understanding the world, knowledge, and its implications on individuals and society.'"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from langchain.prompts import ChatPromptTemplate\n",
                "from langchain.chat_models import ChatOpenAI\n",
                "from langchain.schema.output_parser import StrOutputParser\n",
                "from langchain.schema.runnable import RunnablePassthrough\n",
                "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
                "\n",
                "retriever = vstore.as_retriever(search_kwargs={\"k\": 3})\n",
                "\n",
                "prompt_template = \"\"\"\n",
                "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
                "Context: {context}\n",
                "Question: {question}\n",
                "Your answer:\n",
                "\"\"\"\n",
                "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
                "model = ChatNVIDIA(model=\"mixtral_8x7b\")\n",
                "\n",
                "chain = (\n",
                "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
                "    | prompt\n",
                "    | (guardrails | model)\n",
                "    | StrOutputParser()\n",
                ")\n",
                "\n",
                "chain.invoke(\"In the given context, what subject are philosophers most concerned with?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add your questions here!\n",
                "# chain.invoke(\"<your question>\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "220870af",
            "metadata": {},
            "source": [
                "## Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86789a9a",
            "metadata": {
                "nbmake": {
                    "post_cell_execute": [
                        "# Deletes collection for test suite to allow each test to run with a fresh collection",
                        "vstore.delete_collection()"
                    ]
                }
            },
            "outputs": [],
            "source": [
                "# WARNING: This will delete the collection and all documents in the collection\n",
                "# vstore.delete_collection()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You now have a functional RAG pipeline powered by NVIDIA! NVIDIA offers many different model types suited for different problems. Check out the [catalog](https://catalog.ngc.nvidia.com) for more. "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
