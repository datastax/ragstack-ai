{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Unstructured with LangChain & AstraDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show a basic RAG-style example that uses the Unstructured API to parse a PDF document, store the corresponding document into a vector store (`AstraDB`) and finally, perform some basic queries against that store. The notebook is modeled after the quick start notebooks and hence is meant as a way of getting started with Unstructured, backed by a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T11:48:33.967092Z",
     "start_time": "2024-02-13T11:47:41.568768Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, install the required dependencies\n",
    "!pip install --quiet ragstack-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T11:59:43.987486Z",
     "start_time": "2024-02-13T11:56:57.909870Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass(\"Enter your Unstructured API Key:\")\n",
    "os.environ[\"ASTRA_DB_ENDPOINT\"] = input(\"Enter you Astra DB API Endpoint: \")\n",
    "os.environ[\"ASTRA_DB_TOKEN\"] = getpass(\"Enter you Astra DB Token: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Unstructured API to parse a PDF\n",
    "\n",
    "In this example notebook, we'll focus our analysis on pages 9 and 10 of the referenced paper, available at https://arxiv.org/pdf/1706.03762.pdf. \n",
    "\n",
    "We're doing this to save your usage credits, as the free version of Unstructured's API is limited to 1,000 pages per month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Parsing\n",
    "\n",
    "First we will start with the most basic parsing mode. This works well if your document doesn't contain any complex formatting or tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T12:05:20.215552Z",
     "start_time": "2024-02-13T12:05:15.721697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredAPIFileLoader\n",
    "\n",
    "loader = UnstructuredAPIFileLoader(\n",
    "    file_path=\"./resources/attention_pages_9_10.pdf\",\n",
    "    api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the parser returns 1 document per pdf file.  Lets examine the contents of the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
      "\n",
      "base\n",
      "\n",
      "(A)\n",
      "\n",
      "(B)\n",
      "\n",
      "(C)\n",
      "\n",
      "(D)\n",
      "\n",
      "N dmodel\n",
      "\n",
      "6\n",
      "\n",
      "512\n",
      "\n",
      "2 4 8\n",
      "\n",
      "256 1024\n",
      "\n",
      "dff\n",
      "\n",
      "2048\n",
      "\n",
      "1024 4096\n",
      "\n",
      "h\n",
      "\n",
      "8 1 4 16 32\n",
      "\n",
      "dk\n",
      "\n",
      "64 512 128 32 16 16 32\n",
      "\n",
      "32 128\n",
      "\n",
      "dv\n",
      "\n",
      "64 512 128 32 16\n",
      "\n",
      "32 128\n",
      "\n",
      "Pdrop\n",
      "\n",
      "0.1\n",
      "\n",
      "0.0 0.2\n",
      "\n",
      "✏ls\n",
      "\n",
      "0.1\n",
      "\n",
      "0.0 0.2\n",
      "\n",
      "PPL train steps (dev) 100K 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 300K 4.33\n",
      "\n",
      "BLEU params 106 (dev) 25.8 65 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4\n",
      "\n",
      "⇥\n",
      "\n",
      "58 60 36 50 80 28 168 53 90\n",
      "\n",
      "(E) big\n",
      "\n",
      "6\n",
      "\n",
      "positional embedding instead of sinusoids\n",
      "\n",
      "1024\n",
      "\n",
      "4096\n",
      "\n",
      "16\n",
      "\n",
      "0.3\n",
      "\n",
      "213\n",
      "\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "\n",
      "6.3 English Constituency Parsing\n",
      "\n",
      "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "\n",
      "9\n",
      "\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
      "\n",
      "Parser\n",
      "\n",
      "Training\n",
      "\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative\n",
      "\n",
      "Petrov et al. (2006) [29] Zhu et al. (2013) [40] Dyer et al. (2016) [8] Transformer (4 layers) Zhu et al. (2013) [40] Huang & Harper (2009) [14] McClosky et al. (2006) [26] Vinyals & Kaiser el al. (2014) [37] Transformer (4 layers) Luong et al. (2015) [23] Dyer et al. (2016) [8]\n",
      "\n",
      "WSJ 23 F1 88.3 90.4 90.4 91.7 91.3 91.3 91.3 92.1 92.1 92.7 93.0 93.3\n",
      "\n",
      "increased the maximum output length to input length + 300. We used a beam size of 21 and ↵ = 0.3 for both WSJ only and the semi-supervised setting.\n",
      "\n",
      "Our results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "\n",
      "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n",
      "\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "\n",
      "References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "\n",
      "arXiv:1607.06450, 2016.\n",
      "\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "\n",
      "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\n",
      "\n",
      "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "\n",
      "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrolling through the text above, you can see that the table data isn't well formatted.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Parsing\n",
    "\n",
    "By changing the processing strategy and response mode, we can get more detailed document structure. Unstructured can break the document into elements of different types, which can be helpful for improving your RAG system.  For example, you might want to exclude elements of type `Footer` from your vector store.\n",
    "\n",
    "A list of all the different element types can be found here: https://unstructured-io.github.io/unstructured/introduction/overview.html#id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredAPIFileLoader\n",
    "\n",
    "loader = UnstructuredAPIFileLoader(\n",
    "    file_path=\"./resources/attention_pages_9_10.pdf\",\n",
    "    api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "    mode=\"elements\", # default: \"single\"\n",
    "    strategy=\"hi_res\", # default: \"auto\"\n",
    "    pdf_infer_table_structure=True,\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 27 documents returned from the pdf. We will use the following script to examine the new contents. \n",
    "\n",
    "Note that we will use the `text_as_html` property on Table elements to show the 2 tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: FigureCaption, page_number: 1, parent_id: None, content: Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><th></th><th>N</th><th>dwss</th><th>dn</th><th>b</th><th>di</th><th>Pug</th><th>as</th><th>gon</th><th>| 08</th><th>BORT</th><th>PR</th></thead><tr><td>base</td><td>| 6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>0.1</td><td>01</td><td>100K</td><td>| 492</td><td>258</td><td>65</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>512</td><td></td><td></td><td></td><td>529</td><td>249</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>4</td><td>128</td><td></td><td></td><td></td><td>500</td><td>255</td><td></td></tr><tr><td>(A)</td><td></td><td></td><td></td><td>16</td><td>32</td><td></td><td></td><td></td><td>491</td><td>258</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td>16</td><td></td><td></td><td></td><td>501</td><td>254</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td>516</td><td>251</td><td>58</td></tr><tr><td>®)</td><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td>501</td><td>254</td><td>60</td></tr><tr><td rowspan=\"7\">©)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>237</td><td>36</td></tr><tr><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>519</td><td>253</td><td>50</td></tr><tr><td></td><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>488</td><td>255</td><td>80</td></tr><tr><td></td><td></td><td>256</td><td></td><td></td><td>32</td><td></td><td></td><td></td><td>575</td><td>245</td><td>28</td></tr><tr><td></td><td></td><td>1024</td><td></td><td></td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr><tr><td></td><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td>512</td><td>254</td><td>53</td></tr><tr><td></td><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td>475</td><td>262</td><td>90</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>577</td><td>246</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>495</td><td>255</td><td></td></tr><tr><td>D</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>461</td><td>253</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>547</td><td>257</td><td></td></tr><tr><td>(E)</td><td></td><td></td><td>positional</td><td>embedding</td><td>instead</td><td>of sinusoids</td><td></td><td></td><td>4.92</td><td>25.7</td><td></td></tr><tr><td>big</td><td>| 6</td><td>1024</td><td>4096</td><td>16</td><td></td><td>0.3</td><td></td><td>300K</td><td>| 433</td><td>264</td><td>213</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: NarrativeText, page_number: 1, parent_id: None, content: development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "category: NarrativeText, page_number: 1, parent_id: None, content: In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "category: NarrativeText, page_number: 1, parent_id: None, content: In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "category: Title, page_number: 1, parent_id: None, content: 6.3 English Constituency Parsing\n",
      "category: NarrativeText, page_number: 1, parent_id: 69a26531b834c0d5e4fd1abf729cebb4, content: To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "category: NarrativeText, page_number: 1, parent_id: 69a26531b834c0d5e4fd1abf729cebb4, content: We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "category: NarrativeText, page_number: 1, parent_id: 69a26531b834c0d5e4fd1abf729cebb4, content: We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "category: Footer, page_number: 1, parent_id: None, content: 9\n",
      "category: NarrativeText, page_number: 2, parent_id: None, content: Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><th>Parser</th><th>Training</th><th>WSJ 23 F1</th></thead><tr><td>Vinyals &amp; Kaiser el al. (2014) B2</td><td>T WST only, discriminative</td><td>88.3</td></tr><tr><td>Petrov et al. (2006)</td><td>WSIJ only, discriminative</td><td>90.4</td></tr><tr><td>Zhu et al. (2013) [A0]</td><td>WSIJ only, discriminative</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) (5]</td><td>WSJ only, discriminative</td><td>91.7</td></tr><tr><td>Transformer (4 layers)</td><td>WSIJ only, discriminative</td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [A0]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>Vinyals Transformer (4 layers)</td><td>semi-supervised semi-supervised</td><td>92.7</td></tr><tr><td>Luong et al. (2015) 23]</td><td>multi-task</td><td>93.0</td></tr><tr><td>Dyer et al. (2016)</td><td>generative</td><td>93.3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: NarrativeText, page_number: 2, parent_id: None, content: increased the maximum output length to input length + 300. We used a beam size of 21 and ↵ = 0.3 for both WSJ only and the semi-supervised setting.\n",
      "category: NarrativeText, page_number: 2, parent_id: None, content: Our results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "category: NarrativeText, page_number: 2, parent_id: None, content: In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "category: Title, page_number: 2, parent_id: None, content: 7 Conclusion\n",
      "category: NarrativeText, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "category: NarrativeText, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "category: NarrativeText, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "category: NarrativeText, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n",
      "category: NarrativeText, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "category: UncategorizedText, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "category: ListItem, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: arXiv:1607.06450, 2016.\n",
      "category: ListItem, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "category: ListItem, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "category: ListItem, page_number: 2, parent_id: 1acda5750c5f47875b86ebb114a0bb80, content: [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "category: Footer, page_number: 2, parent_id: None, content: 10\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "for doc in documents:\n",
    "    category = doc.metadata[\"category\"] if \"category\" in doc.metadata else None\n",
    "    page_number = doc.metadata[\"page_number\"] if \"page_number\" in doc.metadata else None\n",
    "    parent_id = doc.metadata[\"parent_id\"] if \"parent_id\" in doc.metadata else None\n",
    "\n",
    "    if category == \"Table\":\n",
    "        display(HTML(doc.metadata[\"text_as_html\"]))\n",
    "    else:\n",
    "        print(f\"category: {category}, page_number: {page_number}, parent_id: {parent_id}, content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the table structure was parsed fairly well. The 2nd (simpler) table is correct. We also see hinting of document structure from the `parent_id` parameter, but unfortunately the LangChain `Document` type masks the element_id of each document.\n",
    "\n",
    "We can instead use the Unstructured API directly to build up the element structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import unstructured\n",
    "\n",
    "elements = unstructured.get_elements_from_api(\n",
    "    file_path=\"./resources/attention_pages_9_10.pdf\",\n",
    "    api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "    strategy=\"hi_res\",\n",
    "    pdf_infer_table_structure=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><th></th><th>N</th><th>dwss</th><th>dn</th><th>b</th><th>di</th><th>Pug</th><th>as</th><th>gon</th><th>| 08</th><th>BORT</th><th>PR</th></thead><tr><td>base</td><td>| 6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>0.1</td><td>01</td><td>100K</td><td>| 492</td><td>258</td><td>65</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>512</td><td></td><td></td><td></td><td>529</td><td>249</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>4</td><td>128</td><td></td><td></td><td></td><td>500</td><td>255</td><td></td></tr><tr><td>(A)</td><td></td><td></td><td></td><td>16</td><td>32</td><td></td><td></td><td></td><td>491</td><td>258</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td>16</td><td></td><td></td><td></td><td>501</td><td>254</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td>516</td><td>251</td><td>58</td></tr><tr><td>®)</td><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td>501</td><td>254</td><td>60</td></tr><tr><td rowspan=\"7\">©)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>237</td><td>36</td></tr><tr><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>519</td><td>253</td><td>50</td></tr><tr><td></td><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>488</td><td>255</td><td>80</td></tr><tr><td></td><td></td><td>256</td><td></td><td></td><td>32</td><td></td><td></td><td></td><td>575</td><td>245</td><td>28</td></tr><tr><td></td><td></td><td>1024</td><td></td><td></td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr><tr><td></td><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td>512</td><td>254</td><td>53</td></tr><tr><td></td><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td>475</td><td>262</td><td>90</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>577</td><td>246</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>495</td><td>255</td><td></td></tr><tr><td>D</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>461</td><td>253</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>547</td><td>257</td><td></td></tr><tr><td>(E)</td><td></td><td></td><td>positional</td><td>embedding</td><td>instead</td><td>of sinusoids</td><td></td><td></td><td>4.92</td><td>25.7</td><td></td></tr><tr><td>big</td><td>| 6</td><td>1024</td><td>4096</td><td>16</td><td></td><td>0.3</td><td></td><td>300K</td><td>| 433</td><td>264</td><td>213</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
      "6.3 English Constituency Parsing\n",
      "parent: '6.3 English Constituency Parsing' content: To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
      "parent: '6.3 English Constituency Parsing' content: We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
      "parent: '6.3 English Constituency Parsing' content: We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><th>Parser</th><th>Training</th><th>WSJ 23 F1</th></thead><tr><td>Vinyals &amp; Kaiser el al. (2014) B2</td><td>T WST only, discriminative</td><td>88.3</td></tr><tr><td>Petrov et al. (2006)</td><td>WSIJ only, discriminative</td><td>90.4</td></tr><tr><td>Zhu et al. (2013) [A0]</td><td>WSIJ only, discriminative</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) (5]</td><td>WSJ only, discriminative</td><td>91.7</td></tr><tr><td>Transformer (4 layers)</td><td>WSIJ only, discriminative</td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [A0]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>Vinyals Transformer (4 layers)</td><td>semi-supervised semi-supervised</td><td>92.7</td></tr><tr><td>Luong et al. (2015) 23]</td><td>multi-task</td><td>93.0</td></tr><tr><td>Dyer et al. (2016)</td><td>generative</td><td>93.3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increased the maximum output length to input length + 300. We used a beam size of 21 and ↵ = 0.3 for both WSJ only and the semi-supervised setting.\n",
      "Our results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
      "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "parent: '7 Conclusion' content: In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "parent: '7 Conclusion' content: For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
      "parent: '7 Conclusion' content: We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "parent: '7 Conclusion' content: The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n",
      "parent: '7 Conclusion' content: Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n",
      "parent: '7 Conclusion' content: References [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "parent: '7 Conclusion' content: arXiv:1607.06450, 2016.\n",
      "parent: '7 Conclusion' content: [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "parent: '7 Conclusion' content: [3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "parent: '7 Conclusion' content: [4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "parents = {}\n",
    "\n",
    "for el in elements:\n",
    "    parents[el.id] = el.text\n",
    "\n",
    "for el in elements:\n",
    "    if el.category == \"Table\":\n",
    "        display(HTML(el.metadata.text_as_html))\n",
    "    elif el.metadata.parent_id:\n",
    "        print(f\"parent: '{parents[el.metadata.parent_id]}' content: {el.text}\")\n",
    "    else:\n",
    "        print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clearly see that Unstructured is parsing both table and document structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing into Astra DB\n",
    "\n",
    "Now we will continue with the RAG process, by creating embeddings for the pdf, and storing them in Astra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import AstraDB\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "astra_db_store = AstraDB(\n",
    "    collection_name=\"langchain_unstructured\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    token=os.getenv(\"ASTRA_DB_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create LangChain Documents by splitting the text after `Table` elements and before `Title` elements. Additionally, we will use the html output format for table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.<table><thead><th></th><th>N</th><th>dwss</th><th>dn</th><th>b</th><th>di</th><th>Pug</th><th>as</th><th>gon</th><th>| 08</th><th>BORT</th><th>PR</th></thead><tr><td>base</td><td>| 6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>0.1</td><td>01</td><td>100K</td><td>| 492</td><td>258</td><td>65</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td>512</td><td></td><td></td><td></td><td>529</td><td>249</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>4</td><td>128</td><td></td><td></td><td></td><td>500</td><td>255</td><td></td></tr><tr><td>(A)</td><td></td><td></td><td></td><td>16</td><td>32</td><td></td><td></td><td></td><td>491</td><td>258</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td>16</td><td></td><td></td><td></td><td>501</td><td>254</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td>516</td><td>251</td><td>58</td></tr><tr><td>®)</td><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td>501</td><td>254</td><td>60</td></tr><tr><td rowspan=\"7\">©)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>237</td><td>36</td></tr><tr><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>519</td><td>253</td><td>50</td></tr><tr><td></td><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>488</td><td>255</td><td>80</td></tr><tr><td></td><td></td><td>256</td><td></td><td></td><td>32</td><td></td><td></td><td></td><td>575</td><td>245</td><td>28</td></tr><tr><td></td><td></td><td>1024</td><td></td><td></td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr><tr><td></td><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td>512</td><td>254</td><td>53</td></tr><tr><td></td><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td>475</td><td>262</td><td>90</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>577</td><td>246</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>495</td><td>255</td><td></td></tr><tr><td>D</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>461</td><td>253</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>547</td><td>257</td><td></td></tr><tr><td>(E)</td><td></td><td></td><td>positional</td><td>embedding</td><td>instead</td><td>of sinusoids</td><td></td><td></td><td>4.92</td><td>25.7</td><td></td></tr><tr><td>big</td><td>| 6</td><td>1024</td><td>4096</td><td>16</td><td></td><td>0.3</td><td></td><td>300K</td><td>| 433</td><td>264</td><td>213</td></tr></table>', metadata={'filename': 'attention_pages_9_10.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1}),\n",
       " Document(page_content='development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.', metadata={'filename': 'attention_pages_9_10.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1}),\n",
       " Document(page_content='6.3 English Constituency ParsingTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, weTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)<table><thead><th>Parser</th><th>Training</th><th>WSJ 23 F1</th></thead><tr><td>Vinyals &amp; Kaiser el al. (2014) B2</td><td>T WST only, discriminative</td><td>88.3</td></tr><tr><td>Petrov et al. (2006)</td><td>WSIJ only, discriminative</td><td>90.4</td></tr><tr><td>Zhu et al. (2013) [A0]</td><td>WSIJ only, discriminative</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) (5]</td><td>WSJ only, discriminative</td><td>91.7</td></tr><tr><td>Transformer (4 layers)</td><td>WSIJ only, discriminative</td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [A0]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>Vinyals Transformer (4 layers)</td><td>semi-supervised semi-supervised</td><td>92.7</td></tr><tr><td>Luong et al. (2015) 23]</td><td>multi-task</td><td>93.0</td></tr><tr><td>Dyer et al. (2016)</td><td>generative</td><td>93.3</td></tr></table>', metadata={'filename': 'attention_pages_9_10.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 1}),\n",
       " Document(page_content='increased the maximum output length to input length + 300. We used a beam size of 21 and ↵ = 0.3 for both WSJ only and the semi-supervised setting.Our results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences.', metadata={'filename': 'attention_pages_9_10.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 2})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "current_doc = None\n",
    "\n",
    "for el in elements:\n",
    "    if el.category in [\"Header\", \"Footer\"]:\n",
    "        continue # skip these\n",
    "    if el.category == \"Title\":\n",
    "        documents.append(current_doc)\n",
    "        current_doc = None\n",
    "    if not current_doc:\n",
    "        current_doc = Document(page_content=\"\", metadata=el.metadata.to_dict())\n",
    "    current_doc.page_content += el.metadata.text_as_html if el.category == \"Table\" else el.text\n",
    "    if el.category == \"Table\":\n",
    "        documents.append(current_doc)\n",
    "        current_doc = None\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['124f9da7f5794806bbb681b1277c5d45',\n",
       " '19757da3ce71413899b2a78eff6657ee',\n",
       " 'd9b8862819e14b469416a4f6f087e53e',\n",
       " 'd1f98d9d7db34e45b081c3c0416901a0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astra_db_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG Example\n",
    "\n",
    "Now that we have populated our vector store, we will build a RAG pipeline and build some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", streaming=False, temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": astra_db_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | PromptTemplate.from_template(prompt)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can ask a question about some text in the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reducing the attention key size hurts model quality.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What does reducing the attention key size do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can try to get a value from the 2nd table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 'WSJ 23 F1' value for 'Dyer et al. (2016) (5]' was 91.7.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"For the transformer to English constituency results, what was the 'WSJ 23 F1' value for 'Dyer et al. (2016) (5]'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can ask a question that doesn't exist in our content to confirm that the LLM rejection is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The supplied context does not provide any information about George Washington's birthdate.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query fails to be answered due to lack of context in Astra DB\n",
    "chain.invoke(\"When was George Washington born?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
