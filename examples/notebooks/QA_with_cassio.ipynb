{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdeb8757",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/QA_with_cassio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05e3f2",
   "metadata": {
    "id": "cf05e3f2"
   },
   "source": [
    "# Knowledge Base Search on Proprietary Data powered by Astra DB\n",
    "This notebook guides you through setting up [RAGStack](https://www.datastax.com/products/ragstack) using [Astra Vector Search](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html), [OpenAI](https://openai.com/about), and [CassIO](https://cassio.org/) to implement a generative Q&A over your own documentation.\n",
    "\n",
    "## Astra Vector Search\n",
    "Astra vector search enables developers to search a database by context or meaning rather than keywords or literal values. This is done by using \u201cembeddings\u201d. Embeddings are a type of representation used in machine learning where high-dimensional or complex data is mapped onto vectors in a lower-dimensional space. These vectors capture the semantic properties of the input data, meaning that similar data points have similar embeddings.\n",
    "Reference: [Astra Vector Search](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html)\n",
    "\n",
    "## CassIO\n",
    "CassIO is the ultimate solution for seamlessly integrating Apache Cassandra\u00ae with generative artificial intelligence and other machine learning workloads. This powerful Python library simplifies the complicated process of accessing the advanced features of the Cassandra database, including vector search capabilities. With CassIO, developers can fully concentrate on designing and perfecting their AI systems without any concerns regarding the complexities of integration with Cassandra.\n",
    "Reference: [CassIO](https://cassio.org/)\n",
    "\n",
    "## OpenAI\n",
    "OpenAI provides various tools and resources to implement your own Document QA Search system. This includes pre-trained language models like GPT-4, which can understand and generate human-like text. Additionally, OpenAI offers guidelines and APIs to leverage their models for document search and question-answering tasks, enabling developers to build powerful and intelligent Document QA Search applications.\n",
    "Reference: [OpenAI](https://openai.com/about)\n",
    "\n",
    "## Demo Summary\n",
    "ChatGPT excels at answering questions, but only on topics it knows from its training data. It offers you a nice dialog interface to ask questions and get answers.\n",
    "\n",
    "But what do you do when you have your own documents? How can you leverage the GenAI and LLM models to get insights into those? We can use Retrieval Augmented Generation (RAG) -- think of a Q/A Bot that can answer specific questions over your documentation.\n",
    "\n",
    "We can do this in two easy steps:\n",
    "1. Analyzing and storing existing documentation.\n",
    "2. Providing search capabilities for the model to retrieve your documentation.\n",
    "\n",
    "This is solved by using LLM models. Ideally you embed the data as vectors and store them in a vector database and then use the LLM models on top of that database.\n",
    "\n",
    "This notebook demonstrates a basic two-step RAG technique for enabling GPT to answer questions using a library of reference on your own documentation using Astra DB Vector Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651400d1",
   "metadata": {
    "id": "651400d1"
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "* Follow [these steps](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html) to create a new vector search enabled database in Astra.\n",
    "* Generate a new [\"Database Administrator\" token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html).\n",
    "  * For Open AI, you will need an [Open AI API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key). This will require an Open AI account with billing enabled.\n",
    "  * For more details, see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org.\n",
    "\n",
    "\n",
    "When you run this notebook, it will ask you to provide each of these items at various steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d88d66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "a6d88d66",
    "nbmake": {
     "post_cell_execute": [
      "from conftest import before_notebook",
      "before_notebook()"
     ]
    },
    "outputId": "dc543d17-3fb2-4362-cc4e-0050bd7787ba",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install required dependencies\n",
    "! pip install -qU ragstack-ai pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935c835d",
   "metadata": {
    "editable": true,
    "nbmake": {
     "post_cell_execute": [
      "import os\n",
      "import string\n",
      "import random\n",
      "collection = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\n",
      "keyspace = \"default_keyspace\"\n"
     ]
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your settings for Astra DB and OpenAI:\n",
    "os.environ[\"ASTRA_DB_ID\"] = input(\"Enter your Astra DB ID: \")\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass(\"Enter your Astra DB Token: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tyZ1IC4d3U15",
   "metadata": {
    "editable": true,
    "id": "tyZ1IC4d3U15",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Provide Sample Data\n",
    "A sample document is provided from CassIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1VsVGojG663U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VsVGojG663U",
    "outputId": "c81811af-1968-448c-ff70-7e54a5adb746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13022  100 13022    0     0  54555      0 --:--:-- --:--:-- --:--:-- 55649\n"
     ]
    }
   ],
   "source": [
    "# retrieve the text of a short story that will be indexed in the vector store\n",
    "# ruff: noqa: E501\n",
    "! curl https://raw.githubusercontent.com/CassioML/cassio-website/main/docs/frameworks/langchain/texts/amontillado.txt --output amontillado.txt\n",
    "SAMPLEDATA = [\"amontillado.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251382e",
   "metadata": {
    "id": "8251382e"
   },
   "source": [
    "# Read Files, Create Embeddings, Store in Vector DB\n",
    "\n",
    "CassIO seamlessly integrates with RAGStack and LangChain, offering Cassandra-specific tools for many tasks. In our example we will use vector stores, indexers, embeddings, and queries.\n",
    "\n",
    "We will use OpenAI for our LLM services. (See [cassio.org](https://cassio.org/start_here/#llm-access) for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bfe919a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cassio\n",
    "\n",
    "cassio.init(\n",
    "    database_id=os.environ[\"ASTRA_DB_ID\"],\n",
    "    token=os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TE5pZsfs7iPT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "TE5pZsfs7iPT",
    "outputId": "c10f1bc5-7448-4b78-f416-034a5428da32",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the needed libraries and declare the LLM model\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Cassandra\n",
    "\n",
    "# Loop through each file and load it into our vector store\n",
    "documents = []\n",
    "for filename in SAMPLEDATA:\n",
    "    path = os.path.join(os.getcwd(), filename)\n",
    "\n",
    "    # Supported file types are pdf and txt\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(path)\n",
    "        new_docs = loader.load_and_split()\n",
    "        print(f\"Processed pdf file: {filename}\")\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        loader = TextLoader(path)\n",
    "        new_docs = loader.load_and_split()\n",
    "        print(f\"Processed txt file: {filename}\")\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {filename}\")\n",
    "\n",
    "    if len(new_docs) > 0:\n",
    "        documents.extend(new_docs)\n",
    "\n",
    "cass_vstore = Cassandra.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    session=None,\n",
    "    table_name=\"qa_cassio\",\n",
    "    keyspace=\"default_keyspace\",\n",
    ")\n",
    "\n",
    "# empty the list of file names -- we don't want to\n",
    "# accidentally load the same files again\n",
    "SAMPLEDATA = []\n",
    "\n",
    "print(\"\\nProcessing done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oTEglBjbaHDj",
   "metadata": {
    "id": "oTEglBjbaHDj"
   },
   "source": [
    "# Now Query the Data and execute some \"searches\" against it\n",
    "First we will start with a similarity search using the Vectorstore's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "yaFkjaAnCIRw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "yaFkjaAnCIRw",
    "outputId": "b5f7b7ce-4649-4ad6-f715-1a4d9ffdf4d0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Document 0\n",
      "\n",
      "The thousand injuries of Fortunato I had borne as I best could, but\n",
      "when he ventured upon insult, I vowed revenge.  You, who so well know\n",
      "the nature of my soul, will not suppose, however, that I gave utterance\n",
      "to a threat.  _At length_ I would be avenged; this was a point definitely\n",
      "settled--but the very definitiveness with which it was resolved,\n",
      "precluded the idea of risk.  I must not only punish, but punish with\n",
      "impunity.  A wrong is unredressed when retribution overtakes its\n",
      "redresser.  It is equally unredressed when the avenger fails to make\n",
      "himself felt as such to him who has done the wrong.\n",
      "\n",
      "It must be understood that neither by word nor deed had I given\n",
      "Fortunato cause to doubt my good will.  I continued, as was my wont, to\n",
      "smile in his face, and he did not perceive that my smile _now_ was at\n",
      "the thought of his immolation.\n",
      "\n",
      "He had a weak point--this Fortunato--although in other regards he was a\n",
      "man to be respected and even feared.  He prided himself on his\n",
      "connoisseurship in wine.  Few Italians have the true virtuoso spirit.\n",
      "For the most part their enthusiasm is adopted to suit the time and\n",
      "opportunity--to practise imposture upon the British and Austrian\n",
      "_millionaires_.  In painting and gemmary, Fortunato, like his countrymen,\n",
      "was a quack--but in the matter of old wines he was sincere.  In this\n",
      "respect I did not differ from him materially: I was skillful in the\n",
      "Italian vintages myself, and bought largely whenever I could.\n",
      "\n",
      "It was about dusk, one evening during the supreme madness of the\n",
      "carnival season, that I encountered my friend.  He accosted me with\n",
      "excessive warmth, for he had been drinking much.  The man wore motley.\n",
      "He had on a tight-fitting parti-striped dress, and his head was\n",
      "surmounted by the conical cap and bells.  I was so pleased to see him,\n",
      "that I thought I should never have done wringing his hand.\n",
      "\n",
      "I said to him--\"My dear Fortunato, you are luckily met.  How remarkably\n",
      "well you are looking to-day!  But I have received a pipe of what passes\n",
      "for Amontillado, and I have my doubts.\"\n",
      "\n",
      "\"How?\" said he.  \"Amontillado?  A pipe?  Impossible!  And in the middle\n",
      "of the carnival!\"\n",
      "\n",
      "\"I have my doubts,\" I replied; \"and I was silly enough to pay the full\n",
      "Amontillado price without consulting you in the matter. You were not to\n",
      "be found, and I was fearful of losing a bargain.\"\n",
      "\n",
      "\"Amontillado!\"\n",
      "\n",
      "\"I have my doubts.\"\n",
      "\n",
      "\"Amontillado!\"\n",
      "\n",
      "\"And I must satisfy them.\"\n",
      "\n",
      "\"Amontillado!\"\n",
      "\n",
      "\"As you are engaged, I am on my way to Luchesi.  If any one has a\n",
      "critical turn, it is he.  He will tell me--\"\n",
      "\n",
      "\"Luchesi cannot tell Amontillado from Sherry.\"\n",
      "\n",
      "\"And yet some fools will have it that his taste is a match for your\n",
      "own.\"\n",
      "\n",
      "\"Come, let us go.\"\n",
      "\n",
      "\"Whither?\"\n",
      "\n",
      "\"To your vaults.\"\n",
      "\n",
      "\"My friend, no; I will not impose upon your good nature.  I perceive\n",
      "you have an engagement.  Luchesi--\"\n",
      "\n",
      "\"I have no engagement;--come.\"\n",
      "\n",
      "\"My friend, no.  It is not the engagement, but the severe cold with\n",
      "which I perceive you are afflicted.  The vaults are insufferably damp.\n",
      "They are encrusted with nitre.\"\n",
      "\n",
      "\"Let us go, nevertheless.  The cold is merely nothing. Amontillado!\n",
      "You have been imposed upon.  And as for Luchesi, he cannot distinguish\n",
      "Sherry from Amontillado.\"\n",
      "\n",
      "Thus speaking, Fortunato possessed himself of my arm. Putting on a mask\n",
      "of black silk, and drawing a _roquelaire_ closely about my person, I\n",
      "suffered him to hurry me to my palazzo.\n",
      "\n",
      "There were no attendants at home; they had absconded to make merry in\n",
      "honour of the time.  I had told them that I should not return until the\n",
      "morning, and had given them explicit orders not to stir from the house.\n",
      "These orders were sufficient, I well knew, to insure their immediate\n",
      "disappearance, one and all, as soon as my back was turned.\n"
     ]
    }
   ],
   "source": [
    "# construct your query\n",
    "query = \"Who is Luchesi?\"\n",
    "\n",
    "# find matching documentation using similarity search\n",
    "matched_docs = cass_vstore.similarity_search(query=query, k=1)\n",
    "\n",
    "# print out the relevant context that an LLM will use to produce an answer\n",
    "for i, d in enumerate(matched_docs):\n",
    "    print(f\"\\n## Document {i}\\n\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cVulGW6aom5",
   "metadata": {
    "id": "9cVulGW6aom5"
   },
   "source": [
    "# Finally do a Q/A Search\n",
    "To be able implement Q/A over documents we need to perform the following steps:\n",
    "\n",
    "1. Create an Index on top of our vector store\n",
    "2. Create a Retriever from that Index\n",
    "3. Ask questions (prompts)!\n",
    "\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cptnBJhCWTz0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "cptnBJhCWTz0",
    "outputId": "106cefaa-4dd4-4b5c-80ec-574a6697bd7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Luchesi is a character mentioned in the story \"The Cask of Amontillado\" by Edgar Allan Poe. He is a rival of the narrator and Fortunato in the field of wine connoisseurship. The narrator mentions Luchesi to challenge Fortunato\\'s expertise and entice him to accompany to the vaults where he eventually meets his fate.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q/A LLM Search\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "index = VectorStoreIndexWrapper(vectorstore=cass_vstore)\n",
    "\n",
    "# Query the index for relevant vectors to our prompt\n",
    "query = \"Who is Luchesi?\"\n",
    "index.query(question=query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "kYz1Jdl1Qhuj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "kYz1Jdl1Qhuj",
    "outputId": "d4bb9005-468d-4f9f-e1cb-6f2de4caacbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Luchesi is a character mentioned in Edgar Allan Poe\\'s short story \"The Cask of Amontillado.\" He is a wine connoisseur who is referenced in the story by the narrator as a potential alternative to Fortunato for assessing the quality of a cask of Amontillado.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, you can use a retrieval chain with a custom prompt\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "query = \"\"\"\n",
    "You are Marv, a sarcastic but factual chatbot.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "query = ChatPromptTemplate.from_template(query)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, retriever=cass_vstore.as_retriever(), chain_type_kwargs={\"prompt\": query}\n",
    ")\n",
    "\n",
    "result = qa.run(\"{question: Who is Luchesi?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a98e5f",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcba05",
   "metadata": {
    "nbmake": {
     "post_cell_execute": [
      "# Deletes collection for test suite to allow each test to run with a fresh collection",
      "cass_vstore.delete_collection()"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: This will delete the collection and all documents in the collection\n",
    "# cass_vstore.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}