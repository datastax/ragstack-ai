{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/QA_with_cassio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf05e3f2",
      "metadata": {
        "id": "cf05e3f2"
      },
      "source": [
        "# Knowledge Base Search on Proprietary Data powered by Astra DB\n",
        "This notebook guides you through setting up [RAGStack](https://www.datastax.com/products/ragstack) using [Astra Vector Search](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html), [OpenAI](https://openai.com/about), and [CassIO](https://cassio.org/) to implement a generative Q&A over your own documentation.\n",
        "\n",
        "## Astra Vector Search\n",
        "Astra vector search enables developers to search a database by context or meaning rather than keywords or literal values. This is done by using “embeddings”. Embeddings are a type of representation used in machine learning where high-dimensional or complex data is mapped onto vectors in a lower-dimensional space. These vectors capture the semantic properties of the input data, meaning that similar data points have similar embeddings.\n",
        "Reference: [Astra Vector Search](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html)\n",
        "\n",
        "## CassIO\n",
        "CassIO is the ultimate solution for seamlessly integrating Apache Cassandra® with generative artificial intelligence and other machine learning workloads. This powerful Python library simplifies the complicated process of accessing the advanced features of the Cassandra database, including vector search capabilities. With CassIO, developers can fully concentrate on designing and perfecting their AI systems without any concerns regarding the complexities of integration with Cassandra.\n",
        "Reference: [CassIO](https://cassio.org/)\n",
        "\n",
        "## OpenAI\n",
        "OpenAI provides various tools and resources to implement your own Document QA Search system. This includes pre-trained language models like GPT-4, which can understand and generate human-like text. Additionally, OpenAI offers guidelines and APIs to leverage their models for document search and question-answering tasks, enabling developers to build powerful and intelligent Document QA Search applications.\n",
        "Reference: [OpenAI](https://openai.com/about)\n",
        "\n",
        "## Demo Summary\n",
        "ChatGPT excels at answering questions, but only on topics it knows from its training data. It offers you a nice dialog interface to ask questions and get answers.\n",
        "\n",
        "But what do you do when you have your own documents? How can you leverage the GenAI and LLM models to get insights into those? We can use Retrieval Augmented Generation (RAG) -- think of a Q/A Bot that can answer specific questions over your documentation.\n",
        "\n",
        "We can do this in two easy steps:\n",
        "1. Analyzing and storing existing documentation.\n",
        "2. Providing search capabilities for the model to retrieve your documentation.\n",
        "\n",
        "This is solved by using LLM models. Ideally you embed the data as vectors and store them in a vector database and then use the LLM models on top of that database.\n",
        "\n",
        "This notebook demonstrates a basic two-step RAG technique for enabling GPT to answer questions using a library of reference on your own documentation using Astra DB Vector Search."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651400d1",
      "metadata": {
        "id": "651400d1"
      },
      "source": [
        "# Getting Started with this notebook\n",
        "\n",
        "* Follow [these steps](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html) to create a new vector search enabled database in Astra.\n",
        "* Generate a new [\"Database Administrator\" token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html).\n",
        "* Download the secure connect bundle for the database you just created (you can do this from the \"Connect\" tab of your database).\n",
        "* You will also need the necessary secret for the LLM provider of your choice:\n",
        "  * If Open AI, then you will need an [Open AI API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key). This will require an Open AI account with billing enabled.\n",
        "  * If Vertex AI, you will need a config file.\n",
        "  * For more details, see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org.\n",
        "\n",
        "\n",
        "When you run this notebook, it will ask you to provide each of these items at various steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311cc2e9",
      "metadata": {
        "id": "311cc2e9"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Install the following libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d88d66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6d88d66",
        "outputId": "dc543d17-3fb2-4362-cc4e-0050bd7787ba"
      },
      "outputs": [],
      "source": [
        "# install required dependencies\n",
        "! pip install \\\n",
        "    \"ragstack-ai\" \\\n",
        "    \"pypdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae20e0b5",
      "metadata": {
        "id": "ae20e0b5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f9f33a",
      "metadata": {
        "id": "16f9f33a"
      },
      "outputs": [],
      "source": [
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5usSnhULsaLV",
      "metadata": {
        "id": "5usSnhULsaLV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IS_COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    IS_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df2888e",
      "metadata": {
        "id": "4df2888e"
      },
      "source": [
        "# Astra DB configuration, Secure Connection Bundle, and Token\n",
        "\n",
        "You will need a secure connect bundle and a user with access permission. For demo purposes the \"administrator\" role will work fine.\n",
        "More information about how to get the bundle can be found [here](https://docs.datastax.com/en/astra-serverless/docs/connect/secure-connect-bundle.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QvY0HTqm3chY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "QvY0HTqm3chY",
        "outputId": "d563332e-8353-41de-f751-506101450243"
      },
      "outputs": [],
      "source": [
        "# Your database's Secure Connect Bundle zip file is needed:\n",
        "if IS_COLAB:\n",
        "    print('Please upload your Secure Connect Bundle zipfile: ')\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        astraBundleFileTitle = list(uploaded.keys())[0]\n",
        "        ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
        "        )\n",
        "else:\n",
        "    # you are running a local-jupyter notebook:\n",
        "    ASTRA_DB_SECURE_BUNDLE_PATH = input(\"Please provide the full path to your Secure Connect Bundle zipfile: \")\n",
        "\n",
        "ASTRA_DB_APPLICATION_TOKEN = getpass(\"Please provide your Database Token ('AstraCS:...' string): \")\n",
        "ASTRA_DB_KEYSPACE = input(\"Please provide the Keyspace name for your Database: \")\n",
        "# define the table name to be used to store our embeddings, CassIO will create the objects in Astra DB for you.\n",
        "# To avoid incompatibilities with existing tables, use a new table name. \n",
        "ASTRA_DB_TABLE_NAME = input(\"Please provide the name of the table to be created: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyZ1IC4d3U15",
      "metadata": {
        "id": "tyZ1IC4d3U15"
      },
      "source": [
        "# Provide Sample Data\n",
        "A sample document is provided from CassIO. You may provide your own files instead in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1VsVGojG663U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VsVGojG663U",
        "outputId": "c81811af-1968-448c-ff70-7e54a5adb746"
      },
      "outputs": [],
      "source": [
        "# retrieve the text of a short story that will be indexed in the vector store\n",
        "! curl https://raw.githubusercontent.com/CassioML/cassio-website/main/docs/frameworks/langchain/texts/amontillado.txt --output amontillado.txt\n",
        "SAMPLEDATA = [\"amontillado.txt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PjxwchCO3n_8",
      "metadata": {
        "id": "PjxwchCO3n_8"
      },
      "outputs": [],
      "source": [
        "# Alternatively, provide your own file. However, you will want to update your queries to match the content of your file. \n",
        "\n",
        "# Upload sample file (Note: this assumes you are on Google Colab. Local Jupyter notebooks can provide the path to their files directly by uncommenting and running just the next line).\n",
        "# SAMPLEDATA = [\"<path_to_file>\"]\n",
        "\n",
        "print('Please upload your own sample file:')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    SAMPLEDATA = uploaded\n",
        "else:\n",
        "    raise ValueError(\n",
        "        'Cannot proceed without Sample Data. Please re-run the cell.'\n",
        "    )\n",
        "\n",
        "print(f'Please make sure to change your queries to match the contents of your file!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e090e7",
      "metadata": {
        "id": "33e090e7"
      },
      "source": [
        "# Connect to Astra DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8c527a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8c527a1",
        "outputId": "0b40a4b2-6f57-48c3-cee1-52c6096f26b1"
      },
      "outputs": [],
      "source": [
        "# Don't mind the \"Closing connection\" error after \"downgrading protocol...\" messages,\n",
        "# it is really just a warning: the connection will work smoothly.\n",
        "cluster = Cluster(\n",
        "    cloud={\n",
        "        \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
        "    },\n",
        "    auth_provider=PlainTextAuthProvider(\n",
        "        \"token\",\n",
        "        ASTRA_DB_APPLICATION_TOKEN,\n",
        "    ),\n",
        ")\n",
        "\n",
        "session = cluster.connect()\n",
        "keyspace = ASTRA_DB_KEYSPACE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8251382e",
      "metadata": {
        "id": "8251382e"
      },
      "source": [
        "# Read Files, Create Embeddings, Store in Vector DB\n",
        "\n",
        "CassIO seamlessly integrates with RAGStack and LangChain, offering Cassandra-specific tools for many tasks. In our example we will use vector stores, indexers, embeddings, and queries.\n",
        "\n",
        "We will use OpenAI for our LLM services. (See [cassio.org](https://cassio.org/start_here/#llm-access) for more details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J5VPZclr6n67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5VPZclr6n67",
        "outputId": "4837f954-06a4-430f-dd1f-5ed9c2f891f4"
      },
      "outputs": [],
      "source": [
        "# We will use OpenAI embeddings, so please provide your OpenAI API Key\n",
        "OPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TE5pZsfs7iPT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE5pZsfs7iPT",
        "outputId": "c10f1bc5-7448-4b78-f416-034a5428da32"
      },
      "outputs": [],
      "source": [
        "# Import the needed libraries and declare the LLM model\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Cassandra\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Loop through each file and load it into our vector store\n",
        "documents = []\n",
        "for filename in SAMPLEDATA:\n",
        "  path = os.path.join(os.getcwd(), filename)\n",
        "\n",
        "  # Supported file types are pdf and txt\n",
        "  if filename.endswith(\".pdf\"):\n",
        "    loader = PyPDFLoader(path)\n",
        "    new_docs = loader.load_and_split()\n",
        "    print(f\"Processed pdf file: {filename}\")\n",
        "  elif filename.endswith(\".txt\"):\n",
        "    loader = TextLoader(path)\n",
        "    new_docs = loader.load_and_split()\n",
        "    print(f\"Processed txt file: {filename}\")\n",
        "  else:\n",
        "    print(f\"Unsupported file type: {filename}\")\n",
        "\n",
        "  if len(new_docs) > 0:\n",
        "    documents.extend(new_docs)\n",
        "\n",
        "cassVStore = Cassandra.from_documents(\n",
        "  documents=documents,\n",
        "  embedding=OpenAIEmbeddings(),\n",
        "  session=session,\n",
        "  keyspace=ASTRA_DB_KEYSPACE,\n",
        "  table_name=ASTRA_DB_TABLE_NAME,\n",
        ")\n",
        "\n",
        "# empty the list of file names -- we don't want to accidentally load the same files again\n",
        "SAMPLEDATA = []\n",
        "\n",
        "print(f\"\\nProcessing done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oTEglBjbaHDj",
      "metadata": {
        "id": "oTEglBjbaHDj"
      },
      "source": [
        "# Now Query the Data and execute some \"searches\" against it\n",
        "First we will start with a similarity search using the Vectorstore's implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yaFkjaAnCIRw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaFkjaAnCIRw",
        "outputId": "b5f7b7ce-4649-4ad6-f715-1a4d9ffdf4d0"
      },
      "outputs": [],
      "source": [
        "# construct your query\n",
        "prompt = \"Who is Luchesi?\"\n",
        "\n",
        "# find matching documentation using similarity search\n",
        "matched_docs = cassVStore.similarity_search(query=prompt, k=1)\n",
        "\n",
        "# print out the relevant context that an LLM will use to produce an answer\n",
        "for i, d in enumerate(matched_docs):\n",
        "    print(f\"\\n## Document {i}\\n\")\n",
        "    print(d.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cVulGW6aom5",
      "metadata": {
        "id": "9cVulGW6aom5"
      },
      "source": [
        "# Finally do a Q/A Search\n",
        "To be able implement Q/A over documents we need to perform the following steps:\n",
        "\n",
        "1. Create an Index on top of our vector store\n",
        "2. Create a Retriever from that Index\n",
        "3. Ask questions (prompts)!\n",
        "\n",
        "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cptnBJhCWTz0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "cptnBJhCWTz0",
        "outputId": "106cefaa-4dd4-4b5c-80ec-574a6697bd7b"
      },
      "outputs": [],
      "source": [
        "# Q/A LLM Search\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "\n",
        "index = VectorStoreIndexWrapper(vectorstore=cassVStore)\n",
        "\n",
        "# Query the index for relevant vectors to our prompt\n",
        "prompt = \"Who is Luchesi?\"\n",
        "index.query(question=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kYz1Jdl1Qhuj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "kYz1Jdl1Qhuj",
        "outputId": "d4bb9005-468d-4f9f-e1cb-6f2de4caacbb"
      },
      "outputs": [],
      "source": [
        "# Alternatively, you can use a retrieval chain with a custom prompt\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt= \"\"\"\n",
        "You are Marv, a sarcastic but factual chatbot.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Your answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=cassVStore.as_retriever(), chain_type_kwargs={\"prompt\": prompt})\n",
        "\n",
        "result = qa.run(\"{question: Who is Luchesi?\")\n",
        "result\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
