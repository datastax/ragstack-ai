{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/llama-astra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with LlamaIndex and AstraDB\n",
    "\n",
    "Build a RAG pipeline with RAGStack, AstraDB, and LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "You will need a vector-enabled Astra database.\n",
    "\n",
    "Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
    "Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
    "Get your Astra DB Endpoint:\n",
    "https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com\n",
    "\n",
    "See the [Prerequisites](https://docs.datastax.com/en/ragstack/docs/prerequisites.html) page for more details.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ragstack-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "nbmake": {
     "post_cell_execute": [
      "import string\n",
      "import random\n",
      "collection = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\n"
     ]
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your settings for Astra DB and OpenAI:\n",
    "os.environ[\"ASTRA_DB_ENDPOINT\"] = input(\"Enter you Astra DB API Endpoint: \")\n",
    "os.environ[\"ASTRA_DB_TOKEN\"] = getpass(\"Enter you Astra DB Token: \")\n",
    "os.environ[\"OPEN_AI_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Create RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model and vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample dataset from Llama Hub into your Astra vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import download_llama_dataset\n",
    "\n",
    "!mkdir -p 'data'\n",
    "\n",
    "dataset = download_llama_dataset(\n",
    "  \"PaulGrahamEssayDataset\", \"./data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the documents from the dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import AstraDBVectorStore\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/source_files\").load_data()\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"First document, id: {documents[0].doc_id}\")\n",
    "print(f\"First document, hash: {documents[0].hash}\")\n",
    "print(\n",
    "    \"First document, text\"\n",
    "    f\" ({len(documents[0].text)} characters):\\n\"\n",
    "    f\"{'=' * 20}\\n\"\n",
    "    f\"{documents[0].text[:360]} ...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector store instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import AstraDBVectorStore\n",
    "import os\n",
    "\n",
    "astra_db_store = AstraDBVectorStore(\n",
    "    token=os.getenv(\"ASTRA_DB_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_ENDPOINT\"),\n",
    "    collection_name=\"test_llama\",\n",
    "    embedding_dimension=1536,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=astra_db_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "\n",
    "Query the index for the most relevant answer to your prompt, \"Why did the author choose to work on AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query_string_1 = \"Why did the author choose to work on AI?\"\n",
    "response = query_engine.query(query_string_1)\n",
    "\n",
    "print(query_string_1)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Use a retriever to retrieve results from your vector store index based on your prompt.\n",
    "\n",
    "This will retrieve three nodes based on your prompt, and return the nodes with their relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    vector_store_query_mode=\"default\",\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "nodes_with_scores = retriever.retrieve(query_string_1)\n",
    "\n",
    "print(query_string_1)\n",
    "print(f\"Found {len(nodes_with_scores)} nodes.\")\n",
    "for idx, node_with_score in enumerate(nodes_with_scores):\n",
    "    print(f\"    [{idx}] score = {node_with_score.score}\")\n",
    "    print(f\"        id    = {node_with_score.node.node_id}\")\n",
    "    print(f\"        text  = {node_with_score.node.text[:90]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMR\n",
    "\n",
    "Set the retriever to sort results by Maximal Marginal Relevance, or MMR, instead of the default similarity search.\n",
    "\n",
    "Send the prompt again. The top result is the most relevant (positive number), while the other results are the least relevant (negative numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    vector_store_query_mode=\"mmr\",\n",
    "    similarity_top_k=3,\n",
    "    vector_store_kwargs={\"mmr_prefetch_factor\": 4},\n",
    ")\n",
    "\n",
    "nodes_with_scores = retriever.retrieve(query_string_1)\n",
    "\n",
    "print(query_string_1)\n",
    "print(f\"Found {len(nodes_with_scores)} nodes.\")\n",
    "for idx, node_with_score in enumerate(nodes_with_scores):\n",
    "    print(f\"    [{idx}] score = {node_with_score.score}\")\n",
    "    print(f\"        id    = {node_with_score.node.node_id}\")\n",
    "    print(f\"        text  = {node_with_score.node.text[:90]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WARNING: This will delete the collection and all documents in the collection\n",
    "#Â astra_db_store.delete_collection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
