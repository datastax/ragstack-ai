{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with LlamaIndex and AstraDB\n",
    "\n",
    "Build a RAG pipeline with RAGStack, AstraDB, and LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "You will need a vector-enabled Astra database.\n",
    "\n",
    "Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
    "Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
    "Get your Astra DB Endpoint:\n",
    "https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com\n",
    "\n",
    "See the [Prerequisites](https://docs.datastax.com/en/ragstack/docs/prerequisites.html) page for more details.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -qU ragstack-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your settings for Astra DB and OpenAI:\n",
    "keys = [\"ASTRA_DB_APPLICATION_TOKEN\", \"ASTRA_DB_API_ENDPOINT\", \"OPENAI_API_KEY\"]\n",
    "for key in keys:\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass(f\"Enter {key}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Collections are where documents are stored. ex: test\n",
    "collection = input(\"Collection: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model and vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample dataset from Llama Hub into your Astra vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llama_dataset import download_llama_dataset\n",
    "\n",
    "!mkdir -p 'data'\n",
    "\n",
    "dataset = download_llama_dataset(\n",
    "  \"PaulGrahamEssayDataset\", \"./data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import AstraDBVectorStore\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"First document, id: {documents[0].doc_id}\")\n",
    "print(f\"First document, hash: {documents[0].hash}\")\n",
    "print(\n",
    "    \"First document, text\"\n",
    "    f\" ({len(documents[0].text)} characters):\\n\"\n",
    "    f\"{'=' * 20}\\n\"\n",
    "    f\"{documents[0].text[:360]} ...\"\n",
    ")\n",
    "\n",
    "astra_db_store = AstraDBVectorStore(\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    "    collection_name=\"test\",\n",
    "    embedding_dimension=1536,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=astra_db_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query\n",
    "\n",
    "Query the index for the most relevant answer to your prompt, \"Why did the author choose to work on AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query_string_1 = \"Why did the author choose to work on AI?\"\n",
    "response = query_engine.query(query_string_1)\n",
    "\n",
    "print(query_string_1)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a retriever to retrieve results from your vector store index based on your prompt.\n",
    "\n",
    "This will retrieve three nodes based on your prompt, and return the nodes with their relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    vector_store_query_mode=\"default\",\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "nodes_with_scores = retriever.retrieve(query_string_1)\n",
    "\n",
    "print(query_string_1)\n",
    "print(f\"Found {len(nodes_with_scores)} nodes.\")\n",
    "for idx, node_with_score in enumerate(nodes_with_scores):\n",
    "    print(f\"    [{idx}] score = {node_with_score.score}\")\n",
    "    print(f\"        id    = {node_with_score.node.node_id}\")\n",
    "    print(f\"        text  = {node_with_score.node.text[:90]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMR\n",
    "\n",
    "Set the retriever to sort results by Maximal Marginal Relevance, or MMR, instead of the default similarity search.\n",
    "\n",
    "Send the prompt again. The top result is the most relevant (positive number), while the other results are the least relevant (negative numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    vector_store_query_mode=\"mmr\",\n",
    "    similarity_top_k=3,\n",
    "    vector_store_kwargs={\"mmr_prefetch_factor\": 4},\n",
    ")\n",
    "\n",
    "nodes_with_scores = retriever.retrieve(query_string_1)\n",
    "\n",
    "print(query_string_1)\n",
    "print(f\"Found {len(nodes_with_scores)} nodes.\")\n",
    "for idx, node_with_score in enumerate(nodes_with_scores):\n",
    "    print(f\"    [{idx}] score = {node_with_score.score}\")\n",
    "    print(f\"        id    = {node_with_score.node.node_id}\")\n",
    "    print(f\"        text  = {node_with_score.node.text[:90]} ...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
