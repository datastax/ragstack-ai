{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/nemo_guardrails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NeMo Guardrails\n",
    "\n",
    "This notebooks demonstrates how to set up and use NVIDIA NeMo's Guardrails. [NVIDIA NeMo](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/) is a cloud-native framework designed for AI models. [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) is a toolkit for adding programmable guardrails to LLM-based conversational applications. In this notebook, you will create an RAG-based application featuring a Retrieval rail that answers only queries relevant to the provided knowledge base. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeMo Guardrails](resources/rails.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "You will need a vector-enabled Astra database. This notebook uses OpenAI, though you can certainly use the NVIDIA models, as NeMo [supports all LLM providers supported by Langchain](https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/user_guides/configuration-guide.md#supported-llm-models).\n",
    "\n",
    "* Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
    "* Create an [OpenAI account](https://openai.com/)\n",
    "* Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
    "* Get your Astra DB Endpoint: \n",
    "  * `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`\n",
    "\n",
    "See the [Prerequisites](https://docs.datastax.com/en/ragstack/examples/prerequisites.html) page for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbmake": {
     "post_cell_execute": [
      "from conftest import before_notebook",
      "before_notebook()"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU ragstack-ai asyncio nemoguardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Enter your settings for Astra DB and OpenAI:\n",
    "os.environ[\"ASTRA_DB_API_ENDPOINT\"] = input(\"Enter your Astra DB API Endpoint: \")\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass(\"Enter your Astra DB Token: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collections are where documents are stored. ex: test\n",
    "collection = \"nemo_guardrails_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The texts are pre-split into portions for ingestion\n",
    "KNOWLEDGE_BASE = [\n",
    "    \"The year 2020 brought unprecedented challenges to the global financial economy, reshaping industries and markets in ways never before seen. From the onset of the COVID-19 pandemic to the profound shifts in consumer behavior, it was a year of resilience, adaptation, and lessons learned. In this blog post, we'll explore key events and statistics that defined the financial landscape of 2020, shedding light on the path forward.\",  # noqa: E501\n",
    "    \"The COVID-19 pandemic led to a sharp economic downturn, with global GDP contracting by an estimated 3.5%. Major stock indices experienced historic declines, with the S&P 500 and Dow Jones Industrial Average falling by over 30% in March.\"  # noqa: E501\n",
    "    \"Governments worldwide responded with unprecedented fiscal and monetary stimulus packages. The total fiscal support provided amounted to over $12 trillion, including direct payments to individuals and loans/grants for businesses. The U.S. government alone passed stimulus packages totaling over $4 trillion, while the Federal Reserve expanded its balance sheet by more than $3 trillion.\"  # noqa: E501\n",
    "    \"Lockdowns and social distancing measures accelerated existing trends towards e-commerce and remote work. Global e-commerce sales surged by over 30% to $4.28 trillion, while the number of people working remotely doubled to over 40% of the global workforce.\",  # noqa: E501\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astra vector store configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vstore = AstraDBVectorStore(\n",
    "    collection_name=collection,\n",
    "    embedding=embedding,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    ")\n",
    "vstore.add_texts(KNOWLEDGE_BASE)\n",
    "retriever = vstore.as_retriever()\n",
    "print(\"Astra vector store configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colang is a modeling language enabling the design of guardrails.\n",
    "# In it, you define user and bot behaviors, as well as the flow of conversations.\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user ask about economy\n",
    "    \"What was the economy like?\"\n",
    "    \"What contributed to the growth or setback of the economy in 2020?\"\n",
    "    \"What was the increase in e-commerce sales in 2020?\"\n",
    "    \"How much did the S&P 500 fall in 2020?\"\n",
    "\n",
    "define flow answer economy question\n",
    "    user ...\n",
    "    $answer = execute rag()\n",
    "    bot $answer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def yaml_config(engine, model) -> str:\n",
    "    return f\"\"\"\n",
    "    models:\n",
    "      - type: main\n",
    "        engine: {engine}\n",
    "        model: {model}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import BaseLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from nemoguardrails import LLMRails\n",
    "from nemoguardrails.actions.actions import ActionResult\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "Use the following context to answer the question. If you don't know the answer,\n",
    "apologize and say that you don't know, do not make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define the custom RAG Action\n",
    "async def rag(context: dict, llm: BaseLLM) -> ActionResult:\n",
    "    user_message = context.get(\"last_user_message\")\n",
    "    context_updates = {}\n",
    "\n",
    "    # Use your pre-defined AstraDB Vector Store as the retriever\n",
    "    relevant_documents = await retriever.aget_relevant_documents(user_message)\n",
    "    relevant_chunks = \"\\n\".join([chunk.page_content for chunk in relevant_documents])\n",
    "\n",
    "    # Use a custom prompt template\n",
    "    prompt_template = PromptTemplate.from_template(TEMPLATE)\n",
    "    input_variables = {\"question\": user_message, \"context\": relevant_chunks}\n",
    "\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    answer = await chain.ainvoke(input_variables)\n",
    "\n",
    "    return ActionResult(return_value=answer, context_updates=context_updates)\n",
    "\n",
    "\n",
    "def init(app: LLMRails) -> None:\n",
    "    app.register_action(rag, \"rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "engine = \"openai\"\n",
    "model_name = \"gpt-4\"\n",
    "yaml = yaml_config(engine, model_name)\n",
    "config = RailsConfig.from_content(\n",
    "    colang_content=COLANG_CONFIG,\n",
    "    yaml_content=yaml,\n",
    ")\n",
    "\n",
    "rails = LLMRails(config)\n",
    "init(rails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The economy in 2020 experienced unprecedented challenges due to the onset of the COVID-19 pandemic. The global GDP contracted by an estimated 3.5% and major stock indices like the S&P 500 and Dow Jones Industrial Average fell by over 30% in March 2020. Governments worldwide responded with unprecedented fiscal and monetary stimulus packages amounting to over $12 trillion. The U.S. government passed stimulus packages totaling over $4 trillion and the Federal Reserve expanded its balance sheet by more than $3 trillion. The pandemic also accelerated trends towards e-commerce and remote work, with global e-commerce sales surging by over 30% to $4.28 trillion and the number of people working remotely doubling to over 40% of the global workforce.\n"
     ]
    }
   ],
   "source": [
    "# Your rails is now ready to answer questions about the provided knowledge base\n",
    "response = rails.generate(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What was the economy like in 2020?\"}]\n",
    ")\n",
    "print(response[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the information provided does not include details about Michael Jordan's career high score.\n"
     ]
    }
   ],
   "source": [
    "# Rails does not know about other subjects\n",
    "response = rails.generate(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What was Michael Jordan's career high score?\"}\n",
    "    ]\n",
    ")\n",
    "print(response[\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}