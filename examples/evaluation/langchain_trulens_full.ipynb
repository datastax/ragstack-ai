{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Evaluation/Benchmarking using TruLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U trulens-eval # includes lang-chain as a dependency\n",
    "! pip3 install -U ipython ipywidgets # required for the trulens UI to run from inside the notebook\n",
    "! pip3 install -U llama-index # for the llamaindex-cli tool to download datasets\n",
    "! pip3 install -U astrapy # to access AstraDB vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should restart your environment after installing the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook assumes the following env vars exist in a .env file:\n",
    "#\n",
    "# ASTRA_DB_API_ENDPOINT=https://<uuid>-<region>.apps.astra.datastax.com\n",
    "# ASTRA_DB_APPLICATION_TOKEN=AstraCS:<secret>:<secret>\n",
    "# AZURE_OPENAI_ENDPOINT=https://<domain>.openai.azure.com/\n",
    "# AZURE_OPENAI_API_KEY=<secret>\n",
    "# OPENAI_API_TYPE=azure\n",
    "# OPENAI_API_VERSION=2023-05-15\n",
    "\n",
    "# and optionally this var if you want to use an external database for TruLens:\n",
    "# TRULENS_DB_CONN_STRING=<db connection string>\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_db_collection_name = \"langchain_openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Azure LLMs for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Azure-based models\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "gpt_35_turbo = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    model_version=\"0613\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "open_ai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init an AstraDB vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_astradb import AstraDBVectorStore\n",
    "import os\n",
    "\n",
    "astra_db_vstore = AstraDBVectorStore(\n",
    "    collection_name=astra_db_collection_name,\n",
    "    embedding=open_ai_embeddings,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "**Mini Squad V2**\n",
    "\n",
    "Description: This is a subset of the original SquadV2 dataset. In particular, it considers only the top 10 Wikipedia pages in terms of having questions about them.\n",
    "\n",
    "Number Of Examples: 195\n",
    "\n",
    "Examples Generated By: Human\n",
    "\n",
    "| Baseline | Context Similarity | Correct-ness | Faithful-ness | Relev-ancy | LLM | Chunk Size | Similarity Top-K | Embed Model |\n",
    "| ---      | ---                | ---          | ---           | ---        | --- | ---        | ---              | ---         |\n",
    "| [llamaindex](https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_datasets/mini_squadv2/llamaindex_baseline.py) | 0.878 | 3.464 | 0.815 | 0.697 | gpt-3.5-turbo | 1024 | 2 | text-embedding-ada-002 |\n",
    "\n",
    "Source(s): https://huggingface.co/datasets/squad_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! llamaindex-cli download-llamadataset MiniSquadV2Dataset --download-dir ./data/mini_squad_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents into memory, chunk, create embeddings, store in AstraDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('data/mini_squad_v2/source_files', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "astra_db_vstore.add_documents(splitter.split_documents(loader.load()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the list of queries and build the golden set truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "queries = []\n",
    "golden_set = []\n",
    "\n",
    "with open(\"./data/mini_squad_v2/rag_dataset.json\") as f:\n",
    "    examples = json.load(f)['examples']\n",
    "    for e in examples:\n",
    "        queries.append(e[\"query\"])\n",
    "        golden_set.append({\n",
    "            \"query\": e[\"query\"],\n",
    "            \"response\": e[\"reference_answer\"],\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a LCEL chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say: \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": astra_db_vstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | gpt_35_turbo\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a query\n",
    "response = rag_chain.invoke(\"What show in New Zealand was the inspiration for the British Series Pop Idol?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init TruLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "if os.getenv(\"TRULENS_DB_CONN_STRING\"):\n",
    "   tru = Tru(database_url=os.getenv(\"TRULENS_DB_CONN_STRING\"))\n",
    "else:\n",
    "    tru = Tru()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Reset the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tru.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Start the Dashboard UI\n",
    "\n",
    "Note that the dashboard may error on the first attempt. It should start on the 2nd try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Feedback Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider import AzureOpenAI\n",
    "from trulens_eval.feedback import Groundedness, GroundTruthAgreement\n",
    "from trulens_eval import TruChain, Feedback\n",
    "from trulens_eval.app import App\n",
    "import numpy as np\n",
    "# Initialize provider class\n",
    "azureOpenAI = AzureOpenAI(deployment_name=\"gpt-35-turbo\")\n",
    "\n",
    "context = App.select_context(rag_chain)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "grounded = Groundedness(groundedness_provider=azureOpenAI)\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"groundedness\")\n",
    "    .on(context.collect()).on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(azureOpenAI.relevance_with_cot_reasons, name=\"answer_relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(azureOpenAI.qs_relevance_with_cot_reasons, name=\"context_relevance\")\n",
    "    .on_input().on(context)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# GroundTruth for comparing the Answer to the Ground-Truth Answer\n",
    "ground_truth_collection = GroundTruthAgreement(golden_set, provider=azureOpenAI)\n",
    "f_answer_correctness = (\n",
    "    Feedback(ground_truth_collection.agreement_measure, name=\"answer_correctness\")\n",
    "    .on_input_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "We use the deferred mode, to ensure all evaluations run to completion.\n",
    "\n",
    "1. Start the script `tru_evaluate.py` in a terminal. Note that this file depends on `tru_shared.py` and your `.env` file.\n",
    "1. Run the code below to initiate the evaluation process\n",
    "1. Wait for the script to claim it is finished. Stop the Script.\n",
    "   * You are looking for something like `✅✅✅ Finished evaluating deferred feedback functions.`\n",
    "   * But the first few times it shows that, it might be incorrect. (It has finished the initial deferred functions, but it needs to re-check if there are more to still evaluate)\n",
    "\n",
    "Notes:\n",
    "* You will see this warning often: `Callback class OpenAICallback is registered for handling create but there are no endpoints waiting to receive the result.`\n",
    "  * It is a known issue and doesn't impact the results\n",
    "* It will take about 20 minutes to finish the evaluations for the 195 queries in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id=\"langchain_astra_512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru_recorder = TruChain(\n",
    "    rag_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=[f_answer_relevance, f_context_relevance, f_groundedness, f_answer_correctness],\n",
    "    feedback_mode=\"deferred\",\n",
    ")\n",
    "for query in queries:\n",
    "    with tru_recorder as recording:\n",
    "        rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Result Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this downloads the full set of records from the database for an app(s)\n",
    "dfRecords, feedbackColumns = tru.get_records_and_feedback([app_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it also includes all the trace information from each call\n",
    "# which I drop to save memory\n",
    "\n",
    "# note that token & cost data collection is currently broken with AzureOpenAI\n",
    "\n",
    "columns_to_keep = feedbackColumns + [\n",
    "    \"record_id\", \"input\", \"output\", \"tags\",\n",
    "    \"latency\", \"total_tokens\", \"total_cost\"]\n",
    "\n",
    "columns_to_drop = [col for col in dfRecords.columns if col not in columns_to_keep]\n",
    "\n",
    "dfRecords.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "dfRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Compute the mean, median, 95th percentile, 99th percentile of the evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tests = feedbackColumns + [\"latency\", \"total_tokens\", \"total_cost\"]\n",
    "\n",
    "results = pd.DataFrame(columns=[\"records\", \"mean\", \"median\", \"95th_percentile\", \"99th_percentile\"])\n",
    "results = results.astype({\"records\": \"int\", \"mean\": \"float\", \"median\": \"float\", \"95th_percentile\": \"float\", \"99th_percentile\": \"float\"})\n",
    "\n",
    "for test in tests:\n",
    "    data = dfRecords[test].dropna().to_list()\n",
    "\n",
    "    records = len(data)\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    percentile_95 = np.percentile(data, 95)\n",
    "    percentile_99 = np.percentile(data, 99)\n",
    "\n",
    "    results.loc[test] = [records, mean, median, percentile_95, percentile_99]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the number of records for each test don't match, then the evaluations have not yet completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragstack-ai-B4Qzu5Pn-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}