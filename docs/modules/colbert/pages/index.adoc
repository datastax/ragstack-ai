= Introduction to ColBERT

ColBERT stands for "Contextualized Late Interaction over BERT".

"Contextualized Late Interaction" describes a unique method of interacting with Stanford University's https://arxiv.org/abs/2004.12832[BERT]{external-link-icon} model.

ColBERT is a machine learning retrieval model that improves the computational efficiency and contextual depth of information retrieval tasks.

*TL;DR:*

1. BERT embeds text chunks as matrices of token-level vectors, enabling much deeper context matching than a single vector embedding per chunk.
2. BERT manages this additional depth by pre-processing documents and queries into uniform lengths with the https://huggingface.co/learn/nlp-course/en/chapter6/6[Wordpiece] tokenizer, ideal for batch processing on GPUs.
3. "Contextualized Late Interaction" first retrieves the top-k chunks with the highest similarity scores to the query token.
The top-k chunks are then sorted again. The query token is compared to every token in the chunk to rank the chunks by the highest aggregate similarity score.

See the xref:examples:colbert.adoc[ColBERT example code] to get started using ColBERT with RAGStack and AstraDB.

== RAGStack-ai-colbert packages

`ragstack-ai-colbert` contains the implementation of the ColBERT retrieval.

The `colbert` module provides a vanilla implementation for ColBERT retrieval. It is not tied to any specific framework and can be used with any of the RAGStack packages.

To use ColBERT with LangChain or LLamaIndex, install ColBERT as an extra:

. `ragstack-ai-langchain[colbert]`
. `ragstack-ai-llamaindex[colbert]`

== How is ColBERT different from RAG?

In the common RAG usage, a standard embedding model represents each chunk as a single vector embedding.
This is called "sparse embedding".
A cosine similarity search performs similarity matching between document and query embeddings, and the top-k results are returned.
This is fast and straightforward, but some context and efficiency are lost.

In the ColBERT model, each chunk is represented as a list of token-level embedding vectors.
This is called "dense embedding".
This per-token "bag of words" within a chunk offers far deeper context than a single vector per chunk.
Document embeddings are pre-computed and indexed with a uniform length to facilitate batch processing.

ColBERT queries are performed in two stages:

1. The query is embedded (densely) and an Approximate Nearest Neighbor (ANN) search compares every query vector token to every context vector token.
Recall that the BERT context chunks have embeddings for each token, so this is a dense comparison.
The closest matches are returned as the top-k chunks.
2. Contextualized Late Interaction ranks the top-k chunks by a fine-grained similarity score.
For each queryâ€™s token embedding, the score function generates a highest similarity score based on the max dot product of the query token vector, and all the token embeddings per chunk. The aggregate of all the max scores across all the query tokens is the overall similarity score of that particular chunk.

== ColBERT, RAGStack, and AstraDB

The https://huggingface.co/colbert-ir/colbertv2.0[ColBERT v2.0]{external-link-icon} library transforms a text chunk into a matrix of token-level embeddings. The output is a 128-dimensional vector for each token in a chunk. This results in a two-dimensional matrix, which doesn't align with the current LangChain interface that outputs a list of floats.

To solve this problem, the ragstack-ai-colbert packages and extras include a new abstract class for mapping token-level embedding vectors to the AstraDB vector database.

The vector storage creates two tables. One table stores the actual text chunks with their unique IDs.
The `chunk_id` links to token tables for that unique chunk.
The values in the `part` column are derived from the `embed_documents` and `embed_tokens` methods.

.Chunks table schema
[source,bash]
----
chunks(
    title text,
    chunk_id int,
    body text,
    PRIMARY KEY (title,part));

----

The second table stores the per-token vectors and the BERT index.

.Colbert embeddings schema
[source,bash]
----
colbert_embeddings(
    title text,
    chunk_id int,
    embedding_id int,
    bert_embedding_vector<float,128>,
    PRIMARY KEY(title, part, embedding_id));

----

Retrieval logic is defined in the https://github.com/datastax/ragstack-ai/blob/main/libs/colbert/ragstack_colbert/colbert_retriever.py[ColbertRetriever] class, which asynchronously retrieves and scores chunks.






