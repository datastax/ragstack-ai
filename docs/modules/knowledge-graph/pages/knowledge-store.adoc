= {graph-store}

{graph-store} is a hybrid vector-and-graph store that combines the benefits of vector stores with the context and relationships of related edges between chunks.

See the xref:examples:knowledge-store.adoc[graph store example code] to get started with {graph-store}.

[IMPORTANT]
====
This feature is currently under development and has not been fully tested. It is not supported for use in production environments. Please use this feature in testing and development environments only.
====

== The `ragstack-ai-knowledge-store` library

The `ragstack-ai-knowledge-store` library contains functions for creating a hybrid vector-and-graph knowledge store. This store combines the benefits of vector stores with the context and relationships of a related edges.

To install the package, run:

[source,bash]
----
pip install ragstack-ai-knowledge-store
----

== What is the difference between coarse-grained and fine-grained knowledge graphs?

Fine-grained knowledge graphs (like xref:knowledge-graph.adoc[]) capture edge relationships between entities.
A knowledge graph is extracted from unstructured information, and its entities and their edge relationships are stored in a vector or graph store.

Fine-grained knowledge graphs capture relationships between information that vector similarity search would miss, and LLMs make it possible to extract knowledge graph triples (source, relationship, target) from unstructured content with only a prompt.

However, extracting this fine-grained knowledge graph from unstructured information is difficult, time-consuming and error-prone, even using LLMs for extraction. A user has to guide the LLM on the kinds of nodes and relationships to be extracted with a schema, and if the knowledge schema changes, the graph has to be processed again. The context advantages of fine-grained knowledge graphs are great, but the cost to build and maintain them is much higher than just chunking and embedding content to a vector store.

**Coarse-grained knowledge graphs** offer a compromise between the ease and scalability of vector similarity search, and the context and relationships of fine-grained knowledge graphs.

Start with nodes that represent content (a specific document about Seattle), instead of fine-grained concepts or entities (a node representing Seattle). A node may represent a table, an image, or a section of a document. Since the node represents the original content, the nodes are exactly what is stored when using vector search.

Unstructured content is loaded, chunked, and written to the store.
Each chunk can be run through a variety of analyses to identify links. For example, links in the content may turn into `links_to edges`, and keywords may be extracted from the chunk to link up with other chunks on the same topic.

Edges between nodes can be added in a few ways. Each chunk may be annotated with URLs that it represents, or each chunk may be associated with keywords.

Retrieval is where the benefits of vector search and knowledge graph traversal come together.
The query's initial starting points in the knowledge graph are identified based on vector similarity to the question, and then additional chunks are selected by following edges from that node. Including nodes that are related both by embedding distance (similarity) and graph distance (related) leads to a more diverse set of chunks with a deeper context and reduced hallucinations.

For a step-by-step example, see the xref:examples:knowledge-store.adoc[{graph-store} example code].





