= RAGStack Python packages

RAGStack comes with a set of Python packages that provide the necessary tools to implement the RAG pattern in your applications.

. `ragstack-ai`: All-in-one package that contains all components supported by RAGStack. While this is the most convenient package to use, it may be heavier than necessary for some use cases.
. `ragstack-ai-langchain`: This package is meant for users who want to use RAGStack with the LangChain framework.
. `ragstack-ai-llamaindex`: This package is meant for users who want to use RAGStack with the LlamaIndex framework.
. `ragstack-ai-colbert`: This package contains the implementation of the ColBERT retrieval.

== Supported integrations for `ragstack-ai-langchain`

The `ragstack-ai-langchain` package includes the minimum set of dependencies for using LangChain with {astra_db}.
LLMs, embeddings, and third-party providers are not included in this package by default, except for OpenAI and Azure OpenAI.

To use LLMs, embeddings, or third-party providers, you can leverage `ragstack-ai-langchain` extras:

. `ragstack-ai-langchain[google]` lets you work with https://python.langchain.com/docs/integrations/platforms/google[Google Vertex AI and Google Gemini API]{external-link-icon}.
. `ragstack-ai-langchain[nvidia]` lets you work with https://python.langchain.com/docs/integrations/providers/nvidia/[NVIDIA hosted API endpoints for NVIDIA AI Foundation Models]{external-link-icon}.

Additional LangChain packages should work out of the box, although you need to manage the packages and their dependencies yourself.

== Supported integrations for `ragstack-ai-llamaindex`

The `ragstack-ai-llamaindex` package includes the minimum set of dependencies for using LlamaIndex with {astra_db}.
LLMs, embeddings, and third-party providers are not included in this package by default, except for OpenAI.

To use LLMs, embeddings, or third-party providers, you can leverage `ragstack-ai-llamaindex` extras:

. `ragstack-ai-llamaindex[google]` lets you work with https://docs.llamaindex.ai/en/stable/examples/llm/vertex/[Google Vertex AI]{external-link-icon} and https://docs.llamaindex.ai/en/stable/examples/llm/gemini/[Google Gemini API]{external-link-icon}.
. `ragstack-ai-llamaindex[azure]` lets you work with https://docs.llamaindex.ai/en/stable/examples/llm/azure_openai/[Azure OpenAI]{external-link-icon}.
. `ragstack-ai-llamaindex[bedrock]` lets you work with https://docs.llamaindex.ai/en/stable/examples/llm/bedrock/[AWS Bedrock]{external-link-icon}.

Additional LLamaIndex packages should work out of the box, although you need to manage the packages and their dependencies yourself.

== ColBERT with `ragstack-ai-langchain` and `ragstack-ai-llamaindex`

The `colbert` module provides a vanilla implementation for ColBERT retrieval. It is not tied to any specific framework and can be used with any of the RAGStack packages.

If you want to use ColBERT with LangChain or LLamaIndex, you can use the following the extras:

. `ragstack-ai-langchain[colbert]`
. `ragstack-ai-llamaindex[colbert]`
