= Quickstart

This quickstart demonstrates a basic RAG pattern using RAGStack and the AstraDB vector database to retrieve context and pass it to a language model for generation.

1. <<Construct information base>>
2. <<Basic retrieval>>
3. <<Generation with augmented context>>

A <<Colab notebook>> and the <<Complete code>> and are available at the bottom of the page.

== Setup

RAGStack includes all the libraries you need for the RAG pattern, including the vector database, embeddings pipeline, and retrieval.

This quickstart also uses the HuggingFace datasets library to load a small dataset of philosophical quotes.

. Install RAGStack and the datasets library:
+
[source,bash]
----
pip3 install ragstack-ai datasets
----

. If you don't have a vector database, create one at https://astra.datastax.com/.
+
The Astra application token must have Database Administrator permissions (e.g. `AstraCS:WSnyFUhRxsrg…`​).
+
The Astra API endpoint is available in the Astra Portal (e.g. `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`).
+
Create an OpenAI key at https://platform.openai.com/ (e.g. `sk-xxxx`).
+
You must have an existing collection in Astra (e.g. `my-collection`).
+
. Create a `.env` file in the root of your program with the values from your Astra Connect tab.
+
[source,bash]
----
ASTRA_DB_APPLICATION_TOKEN="<AstraCS:...>"
ASTRA_DB_API_ENDPOINT="<Astra DB API endpoint>"
OPENAI_API_KEY="sk-..."
ASTRA_DB_COLLECTION="test"
----

== RAG workflow

With your environment set up, you're ready to create a RAG workflow in Python.

. Import the necessary dependencies:
+
[source,python]
----
import os
from dotenv import load_dotenv
from datasets import load_dataset
from langchain.vectorstores.astradb import AstraDB
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
----

. Load the environment variables:
+
[source,python]
----
load_dotenv()
ASTRA_DB_APPLICATION_TOKEN = os.environ.get("ASTRA_DB_APPLICATION_TOKEN")
ASTRA_DB_API_ENDPOINT = os.environ.get("ASTRA_DB_API_ENDPOINT")
OPEN_AI_API_KEY = os.environ.get("OPENAI_API_KEY")
ASTRA_DB_COLLECTION = os.environ.get("ASTRA_DB_COLLECTION")
----

=== Construct information base

. Declare the embeddings model, create your vector database, and configure their required parameters.
+
[source,python]
----
embedding = OpenAIEmbeddings()
vstore = AstraDB(
    embedding=embedding,
    collection_name="test",
    token=os.environ["ASTRA_DB_APPLICATION_TOKEN"],
    api_endpoint=os.environ["ASTRA_DB_API_ENDPOINT"],
)
----

. Load a small dataset of quotes with the Python dataset module.
+
[source,python]
----
philo_dataset = load_dataset("datastax/philosopher-quotes")["train"]
print("An example entry:")
print(philo_dataset[16])
----

. Process metadata and convert to a `Document` object:
+
[source,python]
----
docs = []
for entry in philo_dataset:
    metadata = {"author": entry["author"]}
    if entry["tags"]:
        # Add metadata tags to the metadata dictionary
        for tag in entry["tags"].split(";"):
            metadata[tag] = "y"
    # Add a LangChain document with the quote and metadata tags
    doc = Document(page_content=entry["quote"], metadata=metadata)
    docs.append(doc)
----

. Compute embeddings:
+
[source,python]
----
inserted_ids = vstore.add_documents(docs)
print(f"\nInserted {len(inserted_ids)} documents.")
----

=== Basic retrieval

Confirm your vector store is populated by printing the vectors in your collection:
[source,python]
----
print(vstore.astra_db.collection(ASTRA_DB_COLLECTION).find())
----

=== Generation with augmented context

. Retrieve context from your vector database, pass it to OpenAI with a prompt question, and print the response.
+
[source,python]
----
retriever = vstore.as_retriever(search_kwargs={'k': 3})

prompt_template = """
Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.
Context: {context}
Question: {question}
Your answer:
"""
prompt = ChatPromptTemplate.from_template(prompt_template)
model = ChatOpenAI(openai_api_key=OPEN_AI_API_KEY)

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

response = chain.invoke("In the given context, what subject are philosophers most concerned with?")
print(response)
----

. You should get a response like this:
[source,bash]
----
An example entry:
{'author': 'aristotle', 'quote': 'Love well, be loved and do something of value.', 'tags': 'love;ethics'}

Inserted 450 documents.
The subject that philosophers are most concerned with in the given context is truth.
----

== Colab notebook

Try the example above with a https://colab.research.google.com/github/datastax/ragstack-ai-examples/blob/main/ragstack.ipynb[Colab notebook]{external-link-icon}.

== Complete code

[tabs]
======
Python::
+
[source,python]
----
import os
from dotenv import load_dotenv
from datasets import load_dataset
from langchain.vectorstores.astradb import AstraDB
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

load_dotenv()

ASTRA_DB_APPLICATION_TOKEN = os.environ.get("ASTRA_DB_APPLICATION_TOKEN")
ASTRA_DB_API_ENDPOINT = os.environ.get("ASTRA_DB_API_ENDPOINT")
OPEN_AI_API_KEY = os.environ.get("OPENAI_API_KEY")
ASTRA_DB_COLLECTION = os.environ.get("ASTRA_DB_COLLECTION")

embedding = OpenAIEmbeddings()
vstore = AstraDB(
    embedding=embedding,
    collection_name="test",
    token=os.environ["ASTRA_DB_APPLICATION_TOKEN"],
    api_endpoint=os.environ["ASTRA_DB_API_ENDPOINT"],
)

philo_dataset = load_dataset("datastax/philosopher-quotes")["train"]
print("An example entry:")
print(philo_dataset[16])

docs = []
for entry in philo_dataset:
    metadata = {"author": entry["author"]}
    if entry["tags"]:
        # Add metadata tags to the metadata dictionary
        for tag in entry["tags"].split(";"):
            metadata[tag] = "y"
    # Add a LangChain document with the quote and metadata tags
    doc = Document(page_content=entry["quote"], metadata=metadata)
    docs.append(doc)

inserted_ids = vstore.add_documents(docs)
print(f"\nInserted {len(inserted_ids)} documents.")

print(vstore.astra_db.collection(ASTRA_DB_COLLECTION).find())

retriever = vstore.as_retriever(search_kwargs={'k': 3})

prompt_template = """
Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.
Context: {context}
Question: {question}
Your answer:
"""
prompt = ChatPromptTemplate.from_template(prompt_template)
model = ChatOpenAI(openai_api_key=OPEN_AI_API_KEY)

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

response = chain.invoke("In the given context, what subject are philosophers most concerned with?")
print(response)
----

Result::
+
[source,bash]
----
An example entry:
{'author': 'aristotle', 'quote': 'Love well, be loved and do something of value.', 'tags': 'love;ethics'}

Inserted 450 documents.
The subject that philosophers are most concerned with in the given context is truth.
----
======



