= VectorStore QA with MMR
:navtitle: VectorStore QA with MMR
:page-layout: tutorial
:page-icon-role: bg-[var(--ds-neutral-900)]
:page-toclevels: 1
:page-colab-link: https://colab.research.google.com/github/CassioML/cassio-website/blob/main/docs/frameworks/langchain/.colab/colab_qa-maximal-marginal-relevance.ipynb

This page demonstrates using RAGStack and an vector-enabled {db-serverless} database to perform vector search with the *Maximal Marginal Relevance (MMR)* algorithm.

Instead of selecting the top _k_ stored documents most relevant to the provided query, MMR first identifies a larger pool of relevant results, and then retrieves top _k_ from this pool. MMR algorithms return results with more diverse information.

== Prerequisites

. You will need an vector-enabled {db-serverless} database.
+
.. Create an https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html[Astra
vector database].
+
.. Within your database, create an https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html[Astra
DB Access Token] with Database Administrator permissions.
+
.. Copy the {db-serverless} API Endpoint for your {db-serverless} database.
+
. Set the following environment variables in a `.env` file in the root of your project:
+
[source,text]
----
ASTRA_DB_API_ENDPOINT=https://...
ASTRA_DB_APPLICATION_TOKEN=AstraCS:...
ASTRA_DB_KEYSPACE=default_keyspace #optional
OPENAI_API_KEY=sk-...
----
+
. Install the following dependencies:
+
[source,python]
----
pip install -qU ragstack-ai python-dotenv
----
+
See the https://docs.datastax.com/en/ragstack/docs/prerequisites.html[Prerequisites] page for more details.

== Create embedding model and vector store

. Import dependencies and load environment variables.
+
[source,python]
----
import os
from dotenv import load_dotenv
from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain
from langchain_openai import OpenAI, OpenAIEmbeddings
from langchain.indexes.vectorstore import VectorStoreIndexWrapper
from langchain_astradb import AstraDBVectorStore

load_dotenv()
----
+
. Initialize the OpenAI model and embeddings.
+
[source,python]
----
llm = OpenAI(temperature=0)
myEmbedding = OpenAIEmbeddings()
----
+
. Initialize the vector store.
+
[source,python]
----
myAstraDBVStore = AstraDBVectorStore(
    embedding=myEmbedding,
    api_endpoint=os.environ["ASTRA_DB_API_ENDPOINT"],
    token=os.environ["ASTRA_DB_APPLICATION_TOKEN"],
    namespace=os.environ.get("ASTRA_DB_KEYSPACE"),  # this is optional
    collection_name="mmr_test",
)
index = VectorStoreIndexWrapper(vectorstore=myAstraDBVStore)
----

== Populate the vector store

. Create a list of sentences, with their sources stored as metadata.
Note that the last sentence's content is considerably different from the others.
+
[source,python]
----
# declare data

BASE_SENTENCE_0 =     ("The frogs and the toads were meeting in the night "
                       "for a party under the moon.")

BASE_SENTENCE_1 =     ("There was a party under the moon, that all toads, "
                       "with the frogs, decided to throw that night.")

BASE_SENTENCE_2 =     ("And the frogs and the toads said: \"Let us have a party "
                       "tonight, as the moon is shining\".")

BASE_SENTENCE_3 =     ("I remember that night... toads, along with frogs, "
                       "were all busy planning a moonlit celebration.")

DIFFERENT_SENTENCE =  ("For the party, frogs and toads set a rule: "
                       "everyone was to wear a purple hat.")

# insert into index
texts = [
    BASE_SENTENCE_0,
    BASE_SENTENCE_1,
    BASE_SENTENCE_2,
    BASE_SENTENCE_3,
    DIFFERENT_SENTENCE,
]
metadatas = [
    {"source": "Barney's story at the pub"},
    {"source": "Barney's story at the pub"},
    {"source": "Barney's story at the pub"},
    {"source": "Barney's story at the pub"},
    {"source": "The chronicles at the village library"},
]
----
+
. Load the sentences into the vector store and print their IDs.
+
[source,python]
----
ids = myAstraDBVStore.add_texts(
    texts,
    metadatas=metadatas,
    )
print("\n".join(ids))
----

== Create and compare retrievers

Create one retriever with similarity search, and another retriever with MMR search.

Both will return the top 2 results with the source metadata included.
Ask them a question, and see how the MMR response differs from the similarity response.

. Set the question.
+
[source,python]
----
QUESTION = "Tell me about the party that night."
----
+
. Create a retriever with similarity search.
+
[source,python]
----
retrieverSim = myAstraDBVStore.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": 2,
    },
)

chainSimSrc = RetrievalQAWithSourcesChain.from_chain_type(
    llm,
    retriever=retrieverSim,
)

responseSimSrc = chainSimSrc.invoke({chainSimSrc.question_key: QUESTION})
print("Similarity-based chain:")
print(f"  ANSWER : {responseSimSrc['answer'].strip()}")
print(f"  SOURCES: {responseSimSrc['sources'].strip()}")
----
+
. Create a retriever with MMR search.
+
[source,python]
----
retrieverMMR = myAstraDBVStore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 2,
    },
)

chainMMRSrc = RetrievalQAWithSourcesChain.from_chain_type(
    llm,
    retriever=retrieverMMR,
)

responseMMRSrc = chainMMRSrc.invoke({chainMMRSrc.question_key: QUESTION})
print("MMR-based chain:")
print(f"  ANSWER : {responseMMRSrc['answer'].strip()}")
print(f"  SOURCES: {responseMMRSrc['sources'].strip()}")
----
+
. Run the code and observe the differences in the responses.
+
Similarity search returns only the most similar sentence.
MMR returns the `DIFFERENT_SENTENCE`, which was considerably different from the others.
+
[source,bash]
----
Similarity-based chain:
  ANSWER : The party was thrown by all the toads and frogs under the moon that night.
  SOURCES: Barney's story at the pub
MMR-based chain:
  ANSWER : The party that night was thrown by the frogs and toads, and the rule was for everyone to wear a purple hat.
  SOURCES: Barney's story at the pub, The chronicles at the village library
----

== Complete code example

include::examples:partial$mmr-example.adoc[]