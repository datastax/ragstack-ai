= Knowledge Base Search on Proprietary Data powered by Astra DB

image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai-examples/blob/main/QA_with_cassio.ipynb"]

This notebook guides you through setting up RAGStack using https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html[Astra Vector Search], https://platform.openai.com[OpenAI], and https://cassio.org/[CassIO] to implement a generative Q&A over your own documentation.

== Demo Summary
ChatGPT excels at answering questions and offers a nice dialog interface to ask questions and get answers, but it only knows about topics from its training data.

What do you do when you have your own documents? How can you leverage the GenAI and LLM models to get insights into those? We can use Retrieval Augmented Generation (RAG) -- think of a Q/A Bot that can answer specific questions over your documentation.

We can do this in two easy steps:

* Analyzing and storing existing documentation.
* Providing search capabilities for the model to retrieve your documentation.

This is solved by using LLM models. Ideally, you embed the data as vectors and store them in a vector database and then use the LLM models on top of that database.

This notebook demonstrates a basic two-step RAG technique for enabling GPT to answer questions using a library of reference on your own documentation using Astra DB Vector Search.

== Getting Started with this notebook

See xref:ROOT:prerequisites.adoc[Prerequisites] for instructions on setting up your environment.

. Install the following libraries.
+
[source,python]
----
pip install \
    "ragstack-ai" \
    "openai" \
    "pypdf" \
    "python-dotenv"
----
+
. Import dependencies.
+
[source,python]
----
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Cassandra
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain.chat_models import ChatOpenAI
from langchain.indexes.vectorstore import VectorStoreIndexWrapper
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
----
+
[NOTE]
====
You will need a secure connect bundle and a user with access permission. For demo purposes, the "administrator" role will work fine. More information about how to get the bundle can be found at https://docs.datastax.com/en/astra-serverless/docs/connect/secure-connect-bundle.html[].
====
+
. Initialize the environment variables.
+
[source,python]
----
ASTRA_DB_SECURE_BUNDLE_PATH = os.getenv("ASTRA_DB_SECURE_BUNDLE_PATH")
ASTRA_DB_APPLICATION_TOKEN = os.getenv("ASTRA_DB_APPLICATION_TOKEN")
ASTRA_DB_KEYSPACE = os.getenv("ASTRA_DB_NAMESPACE")
ASTRA_DB_TABLE_NAME = os.getenv("ASTRA_DB_COLLECTION")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
----
+
. Retrieve the text of a short story that will be indexed in the vector store and set it as the sample data. This is a short story by Edgar Allen Poe called "The Cask of Amontillado".
+
[source,python]
----
curl https://raw.githubusercontent.com/CassioML/cassio-website/main/docs/frameworks/langchain/texts/amontillado.txt --output amontillado.txt
SAMPLEDATA = ["amontillado.txt"]
----
+
. Connect to Astra DB.
+
[source,python]
----
cluster = Cluster(cloud={"secure_connect_bundle": ASTRA_DB_SECURE_BUNDLE_PATH},
                  auth_provider=PlainTextAuthProvider("token", ASTRA_DB_APPLICATION_TOKEN))
session = cluster.connect()
----

== Read files, create embeddings, and store in vector DB
CassIO seamlessly integrates with RAGStack and LangChain, offering Cassandra-specific tools for many tasks.
This example uses vector stores, indexers, embeddings, and queries, with OpenAI for LLM services.

. Loop through each file and load it into the vector store.
+
[source,python]
----
documents = []
for filename in SAMPLEDATA:
    path = os.path.join(os.getcwd(), filename)

    if filename.endswith(".pdf"):
        loader = PyPDFLoader(path)
        new_docs = loader.load_and_split()
        print(f"Processed pdf file: {filename}")
    elif filename.endswith(".txt"):
        loader = TextLoader(path)
        new_docs = loader.load_and_split()
        print(f"Processed txt file: {filename}")
    else:
        print(f"Unsupported file type: {filename}")

    if len(new_docs) > 0:
        documents.extend(new_docs)
----
+
. Initialize the vector store with the documents and the OpenAI embeddings.
+
[source,python]
----
cassVStore = Cassandra.from_documents(
    documents=documents,
    embedding=OpenAIEmbeddings(),
    session=session,
    keyspace=ASTRA_DB_KEYSPACE,
    table_name=ASTRA_DB_TABLE_NAME,
)
----
+
. Empty the list of file names -- we don't want to accidentally load the same files again.
+
[source,python]
----
SAMPLEDATA = []
print(f"\nProcessing done.")
----

== Query the vector store and execute some "searches" against it
. Start with a similarity search using the Vectorstore's implementation.
+
[source,python]
----
prompt = "Who is Luchesi?"

matched_docs = cassVStore.similarity_search(query=prompt, k=1)

for i, d in enumerate(matched_docs):
    print(f"\n## Document {i}\n")
    print(d.page_content)
----
+
. To implement Q/A over documents, you need to perform some additional steps.
Create an Index on top of the vector store.
+
[source,python]
----
index = VectorStoreIndexWrapper(vectorstore=cassVStore)
----
+
. Create a retriever from the Index.
A retriever is an interface that returns documents given an unstructured query.
It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) them.
Vector stores can be used as the backbone of a retriever.
. Query the index for relevant vectors to the prompt:
+
[source,python]
+
----
prompt = "Who is Luchesi?"
index.query(question=prompt)
----
+
. Alternatively, use a retrieval chain with a custom prompt:
+
[source,python]
----
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.prompts import ChatPromptTemplate

prompt= """
You are Marv, a sarcastic but factual chatbot. End every response with a joke related to the question.
Context: {context}
Question: {question}
Your answer:
"""
prompt = ChatPromptTemplate.from_template(prompt)

qa = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=cassVStore.as_retriever(), chain_type_kwargs={"prompt": prompt})

result = qa.run("{question: Who is Luchesi?")
result
----
