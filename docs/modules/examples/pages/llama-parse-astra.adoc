= RAG with LlamaParse and {db-serverless}
:navtitle: RAG with LlamaParse and {db-serverless}
:page-layout: tutorial
:page-icon-role: bg-[var(--ds-neutral-900)]
:page-toclevels: 1
:page-colab-link: https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/llama-parse-astra.ipynb

Build a RAG pipeline with RAGStack, {db-serverless}, and LlamaIndex.

This example demonstrates loading and parsing a PDF document with LLamaParse into an {db-serverless} vector store, then querying the index with LlamaIndex.

== Prerequisites

You will need an vector-enabled {db-serverless} database.

* Create an https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html[Astra
vector database].
* Within your database, create an https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html[Astra
DB Access Token] with Database Administrator permissions.
* Get your {db-serverless} API Endpoint:
** `+https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com+`
* Create an API key at https://cloud.llamaindex.ai/[LlamaIndex.ai].
Install the following dependencies:
[source,python]
----
pip install ragstack-ai
----
See the https://docs.datastax.com/en/ragstack/examples/prerequisites.html[Prerequisites] page for more details.

== Set up your local environment

Create a `.env` file in your application directory with the following environment variables:
[source,bash]
----
LLAMA_CLOUD_API_KEY=llx-...
ASTRA_DB_API_ENDPOINT=https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com
ASTRA_DB_APPLICATION_TOKEN=AstraCS:...
OPENAI_API_KEY=sk-...
----

If you're using Google Colab, you'll be prompted for these values in the Colab environment.

See the https://docs.datastax.com/en/ragstack/examples/prerequisites.html[Prerequisites] page for more details.

== Create RAG pipeline

. Import dependencies and load environment variables.
+
[source,python]
----
import os
import requests
from dotenv import load_dotenv
from llama_parse import LlamaParse
from llama_index.vector_stores.astra_db import AstraDBVectorStore
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

load_dotenv()

llama_cloud_api_key = os.getenv("LLAMA_CLOUD_API_KEY")
api_endpoint = os.getenv("ASTRA_DB_API_ENDPOINT")
token = os.getenv("ASTRA_DB_APPLICATION_TOKEN")
openai_api_key = os.getenv("OPENAI_API_KEY")
----
+
. Configure global settings for LlamaIndex. (As of LlamaIndex v.0.10.0, the `Settings` component replaces `ServiceContext`. For more information, see the https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/?h=servicecontext[LlamaIndex documentation]).
+
[source,python]
----
Settings.llm = OpenAI(model="gpt-4", temperature=0.1)
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-small", embed_batch_size=100
)
----
+
. Download a PDF about attention mechanisms in transformer model architectures.
+
[source,python]
----
url = "https://arxiv.org/pdf/1706.03762.pdf"
file_path = "./attention.pdf"

response = requests.get(url, timeout=30)
if response.status_code == 200:
    with open(file_path, "wb") as file:
        file.write(response.content)
    print("Download complete.")
else:
    print("Error downloading the file.")
----
+
. Load the downloaded PDF with LlamaParse as a text Document for indexing.
LlamaParse also supports Markdown-type Documents with `(result_type=markdown)`.
+
[source,python]
----
documents = LlamaParse(result_type="text").load_data(file_path)
print(documents[0].get_content()[10000:11000])
----
+
. Create an {db-serverless} vector store instance.
+
[source,python]
----
astra_db_store = AstraDBVectorStore(
    token=token,
    api_endpoint=api_endpoint,
    collection_name="astra_v_table_llamaparse",
    embedding_dimension=1536
)
----
+
. Parse Documents into nodes and set up storage context to use {db-serverless}.
+
[source,python]
----
node_parser = SimpleNodeParser()
nodes = node_parser.get_nodes_from_documents(documents)
print(nodes[0].get_content())

storage_context = StorageContext.from_defaults(vector_store=astra_db_store)
----
+
. Create a vector store index and query engine from your nodes and contexts.
+
[source,python]
----
index = VectorStoreIndex(nodes=nodes, storage_context=storage_context)
query_engine = index.as_query_engine(similarity_top_k=15)
----

== Execute a query

. Query the {db-serverless} vector store for an example with expected context - this query should return a relevant response.
+
[source,python]
----
query = "What is Multi-Head Attention also known as?"
response_1 = query_engine.query(query)
print("\n***********New LlamaParse+ Basic Query Engine***********")
print(response_1)
----
+
. Query the {db-serverless} vector store for an example with expected lack of context.
This query should return `The context does not provide information about the color of the sky` because your document does not contain information about the color of the sky.
+
[source,python]
----
query = "What is the color of the sky?"
response_2 = query_engine.query(query)
print("\n***********New LlamaParse+ Basic Query Engine***********")
print(response_2)
----

== Complete code

include::examples:partial$llama-parse.adoc[]

