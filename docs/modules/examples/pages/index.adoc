= RAGStack Examples Index

This section contains examples of how to use RAGStack.
We're actively updating this section, so check back often!

<<langchain-astra,LangChain and {db-serverless}>>

<<llama-astra,LlamaIndex and {db-serverless}>>

<<langchain-cassio,LangChain and Cass-IO (Cassandra)>>

[[langchain-astra]]
.LangChain and Astra DB Serverless
[options="header"]
|===
| Description | Colab | Documentation

| Perform multi-modal RAG with LangChain, {db-serverless}, and a Google Gemini Pro Vision model.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/langchain_multimodal_gemini.ipynb"]
| xref:langchain_multimodal_gemini.adoc[]

| Build a simple RAG pipeline using https://catalog.ngc.nvidia.com[NVIDIA AI Foundation Models]{external-link-icon}.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/nvidia.ipynb"]
| xref:nvidia_embeddings.adoc[]

| Build a hotels search application with RAGStack and {db-serverless}.
a| image::https://gitpod.io/button/open-in-gitpod.svg[align="left",110,link="https://gitpod.io/#https://github.com/hemidactylus/langchain-astrapy-hotels-app"]
| xref:hotels-app.adoc[]

| Vector search with the Maximal Marginal Relevance (MMR) algorithm.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/CassioML/cassio-website/blob/main/docs/frameworks/langchain/.colab/colab_qa-maximal-marginal-relevance.ipynb"]
| xref:mmr.adoc[]

| Evaluate a RAG pipeline using LangChain's QA Evaluator.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/langchain_evaluation.ipynb"]
| xref:langchain-evaluation.adoc[]

| Evaluate the response accuracy, token cost, and responsiveness of MultiQueryRAG and ParentDocumentRAG.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/advancedRAG.ipynb"]
| xref:advanced-rag.adoc[]

| Orchestrate the advanced FLARE retrieval technique in a RAG pipeline.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/FLARE.ipynb"]
| xref:flare.adoc[]

| Build a simple RAG pipeline using Unstructured and {db-serverless}.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/langchain-unstructured-astra.ipynb"]
| xref:langchain-unstructured-astra.adoc[]

|===

[[llama-astra]]
.LlamaIndex and Astra DB Serverless
[options="header"]
|===
| Description | Colab | Documentation

| Build a simple RAG pipeline using LlamaIndex and {db-serverless}.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/llama-astra.ipynb"]
| xref:llama-astra.adoc[]

| Build a simple RAG pipeline using LlamaParse and {db-serverless}.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/llama-parse-astra.ipynb"]
| xref:llama-parse-astra.adoc[]

|===

[[langchain-cassio]]
.LangChain and Cass-IO (Cassandra)
[options="header"]
|===
| Description | Colab | Documentation

| Create ColBERT embeddings, index embeddings on Astra, and retrieve embeddings with RAGStack.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/RAGStackColBERT.ipynb"]
| xref:colbert.adoc[]

| Implement a generative Q&A over your own documentation with {db-serverless} Search, OpenAI, and CassIO.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/QA_with_cassio.ipynb"]
| xref:qa-with-cassio.adoc[]

| Store external or proprietary data in {db-serverless} and query it to provide more up-to-date LLM responses.
a| image::https://colab.research.google.com/assets/colab-badge.svg[align="left",link="https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/RAG_with_cassio.ipynb"]
| xref:rag-with-cassio.adoc[]

|===

