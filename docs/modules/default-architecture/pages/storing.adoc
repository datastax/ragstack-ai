= Store Embeddings

We recommend LangChain's OpenAIEmbeddings class for storing your embeddings in a vector store.

We recommend {company} {db-serverless} to store your embeddings. It's a managed service that uses Cassandra under the hood, and is a good choice if you're already using Cassandra or want to use Cassandra for other parts of your application. {db-serverless} integrates with LangChain as a vector store using the https://python.langchain.com/docs/integrations/vectorstores/astradb[AstraPy client].

include::examples:partial$prerequisites.adoc[]

== Store embeddings in vector-enabled {db-serverless} database

This code embeds the loaded `Documents` from the xref:splitting.adoc[] example and stores the embeddings in the {db-serverless} vector store.
[source,python]
----
import os
from dotenv import load_dotenv
from langchain_community.vectorstores import AstraDB
from langchain_openai import OpenAIEmbeddings

load_dotenv()

ASTRA_DB_COLLECTION = os.environ.get("ASTRA_DB_COLLECTION")

embedding = OpenAIEmbeddings()
vstore = AstraDB(
    embedding=embedding,
    collection_name="test",
    token=os.environ["ASTRA_DB_APPLICATION_TOKEN"],
    api_endpoint=os.environ["ASTRA_DB_API_ENDPOINT"],
)
docs = []
inserted_ids = vstore.add_documents(docs)
print(f"\nInserted {len(inserted_ids)} documents.")

print(vstore.astra_db.collection(ASTRA_DB_COLLECTION).find())
----
