= RAG Overview

Retrieval Augmented Generation (RAG) is a popular machine learning technique that retrieves prior context from a memory system to construct a prompt that is passed to a model.

This means the power of Large Language Models, but trained on your data.

== RAG process

Whatever type of data you have, from unstructured text to video, is converted into a structured format that allows it to be easily retrieved. This conversion is performed by segmenting or “chunking” the input data into many pieces, and then embedding each chunk as a vector in a vector database. A vector is a series of numbers that represents the characteristics of each chunk of input data. These vectors are then indexed for searching.

When you ask a question of your data, this index is queried for relevant information - semantically similar vectors, for example - and the results of the index query are passed along with your question to an LLM. LLMs possess amazing powers of reasoning and deduction, but they know nothing about your data. By including the results of the index query, you supply valuable contextual information for the LLM to reason through and return an answer to your question.

Receiving an answer back from the LLM marks the end of the RAG process, but not the end of the story. Each step of the process from chunking strategy to answer quality can be tested, evaluated, and optimized for your production systems. RAGStack also enables advanced RAG techniques for improving responses and reducing LLM round-trips.

The RAG technique has three main moving parts to consider when building your application.

Indexing (link to anchor)
Querying (link to anchor)
Evaluating(link to anchor)









